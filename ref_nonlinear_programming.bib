@book{bazaraaNonlinearProgrammingTheory2006,
  title = {Nonlinear {{Programming}}: {{Theory}} and {{Algorithms}}},
  shorttitle = {Nonlinear {{Programming}}},
  author = {Bazaraa, Mokhtar S. and Sherali, Hanif D. and Shetty, C. M.},
  year = {2006},
  month = may,
  edition = {3},
  publisher = {Wiley-Interscience},
  url = {https://www.wiley.com/en-us/Nonlinear+Programming%3A+Theory+and+Algorithms%2C+3rd+Edition-p-9780471486008},
  abstract = {COMPREHENSIVE COVERAGE OF NONLINEAR PROGRAMMING THEORY AND ALGORITHMS, THOROUGHLY REVISED AND EXPANDEDNonlinear Programming: Theory and Algorithms{\texthorizontalbar}now in an extensively updated Third Edition{\texthorizontalbar}addresses the problem of optimizing an objective function in the presence of equality and inequality constraints. Many realistic problems cannot be adequately represented as a linear program owing to the nature of the nonlinearity of the objective function and/or the nonlinearity of any constraints. The Third Edition begins with a general introduction to nonlinear programming with illustrative examples and guidelines for model construction.Concentration on the three major parts of nonlinear programming is provided:Convex analysis with discussion of topological properties of convex sets, separation and support of convex sets, polyhedral sets, extreme points and extreme directions of polyhedral sets, and linear programmingOptimality conditions and duality with coverage of the nature, interpretation, and value of the classical Fritz John (FJ) and the Karush-Kuhn-Tucker (KKT) optimality conditions; the interrelationships between various proposed constraint qualifications; and Lagrangian duality and saddle point optimality conditionsAlgorithms and their convergence, with a presentation of algorithms for solving both unconstrained and constrained nonlinear programming problemsImportant features of the Third Edition include:New topics such as second interior point methods, nonconvex optimization, nondifferentiable optimization, and moreUpdated discussion and new applications in each chapterDetailed numerical examples and graphical illustrationsEssential coverage of modeling and formulating nonlinear programsSimple numerical problemsAdvanced theoretical exercisesThe book is a solid reference for professionals as well as a useful text for students in the fields of operations research, management science, industrial engineering, applied mathematics, and also in engineering disciplines that deal with analytical optimization techniques. The logical and self-contained format uniquely covers nonlinear programming techniques with a great depth of information and an abundance of valuable examples and illustrations that showcase the most current advances in nonlinear problems.},
  isbn = {978-0-471-48600-8},
  langid = {english}
}

@book{beckIntroductionNonlinearOptimization2014,
  title = {Introduction to {{Nonlinear Optimization}}: : {{Theory}}, {{Algorithms}}, and {{Applications}} with {{MATLAB}}},
  author = {Beck, Amir},
  year = {2014},
  month = oct,
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611973655},
  url = {https://sites.google.com/site/amirbeck314/books},
  urldate = {2022-01-26},
  abstract = {This book emerged from the idea that an optimization training should include three basic components: a strong theoretical and algorithmic foundation, familiarity with various applications, and the ability to apply the theory and algorithms on actual ``real-life'' problems. The book is intended to be the basis of such an extensive training. The mathematical development of the main concepts in nonlinear optimization is done rigorously, where a special effort was made to keep the proofs as simple as possible. The results are presented gradually and accompanied with many illustrative examples. Since the aim is not to give an encyclopedic overview, the focus is on the most useful and important concepts. The theory is complemented by numerous discussions on applications from various scientific fields such as signal processing, economics and localization. Some basic algorithms are also presented and studied to provide some flavor of this important aspect of optimization. Many topics are demonstrated by MATLAB programs, and ideally, the interested reader will find satisfaction in the ability of actually solving problems on his or her own. The book contains several topics that, compared to other classical textbooks, are treated differently. The following are some examples of the less common issues.},
  isbn = {978-1-61197-364-8}
}

@book{beckIntroductionNonlinearOptimization2023,
  title = {Introduction to {{Nonlinear Optimization}}: {{Theory}}, {{Algorithms}}, and {{Applications}} with {{Python}} and {{MATLAB}}},
  shorttitle = {Introduction to {{Nonlinear Optimization}}},
  author = {Beck, Amir},
  year = {2023},
  month = aug,
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  edition = {2},
  publisher = {Society for Industrial \& Applied Mathematics,U.S.},
  address = {Philadelphia, PA, USA},
  url = {https://doi.org/10.1137/1.9781611977622},
  abstract = {Built on the framework of the successful first edition, this book serves as a modern introduction to the field of optimization. The author's objective is to provide the foundations of theory and algorithms of nonlinear optimization as well as to present a variety of applications from diverse areas of applied sciences. Introduction to Nonlinear Optimization gradually yet rigorously builds connections between theory, algorithms, applications, and actual implementation. The book contains several topics not typically included in optimization books, such as optimality conditions in sparsity constrained optimization, hidden convexity, and total least squares. Readers will discover a wide array of applications such as circle fitting, Chebyshev center, the Fermat--Weber problem, denoising, clustering, total least squares, and orthogonal regression. These applications are studied both theoretically and algorithmically, illustrating concepts such as duality. Python and MATLAB programs are used to show how the theory can be implemented. The extremely popular CVX toolbox (MATLAB) and CVXPY module (Python) are described and used.More than 250 theoretical, algorithmic, and numerical exercises enhance the reader's understanding of the topics. (More than 70 of the exercises provide detailed solutions, and many others are provided with final answers.) The theoretical and algorithmic topics are illustrated by Python and MATLAB examples.},
  isbn = {978-1-61197-761-5},
  langid = {english}
}

@book{bertsekasConstrainedOptimizationLagrange1996,
  title = {Constrained {{Optimization}} and {{Lagrange Multiplier Methods}}},
  author = {Bertsekas, Dimitri P. and Bertsekas, Dimitri P.},
  year = {1996},
  month = jan,
  publisher = {Athena Scientific},
  address = {Belmont, Mass},
  url = {http://www.athenasc.com/lmultbook.html},
  isbn = {978-1-886529-04-5},
  langid = {english}
}

@book{bertsekasNonlinearProgramming2016,
  title = {Nonlinear {{Programming}}},
  shorttitle = {Nonlinear {{Programming}}},
  author = {Bertsekas, Dimitri},
  year = {2016},
  month = jun,
  edition = {3rd},
  publisher = {Athena Scientific},
  address = {Belmont, Mass},
  url = {http://www.athenasc.com/nonlinbook.html},
  isbn = {978-1-886529-05-2},
  langid = {english}
}

@article{bertsekasProjectedNewtonMethods1982,
  title = {Projected {{Newton Methods}} for {{Optimization Problems}} with {{Simple Constraints}}},
  author = {Bertsekas, D.},
  year = {1982},
  month = mar,
  journal = {SIAM Journal on Control and Optimization},
  volume = {20},
  number = {2},
  pages = {221--246},
  issn = {0363-0129},
  doi = {10.1137/0320018},
  url = {https://epubs.siam.org/doi/abs/10.1137/0320018},
  urldate = {2019-04-11},
  abstract = {We consider the problem \${\textbackslash}min {\textbackslash}\{ f(x){\textbar}x {\textbackslash}geqq 0{\textbackslash}\} \$, and propose algorithms of the form \$x\_\{k + 1\}  = [x\_k  - {\textbackslash}alpha \_k D\_k {\textbackslash}nabla f(x\_k )]{\textasciicircum} +  \$, where \$[ {\textbackslash}cdot ]{\textasciicircum} +  \$ denotes projection on the positive orthant, \${\textbackslash}alpha \_k \$ is a stepsize chosen by an Armijo-like rule and \$D\_k \$ is a positive definite symmetric matrix which is partly diagonal. We show that \$D\_k \$ can be calculated simply on the basis of second derivatives of f so that the resulting Newton-like algorithm has a typically superlinear rate of convergence. With other choices of \$D\_k \$ convergence at a typically linear rate is obtained. The algorithms are almost as simple as their unconstrained counterparts. They are well suited for problems of large dimension such as those arising in optimal control while being competitive with existing methods for low-dimensional problems. The effectiveness of the Newton-like algorithm is demonstrated via computational examples involving as many as 10,000 variables. Extensions to general linearly constrained problems are also provided. These extensions utilize a notion of an active generalized rectangle patterned after the notion of an active manifold used in manifold suboptimization methods. By contrast with these methods, many constraints can be added or subtracted from the binding set at each iteration without the need to solve a quadratic programming problem.}
}

@book{bieglerNonlinearProgramming2010,
  title = {Nonlinear {{Programming}}},
  author = {Biegler, Lorenz T.},
  year = {2010},
  month = jan,
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898719383},
  url = {https://my.siam.org/Store/Product/viewproduct/?ProductId=1488},
  urldate = {2022-01-26},
  abstract = {Chemical engineering applications have been a source of challenging optimization problems for over 50 years. For many chemical process systems, detailed steady state and dynamic behavior can now be described by a rich set of detailed nonlinear models, and relatively small changes in process design and operation can lead to significant improvements in efficiency, product quality, environmental impact, and profitability.With these characteristics, it is not surprising that systematic optimization strategies have played an important role in chemical engineering practice. In particular, over the past 35 years, nonlinear programming (NLP) has become an indispensable tool for the optimization of chemical processes. These tools are now applied at research and process development stages, in the design stage, and in the online operation of these processes. More recently, the scope of these applications is being extended to cover more challenging, large-scale tasks including process control based on the optimization of nonlinear dynamic models, as well as the incorporation of nonlinear models into strategic planning functions. Moreover, the ability to solve large-scale process optimization models cheaply, even online, is aided by recent breakthroughs in nonlinear programming, including the development of modern barrier methods, deeper understanding of line search and trust region strategies to aid global convergence, efficient exploitation of second derivatives in algorithmic development, and the availability of recently developed and widely used NLP codes, including those for barrier methods [81, 391, 404], sequential quadratic programming (SQP) [161, 159], and reduced gradient methods [119, 245, 285]. Finally, the availability of optimization modeling environments, such as AIMMS, AMPL, and GAMS, as well as the NEOS server, has made the formulation and solution of optimization accessible to a much wider user base. All of these advances have a huge impact in addressing and solving process engineering problems previously thought intractable. In addition to developments in mathematical programming, research in process systems engineering has led to optimization modeling formulations that leverage these algorithmic advances, with specific model structure and characteristics that lead to more efficient solutions. This text attempts to make these recent optimization advances accessible to engineers and practitioners. Optimization texts for engineers usually fall into two categories. First, excellent mathematical programming texts (e.g., [134, 162, 294, 100, 227]) emphasize fundamental properties and numerical analysis, but have few specific examples with relevance to real-world applications, and are less accessible to practitioners. On the other hand, equally good engineering texts (e.g., [122, 305, 332, 53]) emphasize applications with well-known methods and codes, but often without their underlying fundamental properties. While their approach is accessible and quite useful for engineers, these texts do not aid in a deeper understanding of the methods or provide extensions to tackle large-scale problems efficiently.},
  isbn = {978-0-89871-702-0}
}

@book{dennisNumericalMethodsUnconstrained1996,
  title = {Numerical {{Methods}} for {{Unconstrained Optimization}} and {{Nonlinear Equations}}},
  author = {Dennis, J. E. and Schnabel, Robert B.},
  year = {1996},
  month = jan,
  series = {Classics in {{Applied Mathematics}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611971200},
  url = {https://epubs.siam.org/doi/book/10.1137/1.9781611971200},
  urldate = {2022-01-26},
  abstract = {We are delighted that SIAM is republishing our original 1983 book after what many in the optimization field have regarded as ``premature termination'' by the previous publisher. At 12 years of age, the book may be a little young to be a ``classic,'' but since its publication it has been well received in the numerical computation community. We are very glad that it will continue to be available for use in teaching, research, and applications. We set out to write this book in the late 1970s because we felt that the basic techniques for solving small to medium-sized nonlinear equations and unconstrained optimization problems had matured and converged to the point where they would remain relatively stable. Fortunately, the intervening years have confirmed this belief. The material that constitutes most of this book---the discussion of Newton-based methods, globally convergent line search and trust region methods, and secant (quasi-Newton) methods for nonlinear equations, unconstrained optimization, and nonlinear least squares---continues to represent the basis for algorithms and analysis in this field. On the teaching side, a course centered around Chapters 4 to 9 forms a basic, in-depth introduction to the solution of nonlinear equations and unconstrained optimization problems. For researchers or users of optimization software, these chapters give the foundations of methods and software for solving small to medium-sized problems of these types.},
  isbn = {978-0-89871-364-0}
}

@book{deuflhardNewtonMethodsNonlinear2011,
  title = {Newton {{Methods}} for {{Nonlinear Problems}}: {{Affine Invariance}} and {{Adaptive Algorithms}}},
  shorttitle = {Newton {{Methods}} for {{Nonlinear Problems}}},
  author = {Deuflhard, Peter},
  year = {2011},
  month = sep,
  series = {Springer {{Series}} in {{Computational Mathematics}}},
  number = {35},
  publisher = {Springer Science \& Business Media},
  url = {https://link.springer.com/book/10.1007/978-3-642-23899-4},
  abstract = {This book deals with the efficient numerical solution of challenging nonlinear problems in science and engineering, both in finite dimension (algebraic systems) and in infinite dimension (ordinary and partial differential equations). Its focus is on local and global Newton methods for direct problems or Gauss-Newton methods for inverse problems. The term 'affine invariance' means that the presented algorithms and their convergence analysis are invariant under one out of four subclasses of affine transformations of the problem to be solved. Compared to traditional textbooks, the distinguishing affine invariance approach leads to shorter theorems and proofs and permits the construction of fully adaptive algorithms. Lots of numerical illustrations, comparison tables, and exercises make the text useful in computational mathematics classes. At the same time, the book opens many directions for possible future research.},
  googlebooks = {5rVcytMq3DoC},
  isbn = {978-3-642-23899-4},
  langid = {english}
}

@inproceedings{diehlLocalConvergenceGeneralized2019,
  title = {Local {{Convergence}} of {{Generalized Gauss-Newton}} and {{Sequential Convex Programming}}},
  booktitle = {2019 {{IEEE}} 58th {{Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Diehl, Moritz and Messerer, Florian},
  year = {2019},
  month = dec,
  pages = {3942--3947},
  issn = {2576-2370},
  doi = {10.1109/CDC40024.2019.9029288},
  abstract = {We analyze the convergence properties of two Newton-type algorithms for the solution of unconstrained nonlinear optimization problems with convex substructure: Generalized Gauss-Newton (GGN) and Sequential Convex Programming (SCP). While both algorithms are identical to the classical Gauss-Newton method for the special case of nonlinear least squares, they differ when applied to more general convex outer functions. We show under mild assumptions that GGN and SCP have locally linear convergence with the same contraction rate. The convergence or divergence rate can be characterized as the smallest scalar that satisfies two linear matrix inequalities. We further show that bad convergence or divergence at a given local minimum can be a desirable property in the context of estimation problems with symmetric likelihood functions, because it avoids that the algorithm is attracted by statistically undesirable local minima. Both algorithms and their convergence properties are illustrated with a numerical example.}
}

@misc{diehlSurveyGeneralizedGaussNewton2019,
  title = {A {{Survey}} of {{Generalized Gauss-Newton}} and {{Sequential Convex Programming Methods}}},
  author = {Diehl, Moritz},
  year = {2019},
  month = sep,
  address = {Nice},
  url = {https://fgs-2019.sciencesconf.org/data/diehl.pdf}
}

@book{dostalOptimalQuadraticProgramming2009,
  title = {Optimal {{Quadratic Programming Algorithms}}: {{With Applications}} to {{Variational Inequalities}}},
  author = {Dost{\'a}l, Zdenek},
  year = {2009},
  series = {Springer {{Optimization}} and {{Its Applications}}},
  publisher = {Springer},
  address = {New York, NY},
  url = {https://doi.org/10.1007/b138610},
  urldate = {2022-05-31},
  abstract = {Solving optimization problems in complex systems often requires the implementation of advanced mathematical techniques. Quadratic programming (QP) is one technique that allows for the optimization of a quadratic function in several variables in the presence of linear constraints. QP problems arise in fields as diverse as electrical engineering, agricultural planning, and optics. Given its broad applicability, a comprehensive understanding of quadratic programming is a valuable resource in nearly every scientific field. Optimal Quadratic Programming Algorithms presents recently developed algorithms for solving large QP problems. The presentation focuses on algorithms which are, in a sense optimal, i.e., they can solve important classes of problems at a cost proportional to the number of unknowns. For each algorithm presented, the book details its classical predecessor, describes its drawbacks, introduces modifications that improve its performance, and demonstrates these improvements through numerical experiments. This self-contained monograph can serve as an introductory text on quadratic programming for graduate students and researchers. Additionally, since the solution of many nonlinear problems can be reduced to the solution of a sequence of QP problems, it can also be used as a convenient introduction to nonlinear programming. The reader is required to have a basic knowledge of calculus in several variables and linear algebra.},
  isbn = {978-0-387-84805-1},
  langid = {english}
}

@unpublished{johnsonQuasiNewtonOptimizationOrigin2021,
  type = {Notes for 18335 at {{MIT}}},
  title = {Quasi-{{Newton}} Optimization: {{Origin}} of the {{BFGS}} Update},
  author = {Johnson, Steven G.},
  year = {2021},
  month = may,
  url = {https://github.com/mitmath/18335/blob/spring21/notes/BFGS.pdf},
  urldate = {2024-02-09},
  langid = {english}
}

@techreport{levinCSISyEMath2019,
  type = {Lecture Notes},
  title = {{{CS}}/{{ISyE}}/{{Math}} 730: {{Nonlinear Optimization}} 2 {{Lecture Notes}}, {{Lecturer}}: {{Wright Steven}}},
  author = {Levin, Owen},
  year = 2019,
  institution = {University of Wisconsin--Madison},
  url = {https://pages.cs.wisc.edu/~olevin/Optimization_Notes.pdf}
}

@book{luenbergerLinearNonlinearProgramming2016,
  title = {Linear and {{Nonlinear Programming}}},
  author = {Luenberger, David G. and Ye, Yinyu},
  year = {2016},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  edition = {4},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-319-18842-3},
  url = {https://www.springer.com/gp/book/9783319188416},
  urldate = {2020-12-29},
  abstract = {This new edition covers the central concepts of practical optimization techniques, with an emphasis on methods that are both state-of-the-art and popular. One major insight is the connection between the purely analytical character of an optimization problem and the behavior of algorithms used to solve a problem. This was a major theme of the first edition of this book and the fourth edition expands and further illustrates this relationship. As in the earlier editions, the material in this fourth edition is organized into three separate parts. Part I is a self-contained introduction to linear programming. The presentation in this part is fairly conventional, covering the main elements of the underlying theory of linear programming, many of the most effective numerical algorithms, and many of its important special applications. Part II, which is independent of Part I, covers the theory of unconstrained optimization, including both derivations of the appropriate optimality conditions and an introduction to basic algorithms. This part of the book explores the general properties of algorithms and defines various notions of convergence. Part III extends the concepts developed in the second part to constrained optimization problems. Except for a few isolated sections, this part is also independent of Part I. It is possible to go directly into Parts II and III omitting Part I, and, in fact, the book has been used in this way in many universities.New to this edition is a chapter devoted to Conic Linear Programming, a powerful generalization of Linear Programming. Indeed, many conic structures are possible and useful in a variety of applications. It must be recognized, however, that conic linear programming is an advanced topic, requiring special study. Another important topic is an accelerated steepest descent method that exhibits superior convergence properties, and for this reason, has become quite popular. The proof of the convergence property for both standard and accelerated steepest descent methods are presented in Chapter 8. As in previous editions, end-of-chapter exercises appear for all chapters.From the reviews of the Third Edition:``{\dots} this very well-written book is a classic textbook in Optimization. It should be present in the bookcase of each student, researcher, and specialist from the host of disciplines from which practical optimization applications are drawn.'' (Jean-Jacques Strodiot, Zentralblatt MATH, Vol. 1207, 2011)},
  isbn = {978-3-319-18841-6},
  langid = {english}
}

@book{luenbergerLinearNonlinearProgramming2021,
  title = {Linear and {{Nonlinear Programming}}},
  author = {Luenberger, David G. and Ye, Yinyu},
  year = {2021},
  month = nov,
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  edition = {5},
  number = {228},
  publisher = {Springer},
  address = {Cham, Switzerland},
  url = {https://doi.org/10.1007/978-3-030-85450-8},
  abstract = {The 5th edition of this classic textbook covers the central concepts of practical optimization techniques, with an emphasis on methods that are both state-of-the-art and popular. One major insight is the connection between the purely analytical character of an optimization problem and the behavior of algorithms used to solve that problem. End-of-chapter exercises are provided for all chapters. The material is organized into three separate parts. Part I offers a self-contained introduction to linear programming. The presentation in this part is fairly conventional, covering the main elements of the underlying theory of linear programming, many of the most effective numerical algorithms, and many of its important special applications. Part II, which is independent of Part I, covers the theory of unconstrained optimization, including both derivations of the appropriate optimality conditions and an introduction to basic algorithms. This part of the book explores the general properties of algorithms and defines various notions of convergence. In turn, Part III extends the concepts developed in the second part to constrained optimization problems. Except for a few isolated sections, this part is also independent of Part I. As such, Parts II and III can easily be used without reading Part I and, in fact, the book has been used in this way at many universities. New to this edition are popular topics in data science and machine learning, such as the Markov Decision Process, Farkas' lemma, convergence speed analysis, duality theories and applications, various first-order methods, stochastic gradient method, mirror-descent method, Frank-Wolf method, ALM/ADMM method, interior trust-region method for non-convex optimization, distributionally robust optimization, online linear programming, semidefinite programming for sensor-network localization, and infeasibility detection for nonlinear optimization.},
  isbn = {978-3-030-85449-2},
  langid = {english}
}

@book{mangasarianNonlinearProgramming1994,
  title = {Nonlinear {{Programming}}},
  author = {Mangasarian, Olvi L.},
  year = {1994},
  series = {Classics in {{Applied Mathematics}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  url = {https://doi.org/10.1137/1.9781611971255},
  urldate = {2022-05-09},
  abstract = {This reprint of the 1969 book of the same name is a concise, rigorous, yet accessible, account of the fundamentals of constrained optimization theory. Many problems arising in diverse fields such as machine learning, medicine, chemical engineering, structural design, and airline scheduling can be reduced to a constrained optimization problem. This book provides readers with the fundamentals needed to study and solve such problems. Beginning with a chapter on linear inequalities and theorems of the alternative, basics of convex sets and separation theorems are then derived based on these theorems. This is followed by a chapter on convex functions that includes theorems of the alternative for such functions. These results are used in obtaining the saddlepoint optimality conditions of nonlinear programming without differentiability assumptions. Properties of differentiable convex functions are derived and then used in two key chapters of the book, one on optimality conditions for differentiable nonlinear programs and one on duality in nonlinear programming. Generalizations of convex functions to pseudoconvex and quasiconvex functions are given and then used to obtain generalized optimality conditions and duality results in the presence of nonlinear equality constraints. The book has four useful self-contained appendices on vectors and matrices, topological properties of n-dimensional real space, continuity and minimization, and differentiable functions.},
  isbn = {978-0-89871-341-1},
  langid = {english}
}

@inproceedings{messererDeterminingExactLocal2020,
  title = {Determining the {{Exact Local Convergence Rate}} of {{Sequential Convex Programming}}},
  booktitle = {2020 {{European Control Conference}} ({{ECC}})},
  author = {Messerer, Florian and Diehl, Moritz},
  year = {2020},
  month = may,
  pages = {1280--1285},
  doi = {10.23919/ECC51009.2020.9143749},
  abstract = {Sequential Convex Programming (SCP) is an iterative algorithm for solving Nonlinear Programs (NLP) with "convex-over-nonlinear" substructure. At every iteration it solves a convex, but generally nonlinear, approximation to the original NLP, exploiting its outer convexities. It is already known that SCP has linear local convergence, though without a tight characterization of the rate. Sequential Convex Quadratic Programming (SCQP) is another "convex-over-nonlinear" substructure exploiting algorithm, for which a tight characterization of the convergence rate has already been obtained. In this paper we show under mild assumptions that in fact both methods have the same local linear convergence - or divergence - rate. We can therefore determine the convergence rate of SCP, which is tightly characterized by two simple matrix inequalities evaluated at the solution. We further reason why - as a heuristic - SCP should in general show more robust global convergence than SCQP. We illustrate this, as well as the local convergence rate, with a numerical example.}
}

@book{nocedalNumericalOptimization2006,
  title = {Numerical {{Optimization}}},
  author = {Nocedal, Jorge and Wright, Stephen},
  year = {2006},
  month = jul,
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  edition = {2},
  publisher = {Springer},
  address = {New York},
  url = {https://doi.org/10.1007/978-0-387-40065-5},
  abstract = {Optimization is an important tool used in decision science and for the analysis of physical systems used in engineering. One can trace its roots to the Calculus of Variations and the work of Euler and Lagrange. This natural and reasonable approach to mathematical programming covers numerical methods for finite-dimensional optimization problems. It begins with very simple ideas progressing through more complicated concepts, concentrating on methods for both unconstrained and constrained optimization.},
  isbn = {978-0-387-30303-1},
  langid = {english}
}

@article{ozdaglarRelationPseudonormalityQuasiregularity2004,
  title = {The Relation between Pseudonormality and Quasiregularity in Constrained Optimization},
  author = {Ozdaglar, Asuman E. and Bertsekas, Dimitri P.},
  year = {2004},
  month = oct,
  journal = {Optimization Methods and Software},
  volume = {19},
  number = {5},
  pages = {493--506},
  publisher = {Taylor \& Francis},
  issn = {1055-6788},
  doi = {10.1080/10556780410001709420},
  url = {https://doi.org/10.1080/10556780410001709420},
  urldate = {2022-11-01},
  abstract = {We consider optimization problems with equality, inequality, and abstract set constraints. We investigate the relations between various characteristics of the constraint set related to the existence of Lagrange multipliers. For problems with no abstract set constraint, the classical condition of quasiregularity provides the connecting link between the most common constraint qualifications and existence of Lagrange multipliers. In earlier work, we introduced a new and general condition, pseudonormality, that is central within the theory of constraint qualifications, exact penalty functions, and existence of Lagrange multipliers. In this paper, we explore the relations between pseudonormality, quasiregularity, and existence of Lagrange multipliers, and show that, unlike pseudonormality, quasiregularity cannot play the role of a general constraint qualification in the presence of an abstract set constraint. In particular, under a regularity assumption on the abstract constraint set, we show that pseudonormality implies quasiregularity. However, contrary to pseudonormality, quasiregularity does not imply the existence of Lagrange multipliers, except under additional assumptions.}
}
