[
  {
    "objectID": "cont_numerical_direct.html",
    "href": "cont_numerical_direct.html",
    "title": "Numerical methods for direct approach",
    "section": "",
    "text": "We have already seen that direct methods for discrete-time optimal control problems essentially just regroup and reshape the problem data so that a nonlinear programming solver can accept them. In order to follow the same approach with continous-time optimal control problems, some kind of discretization is inevitable in order to formulate a nonlinear program. Depending of what is discretized and how, several groups of methods can be identified\nThe core principles behind the methods are identical to those discussed in the previous section on numerical methods for indirect approaches, but the adjective “direct” suggests that there must be some modifications.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#direct-shooting-methods",
    "href": "cont_numerical_direct.html#direct-shooting-methods",
    "title": "Numerical methods for direct approach",
    "section": "Direct shooting methods",
    "text": "Direct shooting methods\nIn contrast with the application of shooting methods to the TP-BVP originating from the indirect approach, here the variables through whose values we aim the fictitious cannon are not the initial co-state vector but the (possibly long) vector parameterizing the whole control trajectory.\nIn the simplest – and fairly practical – case of a piecewise contstant control trajectory, which is motivated by the eventual discrete-time implementation using a zero-order hold (ZOH), the control trajectory is parameterized just by the sequence of values u_0, u_1, \\ldots, u_{N-1}.\n\n\n\n\n\n\nFigure 1: Direct shooting – only the control trajectory is discretized and the parameters of this discretization serve as optimization variables\n\n\n\nThe state trajectory x(t) corresponding to the fixed initial state \\mathrm x_\\mathrm{i} and the sequence of controls is obtained by using some IVP ODE numerical solver.\nFor the chosen control trajectory and the simulated state trajectory, the cost function J(u(\\cdot);x_\\mathrm{i}) = \\phi(x(t_\\mathrm{f}),t_\\mathrm{f}) + \\int L(x,u) \\mathrm d t is then evaluated. This can also only be done numerically, typically using methods for numerical integration.\nOnce the cost function is evaluated, numerical optimization solver is used to update the vector parameterizing the control trajectory so that the cost is reduced. For the new control trajectory, the state response is simulated, … and so on.\nSince optimization is only done over the optimization variables that parameterize the control trajectory, the method resembles the sequential method of the direct approach to discrete-time optimal control.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#direct-multiple-shooting-methods",
    "href": "cont_numerical_direct.html#direct-multiple-shooting-methods",
    "title": "Numerical methods for direct approach",
    "section": "Direct multiple shooting methods",
    "text": "Direct multiple shooting methods\n…",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#direct-transcription-methods",
    "href": "cont_numerical_direct.html#direct-transcription-methods",
    "title": "Numerical methods for direct approach",
    "section": "Direct transcription methods",
    "text": "Direct transcription methods\nIn contrast with the direct shooting methods, here both the control and state trajectories are discretized. There is then no need for a numerical solver for IVP ODE.\n\n\n\n\n\n\nFigure 2: Direct transcription/discretization – both the control and state trajectories are discretized and the resulting vectors of parameterers of this discretization serve as optimization variables\n\n\n\nThe optimization is then done over the optimization variables that parameterize both the control trajectory and the state trajectory. In this regard the methods resemble the simultaneous method of the direct approach to discrete-time optimal control.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#direct-collocation-methods",
    "href": "cont_numerical_direct.html#direct-collocation-methods",
    "title": "Numerical methods for direct approach",
    "section": "Direct collocation methods",
    "text": "Direct collocation methods\nSimilarly as in the direct transcription methods, both the control and state trajectories are discretized and the parameterization of this discretization enters the optimization.\nBut here it is not just the values of the state and the control variables at the discretization points that are the optimization variables, but rather the coefficients of the polynomials that approximate the corresponding variables at the intermediate times between the discretization points.\n\n\n\n\n\n\nFigure 3: Direct collocation – both the control and state trajectories are approximated by piecewise polynomials and parameters of these polynomials serve as optimization variables\n\n\n\nHowever, similarly as we have already seen while discussing the collocation methods for the indirect approach, every direct collocation methods is equivalent to some direct transcription method. For example, collocation with a quadratic polynomial and the collocation points at the beginning and end of the interval is equivalent to the implicit trapezoidal rule. Some people even say that the implicit trapezoidal method is a collocation method.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "cont_numerical_direct.html#minimumtime-optimal-control-in-direct-approach",
    "href": "cont_numerical_direct.html#minimumtime-optimal-control-in-direct-approach",
    "title": "Numerical methods for direct approach",
    "section": "Minimum‐time optimal control in direct approach",
    "text": "Minimum‐time optimal control in direct approach\nWhile the indirect approach to optimal control allows for relaxing the final time t_\\mathrm{f} (and possibly including it in the cost function), the direct approach does not seem to support it. The final time has to be finite and constant.\nHere we show one way to turn the final time is t_\\mathrm{f} into an optimization variable. We achieve it by augmenting the state equations with a new variable… guess what… t_\\mathrm{f} accompanied with the corresponding state equation \\dot t_\\mathrm{f} = 0.\nWe now introduce a new normalized time variable \\tau\\in [0,1] such that \n  t = \\tau t_\\mathrm{f}.\n\nThe differentials of time then relate correspondingly \n  \\mathrm{d} t = t_\\mathrm{f}\\mathrm{d}\\tau.\n\nThe original state equation \\frac{\\mathrm{d}x}{\\mathrm{d}t} = f(x) then modifies to \\boxed{\n  \\frac{\\mathrm{d}x}{\\mathrm{d}\\tau} = t_\\mathrm{f} f(x).}\n\nStrictly speaking this should be \\frac{\\mathrm{d}\\hat x}{\\mathrm{d}\\tau} = t_\\mathrm{f} f(\\hat x), reflecting that x(t) = x(t_\\mathrm{r} \\tau) = \\hat x(\\tau) . But the abuse of notation is perhaps acceptable.\nThe cost function will then include a term penalizing t_\\mathrm{f}. Or perhaps the whole cost function is just t_\\mathrm{f} .Or perhaps the final time does not even have to be penalized at all, it is just an optimization variable, it just adds a degree of freedom to the optimal control problem. Of course, in the latter case this is no longer a minimum-time problem, just a free-final time problem.\nIn direct transcription methods for optimal control, the number of discretization/integration (sub)intervals is fixed and not subject to optimization, otherwise the resulting NLP would have a varying size for a varying final time, which is not convenient. Instead, the length of the integration interval, or the discretization/sampling interval is varied hN = t_\\mathrm{f}.\nRecall that all (single-step) discretization methods can be interpreted as an approximate computation of the definite integral in x_{k+1} = x_k + \\int_{t_k}^{t_{k+1}} f(x(t),t)\\mathrm{d}t, which in s-stage Runge-Kutta methods with a fixed time step h always looks like x_{k+1} = x_k + h \\sum_{j=1}^s b_{j} f_{kj}.\nSince the length of the integration interval h is now an optimization variable, even a problem that is originally linear is now turned into a nonlinear one. Furthermore, we can see consistency between x_{k+1} = x_k + \\frac{t_\\mathrm{f}}{N} \\sum_{j=1}^s b_{j} f_{kj} and the above result for the continuous-time problem that does not consider discretization.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for direct approach"
    ]
  },
  {
    "objectID": "reduction_order_controller.html",
    "href": "reduction_order_controller.html",
    "title": "Controller order reduction",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "14. Model and controller order reduction",
      "Controller order reduction"
    ]
  },
  {
    "objectID": "opt_theory_references.html",
    "href": "opt_theory_references.html",
    "title": "References",
    "section": "",
    "text": "If a single reference book on nonlinear optimization is to be recommended, be it (Nocedal and Wright 2006) that sits on your book shelf.\nIf one or two more can still fit, (Bertsekas 2016), (Luenberger and Ye 2021) are classical comprehensive references on nonlinear programming (the latter covers linear programming too).\nWhile all the three books are only available for purchase, there is a wealth of resources are freely available online such as the notes (Gros and Diehl 2022) accompanying a course on optimal control, which do a decent job of introduction to a nonlinear programming, and beautifully typeset modern textbooks (Kochenderfer and Wheeler 2019) and (Martins and Ning 2022), the former based on Julia language. Yet another high-quality textbook that is freely available online is (Bierlaire 2018).\nWhen restricting to convex optimization, the bible of this field (Boyd and Vandenberghe 2004) is also freely available online. It is a must-have for everyone interested in optimization. Another textbook biased towards convex optimization is (Calafiore 2014), which is freely accessible through its web version. Yet another advanced and treatment of convex optimization is (Ben-Tal and Nemirovski 2023), which is also freely available online.\nMaybe a bit unexpected resources on theory are materials accompanying some optimization software. Partilarly recommendable is (“MOSEK Modeling Cookbook” 2024), it is very useful even if you do not indend to use their software.\n\n\n\n\n Back to topReferences\n\nBen-Tal, Aharon, and Arkadi Nemirovski. 2023. “Lectures on Modern Convex Optimization - 2020/2021/2022/2023 Analysis, Algorithms, Engineering Applications.” Lecture Notes. Technion & Georgia Institute of Technology. https://www2.isye.gatech.edu/~nemirovs/LMCOLN2023Spring.pdf.\n\n\nBertsekas, Dimitri. 2016. Nonlinear Programming. 3rd ed. Belmont, Mass: Athena Scientific. http://www.athenasc.com/nonlinbook.html.\n\n\nBierlaire, Michel. 2018. Optimization: Principles and Algorithms. 2nd ed. Lausanne: EPFL Press. https://transp-or.epfl.ch/books/optimization/html/OptimizationPrinciplesAlgorithms2018.pdf.\n\n\nBoyd, Stephen, and Lieven Vandenberghe. 2004. Convex Optimization. Seventh printing with corrections 2009. Cambridge, UK: Cambridge University Press. https://web.stanford.edu/~boyd/cvxbook/.\n\n\nCalafiore, Giuseppe C. 2014. Optimization Models. Cambridge, UK: Cambridge University Press. https://people.eecs.berkeley.edu/~elghaoui/optmodbook.html.\n\n\nGros, Sebastien, and Moritz Diehl. 2022. “Numerical Optimal Control (Draft).” Systems Control; Optimization Laboratory IMTEK, Faculty of Engineering, University of Freiburg. https://www.syscop.de/files/2020ss/NOC/book-NOCSE.pdf.\n\n\nKochenderfer, Mykel J., and Tim A. Wheeler. 2019. Algorithms for Optimization. The MIT Press. https://algorithmsbook.com/optimization/.\n\n\nLuenberger, David G., and Yinyu Ye. 2021. Linear and Nonlinear Programming. 5th ed. Vol. 228. International Series in Operations Research & Management Science. Cham, Switzerland: Springer. https://doi.org/10.1007/978-3-030-85450-8.\n\n\nMartins, Joaquim R. R. A., and Andrew Ning. 2022. Engineering Design Optimization. Cambridge ; New York, NY: Cambridge University Press. https://mdobook.github.io/.\n\n\n“MOSEK Modeling Cookbook.” 2024. Mosek ApS. https://docs.mosek.com/MOSEKModelingCookbook-a4paper.pdf.\n\n\nNocedal, Jorge, and Stephen Wright. 2006. Numerical Optimization. 2nd ed. Springer Series in Operations Research and Financial Engineering. New York: Springer. https://doi.org/10.1007/978-0-387-40065-5.",
    "crumbs": [
      "1. Optimization – theory",
      "References"
    ]
  },
  {
    "objectID": "discr_dir_mpc_explicit.html",
    "href": "discr_dir_mpc_explicit.html",
    "title": "Explicit MPC",
    "section": "",
    "text": "The popular framework of MPC prescribes a control law implicitly. at a given time the optimal control input",
    "crumbs": [
      "6. More on MPC",
      "Explicit MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_explicit.html#literature",
    "href": "discr_dir_mpc_explicit.html#literature",
    "title": "Explicit MPC",
    "section": "Literature",
    "text": "Literature\n(A. Bemporad, Borrelli, and Morari 2002), (Alberto Bemporad 2021), (Alessio and Bemporad 2009), (Borrelli, Bemporad, and Morari 2017)",
    "crumbs": [
      "6. More on MPC",
      "Explicit MPC"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html",
    "href": "opt_theory_constrained.html",
    "title": "Theory for constrained optimization",
    "section": "",
    "text": "We consider the following optimization problem with equality constraints \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bm x) = \\mathbf 0,\n\\end{aligned}\n where \\mathbf h(\\bm x) \\in \\mathbb R^m defines a set of m equations \n\\begin{aligned}\nh_1(\\bm x) &= 0\\\\\nh_2(\\bm x) &= 0\\\\\n\\vdots\\\\\nh_m(\\bm x) &= 0.\n\\end{aligned}\n\nAugmenting the original cost function f with the constraint functions h_i scaled by Lagrange variables \\lambda_i gives the Lagrangian function \n\\mathcal{L}(\\bm x,\\boldsymbol\\lambda) \\coloneqq f(\\bm x) + \\sum_{i=1}^m \\lambda_i h_i(\\bm x) = f(\\bm x) + \\boldsymbol \\lambda^\\top \\mathbf h(\\bm x).\n\n\n\n\n\nThe first-order necessary condition of optimality is \n\\nabla \\mathcal{L}(\\bm x,\\boldsymbol\\lambda) = \\mathbf 0,\n which amounts to two (generally vector) equations \n\\boxed{\n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\bm x) &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nDefining a matrix \\nabla \\mathbf h(\\bm x) \\in \\mathbb R^{n\\times m} as horizontally stacked gradients of the constraint functions \n\\nabla \\mathbf h(\\bm x) \\coloneqq \\begin{bmatrix}\n                                 \\nabla h_1(\\bm x) && \\nabla h_2(\\bm x) && \\ldots && \\nabla h_m(\\bm x)\n                            \\end{bmatrix},\n in fact, a transpose of the Jacobian matrix, the necessary condition can be rewritten in a vector form as \\boxed\n{\\begin{aligned}\n\\nabla f(\\bm x) + \\nabla \\mathbf h(\\bm x)\\boldsymbol \\lambda &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nBeware of the nonregularity issue! The (\\nabla \\mathbf h(\\bm x))^\\mathrm T is regular at a given \\bm x (the \\bm x is a regular point) if it has a full column rank. Rank-deficiency reveals a defect in formulation.\n\nExample 1 (Equality-constrained quadratic program) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} &\\quad \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{r}^\\top\\bm{x}\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x + \\mathbf b = \\mathbf 0.\n\\end{aligned}\n\nThe first-order necessary condition of optimality is\n\n\\begin{bmatrix}\n  \\mathbf Q & \\mathbf A^\\top\\\\\\mathbf A & \\mathbf 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\bm x \\\\ \\boldsymbol \\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  -\\mathbf r\\\\\\mathbf b\n\\end{bmatrix}.\n\n\n\n\n\n\nUsing the unconstrained Hessian \\nabla^2_{\\mathbf{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda) is too conservative. Instead, use projected Hessian\n\n\\mathbf{Z}^\\mathrm{T}\\;\\nabla^2_{\\bm{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda)\\;\\mathbf Z &gt; 0,\n where \\mathbf Z is an (orthonormal) basis of the nullspace of the Jacobian (\\nabla \\mathbf h(\\bm x))^\\top.",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#equality-constraints",
    "href": "opt_theory_constrained.html#equality-constraints",
    "title": "Theory for constrained optimization",
    "section": "",
    "text": "We consider the following optimization problem with equality constraints \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bm x) = \\mathbf 0,\n\\end{aligned}\n where \\mathbf h(\\bm x) \\in \\mathbb R^m defines a set of m equations \n\\begin{aligned}\nh_1(\\bm x) &= 0\\\\\nh_2(\\bm x) &= 0\\\\\n\\vdots\\\\\nh_m(\\bm x) &= 0.\n\\end{aligned}\n\nAugmenting the original cost function f with the constraint functions h_i scaled by Lagrange variables \\lambda_i gives the Lagrangian function \n\\mathcal{L}(\\bm x,\\boldsymbol\\lambda) \\coloneqq f(\\bm x) + \\sum_{i=1}^m \\lambda_i h_i(\\bm x) = f(\\bm x) + \\boldsymbol \\lambda^\\top \\mathbf h(\\bm x).\n\n\n\n\n\nThe first-order necessary condition of optimality is \n\\nabla \\mathcal{L}(\\bm x,\\boldsymbol\\lambda) = \\mathbf 0,\n which amounts to two (generally vector) equations \n\\boxed{\n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\bm x) &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nDefining a matrix \\nabla \\mathbf h(\\bm x) \\in \\mathbb R^{n\\times m} as horizontally stacked gradients of the constraint functions \n\\nabla \\mathbf h(\\bm x) \\coloneqq \\begin{bmatrix}\n                                 \\nabla h_1(\\bm x) && \\nabla h_2(\\bm x) && \\ldots && \\nabla h_m(\\bm x)\n                            \\end{bmatrix},\n in fact, a transpose of the Jacobian matrix, the necessary condition can be rewritten in a vector form as \\boxed\n{\\begin{aligned}\n\\nabla f(\\bm x) + \\nabla \\mathbf h(\\bm x)\\boldsymbol \\lambda &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nBeware of the nonregularity issue! The (\\nabla \\mathbf h(\\bm x))^\\mathrm T is regular at a given \\bm x (the \\bm x is a regular point) if it has a full column rank. Rank-deficiency reveals a defect in formulation.\n\nExample 1 (Equality-constrained quadratic program) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} &\\quad \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{r}^\\top\\bm{x}\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x + \\mathbf b = \\mathbf 0.\n\\end{aligned}\n\nThe first-order necessary condition of optimality is\n\n\\begin{bmatrix}\n  \\mathbf Q & \\mathbf A^\\top\\\\\\mathbf A & \\mathbf 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\bm x \\\\ \\boldsymbol \\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  -\\mathbf r\\\\\\mathbf b\n\\end{bmatrix}.\n\n\n\n\n\n\nUsing the unconstrained Hessian \\nabla^2_{\\mathbf{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda) is too conservative. Instead, use projected Hessian\n\n\\mathbf{Z}^\\mathrm{T}\\;\\nabla^2_{\\bm{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda)\\;\\mathbf Z &gt; 0,\n where \\mathbf Z is an (orthonormal) basis of the nullspace of the Jacobian (\\nabla \\mathbf h(\\bm x))^\\top.",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#inequality-constraints",
    "href": "opt_theory_constrained.html#inequality-constraints",
    "title": "Theory for constrained optimization",
    "section": "Inequality constraints",
    "text": "Inequality constraints\n\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf g(\\bm x) \\leq \\mathbf 0,\n\\end{aligned}\n where \\mathbf g(\\bm x) \\in \\mathbb R^p defines a set of p inequalities.\n\nFirst-order necessary condition of optimality\nKarush-Kuhn-Tucker (KKT) conditions of optimality are then composed of these four (sets of) conditions \n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^p \\mu_i \\nabla g_i(\\bm x) &= \\mathbf 0,\\\\\n\\mathbf{g}(\\bm{x}) &\\leq \\mathbf 0,\\\\\n\\mu_i g_i(\\bm x) &= 0,\\quad i = 1,2,\\ldots, m\\\\\n\\mu_i &\\geq 0,\\quad   i = 1,2,\\ldots, m.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#equality-and-inequality-constraints",
    "href": "opt_theory_constrained.html#equality-and-inequality-constraints",
    "title": "Theory for constrained optimization",
    "section": "Equality and inequality constraints",
    "text": "Equality and inequality constraints\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bm x) = \\mathbf 0,\\\\\n                    &\\quad \\mathbf g(\\bm x) \\leq \\mathbf 0.\n\\end{aligned}\n\n\nFirst-order necessary condition of optimality\nThe KKT conditions\n\n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\bm x) + \\sum_{i=1}^p \\mu_i \\nabla g_i(\\bm x) &= \\mathbf 0\\\\\n\\mathbf{h}(\\mathbf{x}) &= \\mathbf 0\\\\\n\\mathbf{g}(\\mathbf{x}) &\\leq \\mathbf 0\\\\\n\\mu_i g_i(\\bm x) &= 0,\\quad i = 1,\\ldots, m\\\\\n\\mu_i &\\geq 0,\\quad   i = 1,\\ldots, m.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#duality",
    "href": "opt_theory_constrained.html#duality",
    "title": "Theory for constrained optimization",
    "section": "Duality",
    "text": "Duality\nDuality theory offers another view of the original optimization problem by bringing in another but related one.\nCorresponding to the general optimization problem \n  \\begin{aligned}\n  \\operatorname*{minimize}\\;&f(\\bm x)\\\\\n  \\text{subject to}\\; & \\mathbf g(\\bm x)\\leq \\mathbf 0\\\\\n  & \\mathbf h(\\bm x) = \\mathbf 0,\n  \\end{aligned}\n\nwe form the Lagrangian function \\mathcal L(\\bm x,\\bm \\lambda,\\bm \\mu) = f(\\bm x) + \\bm \\lambda^\\top \\mathbf h(\\bm x) + \\bm \\mu^\\top \\mathbf g(\\bm x)\nFor any (fixed) values of (\\bm \\lambda,\\bm \\mu) such that \\bm \\mu\\geq 0, we define the Lagrange dual function through the following unconstrained optimization problem \nq(\\bm\\lambda,\\bm\\mu) = \\inf_{\\bm x}\\mathcal L(\\bm x,\\bm \\lambda,\\bm \\mu).\n\nObviously, it is alway possible to pick a feasible solution \\bm x, in which case the value of the Lagrangian and the original function coincide, and so the result of this minimization is no worse (larger) than the minimum for the original optimization problem. It can thus serve as a lower bound q(\\bm \\lambda,\\bm \\mu) \\leq f(\\bm x^\\star).\nThis result is called weak duality. A natural idea is to find the values of \\bm \\lambda and \\bm \\mu such that this lower bound is tightest, that is, \n\\begin{aligned}\n  \\operatorname*{maximize}_{\\bm\\lambda, \\bm\\mu}\\; & q(\\bm\\lambda,\\bm\\mu)\\\\\n  \\text{subject to}\\;& \\bm\\mu \\geq \\mathbf 0.\n\\end{aligned}\n\nUnder some circumstances the result can be tight, which leads to strong duality, which means that the minimum of the original (primal) problem and the maximum of the dual problem coincide. \nq(\\bm \\lambda^\\star,\\bm \\mu^\\star) = f(\\bm x^\\star).\n\nThis related dual optimization problem can have some advantages for development of both theory and algorithms.",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for constrained optimization"
    ]
  },
  {
    "objectID": "cont_indir_references.html",
    "href": "cont_indir_references.html",
    "title": "References",
    "section": "",
    "text": "Indirect approach to optimal control is based on calculus of variations (and its later extension in the form of Pontryagin’s principle of maximum). Calculus of variations is an advanced mathematical discipline that requires non-trivial foundations and effort to master. In our course, however, we take the liberty of aiming for intuitive understanding rather than mathematical rigor. At roughly the same level, the calculus of variations is introduced in books on optimal control, such as the classic and affordable (Kirk 2004), the popular and online available (Lewis, Vrabie, and Syrmo 2012), or very accessible and also freely available online (Liberzon 2011).\nWith anticipation, we provide here a reference to the paper (Sussmann and Willems 1997), which shows how the celebrated Pontryagin’s principle of maximum extends the calculus of variations significantly. But we will only discuss this in the next chapter.\nFor those interested in a having a standard reference for the calculus of variations, the classic (Gelfand and Fomin 2020) is recommendable, the more so that it is fairly slim.\n\n\n\n\n Back to topReferences\n\nGelfand, I. M., and S. V. Fomin. 2020. Calculus of Variations. Reprint of the 1963 edition. Mineola, N.Y: Dover Publications.\n\n\nKirk, Donald E. 2004. Optimal Control Theory: An Introduction. Reprint of the 1970 edition. Dover Publications.\n\n\nLewis, Frank L., Draguna Vrabie, and Vassilis L. Syrmo. 2012. Optimal Control. 3rd ed. John Wiley & Sons. https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf.\n\n\nLiberzon, Daniel. 2011. Calculus of Variations and Optimal Control Theory: A Concise Introduction. Princeton University Press. http://liberzon.csl.illinois.edu/teaching/cvoc/cvoc.html.\n\n\nSussmann, H. J., and J. C. Willems. 1997. “300 Years of Optimal Control: From the Brachystochrone to the Maximum Principle.” IEEE Control Systems 17 (3): 32–44. https://doi.org/10.1109/37.588098.",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "References"
    ]
  },
  {
    "objectID": "cont_indir_via_calculus_of_variations.html",
    "href": "cont_indir_via_calculus_of_variations.html",
    "title": "Indirect approach to optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Indirect approach to optimal control"
    ]
  },
  {
    "objectID": "limits_of_performance_SISO.html",
    "href": "limits_of_performance_SISO.html",
    "title": "Limits for SISO systems",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "13. Limits of performance",
      "Limits for SISO systems"
    ]
  },
  {
    "objectID": "cont_indir_calculus_of_variations.html",
    "href": "cont_indir_calculus_of_variations.html",
    "title": "Calculus of variations",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Calculus of variations"
    ]
  },
  {
    "objectID": "cont_numerical_software.html",
    "href": "cont_numerical_software.html",
    "title": "Software",
    "section": "",
    "text": "The methods studied in this chapter are already quite mature and well described. Software implementations exist. Here we enumerate some of them.\nacados Implemented in C but interfaces exist for Matlab and Python FOSS (Verschueren et al. 2022)\nGPOPS-II Matlab\nrockit In Python, built on top of CasADi, interface to Matlab.\n\n\n\n\n Back to topReferences\n\nVerschueren, Robin, Gianluca Frison, Dimitris Kouzoupis, Jonathan Frey, Niels van Duijkeren, Andrea Zanelli, Branimir Novoselnik, Thivaharan Albin, Rien Quirynen, and Moritz Diehl. 2022. “Acados—a Modular Open-Source Framework for Fast Embedded Optimal Control.” Mathematical Programming Computation 14 (1): 147–83. https://doi.org/10.1007/s12532-021-00208-8.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Software"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html",
    "href": "discr_indir_LQR_inf_horizon.html",
    "title": "Discrete-time LQR-optimal control on an infinite horizon",
    "section": "",
    "text": "In this section we are going to solve the LQR problem on the time horizon extended to infinity, that is, our goal is to find an infinite (vector) control sequence \\bm u_0, \\bm u_{1},\\ldots, \\bm u_{\\infty} that minimizes \nJ_0^\\infty = \\frac{1}{2}\\sum_{k=0}^{\\infty}\\left[\\bm x_k^\\top \\mathbf Q \\bm x_k+\\bm u_k^\\top \\mathbf R\\bm u_k\\right],\n where, as before \\mathbf Q = \\mathbf Q^\\top \\succeq 0 and \\mathbf R = \\mathbf R^\\top \\succ 0 and the system is modelled by \n\\bm x_{k+1} = \\mathbf A \\bm x_{k} + \\mathbf B \\bm u_k, \\qquad \\bm x_0 = \\mathbf x_0.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR-optimal control on an infinite horizon"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html#why-the-infinite-time-horizon",
    "href": "discr_indir_LQR_inf_horizon.html#why-the-infinite-time-horizon",
    "title": "Discrete-time LQR-optimal control on an infinite horizon",
    "section": "Why the infinite time horizon?",
    "text": "Why the infinite time horizon?\nThe first question that must inevitably pop up is the one about the motivation for introducing the infinite time horizon:\n\nDoes the introduction of an infinite time horizon reflect that we do not care about when the controller accomplishes the task?\n\nNo, certainly not. The infinite time horizon is introduced to model the case when the system is expected to operate indefinitely. This is a common scenario in practice, for example, in the case of temperature control in a building.\nSimilarly, the infinite time horizon can be used in the scenarios when the final time is not known and we leave it up to the controller to take as much time as it needs to reach the desired state. But even then we can still express our desire to reach the desired state as soon as possible by choosing the weights \\mathbf Q and \\mathbf R appropriately.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR-optimal control on an infinite horizon"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html#steady-state-solution-to-discrete-time-riccati-equation",
    "href": "discr_indir_LQR_inf_horizon.html#steady-state-solution-to-discrete-time-riccati-equation",
    "title": "Discrete-time LQR-optimal control on an infinite horizon",
    "section": "Steady-state solution to discrete-time Riccati equation",
    "text": "Steady-state solution to discrete-time Riccati equation\nWe have seen in the previous section that the solution to LQR problem with free final state and finite time horizon is given by a time-varying state feedback control law \\bm u_k = \\mathbf K_k \\bm x_k. The sequence of gains \\mathbf K_k for k=0,\\ldots, N-1, is given by the sequence of matrices \\mathbf S_k for k=0,\\ldots, N, which in turn is given as the solution to the (discrete-time) Riccati equation initialized by the penalty \\mathbf S_N and solved backwards in time. But we have also seen that, at least in our example, provided the time interval was long enough, the sequence \\mathbf K_k and \\mathbf S_k both converged to some steady state values as the time k proceeded backwards towards the beginning of the time interval.\nWhile using these steady-state values instead of the full sequences lead to a suboptimal solution on a finite time horizon, it turns out that it actually gives the optimal solution on an infinite time horizon. Although our argument here may be viewed as rather hand-wavy, it is intuitive — there is no end to the time interval, hence the steady-state values are not given a chance to change “towards the end”, as we observed in the finite time horizon case.\n\n\n\n\n\n\nNote\n\n\n\nOther approaches exist for solving the infinite time horizon LQR problem that do not make any reference to the finite time horizon problem, some of them are very elegant and concise, but here we intentionally stick to viewing it as the extension of the finite time horizon problem.\n\n\n\nNotation\nBefore we proceed with the discussion of how to find the steady-state values (the limits) of \\mathbf S_k and subsequently \\mathbf K_k, we must discuss the notation first. So, while icreasing the time horizon N and the solution to the Riccati equation settles towards the beginning of the time interval. We can thenk pick the steady-state values right at the initial time k=0, that is, \\mathbf S_0 and \\mathbf K_0. But thanks to time invariance, we can also fix the final time to some (arbitrary) N and strech the interval by moving its beginning toward -\\infty. The limits of the sequences \\mathbf S_k and \\mathbf K_k can be then considered at k goes toward -\\infty. It seems appropriate to denote these limits as \\mathbf S_{-\\infty} and \\mathbf K_{-\\infty} then. Well, the fact is that the commonly accepted notation for the limits found in the literature is just \\mathbf S_\\infty and \\mathbf K_\\infty \n\\mathbf S_\\infty \\triangleq \\lim_{k\\rightarrow -\\infty} \\mathbf S_k, \\qquad \\mathbf K_\\infty \\triangleq \\lim_{k\\rightarrow -\\infty} \\mathbf K_k.\n\n\n\nHow to compute the steady-state solution to Riccati equation?\nLeaving aside for the moment the important question whether and under which conditions such a limit \\mathbf S_\\infty exists, the immediate question is how to compute such limit. One straightforward strategy is to run the recurrent scheme (Riccati equation) and generate the sequence \\mathbf S_{N}, \\mathbf S_{N-1}, \\mathbf S_{N-2}, \\ldots so long as there is a nonnegligible improvement, that is, once \\mathbf S_{k}\\approx\\mathbf S_{k+1}, stop iterating. That is certainly doable.\nThere is, however, another idea. We apply the steady-state condition \n\\mathbf S_{\\infty} = \\mathbf S_k=\\mathbf S_{k+1}\n to the Riccati equation. The resulting equation \n\\mathbf S_{\\infty}=\\mathbf A^\\text{T}\\left[\\mathbf S_{\\infty}-\\mathbf S_{\\infty}\\mathbf B(\\mathbf B^\\text{T}\\mathbf S_{\\infty}\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\text{T}\\mathbf S_{\\infty}\\right]\\mathbf A+\\mathbf Q\n is called discrete-time algebraic Riccati equation (DARE) and it is one of the most important equations in the field of computational control design.\nThe equation may look quite “messy” and offers hardly any insight. Remember the good advice to shring the problem to the scalar size while studying similar matrix-vector expressions and striving to get some insight. Our DARE simplifies to \ns_\\infty = a^2s_\\infty - \\frac{a^2b^2s_\\infty^2}{b^2s_\\infty+r} + q\n\nMultiplying both sides by the denominator we get the equivalent quadratic (in s_\\infty) equation \nb^2s_\\infty^2 + (r - a^2b^2 - b^2q)s_\\infty - qr = 0.\n\nVoilà! A scalar DARE is just a quadratic equation, for which the solutions can be found readily.\nThere is a caveat here, though, reflected in using plural in “solutions” above. Quadratic equation can have two (or none) real solutions. But the sequence produced by original recursive Riccati equation is determined uniquely! What’s up? How are the solutions to ARE related to the limiting solution of recursive Riccati equation?\nAnswering this question will keep us busy for most of this lecture. We will structure this broad question into several sub-questions\n\nUnder which conditions it is guaranteed that there exists a (bounded) limiting solution \\mathbf S_\\infty to the recursive Riccati equation for all initial (actually final) values \\mathbf S_N?\nUnder which conditions is the limit solution unique for arbitrary \\mathbf S_N?\nUnder which conditions is it guaranteed that the time-invariant feedback gain \\mathbf K_\\infty computed from \\mathbf S_\\infty stabilizes the system (on the infinite control interval)?",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR-optimal control on an infinite horizon"
    ]
  },
  {
    "objectID": "limits_of_performance_MIMO.html",
    "href": "limits_of_performance_MIMO.html",
    "title": "Limits for MIMO systems",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "13. Limits of performance",
      "Limits for MIMO systems"
    ]
  },
  {
    "objectID": "cont_indir_Pontryagin.html",
    "href": "cont_indir_Pontryagin.html",
    "title": "Pontryagin’s maximum principle",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Pontryagin's maximum principle"
    ]
  },
  {
    "objectID": "cont_numerical_indirect.html",
    "href": "cont_numerical_indirect.html",
    "title": "Numerical methods for indirect approach",
    "section": "",
    "text": "The indirect approach to optimal control reformulates the optimal control problem as a system of differential and algebraic equations (DAE) with the values of some variables specified at both ends of the time interval – the two-point boundary value problem (TP–BVP). It is only in special cases that we are able to reformulate the TP–BVP as an initial value problem (IVP), the prominent example of which is the LQR problem and the associate differential Riccati equation solved backwards in time. However, generally we need to solve the TP–BVP DAE and the only way to do it is by numerical methods. Here we give some.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for indirect approach"
    ]
  },
  {
    "objectID": "cont_numerical_indirect.html#gradient-method-for-the-tp-bvp-dae-for-free-final-state",
    "href": "cont_numerical_indirect.html#gradient-method-for-the-tp-bvp-dae-for-free-final-state",
    "title": "Numerical methods for indirect approach",
    "section": "Gradient method for the TP-BVP DAE for free final state",
    "text": "Gradient method for the TP-BVP DAE for free final state\nRecall that with the Hamiltonian defined as H(\\bm x, \\bm u, \\bm \\lambda) = L(\\bm x, \\bm u) + \\bm \\lambda^\\top \\mathbf f(\\bm x, \\bm u), the necessary conditions of optimality for the fixed final time and free final state are are given by the following system of differential and algebraic equations (DAE) \n\\begin{aligned}\n\\dot{\\bm{x}} &= \\nabla_{\\bm\\lambda} H(\\bm x,\\bm u,\\bm \\lambda) \\\\\n\\dot{\\bm{\\lambda}} &= -\\nabla_{\\bm x} H(\\bm x,\\bm u,\\bm \\lambda) \\\\\n\\mathbf 0 &= \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)\\qquad (\\text{or} \\qquad \\bm u^\\star = \\text{argmax } H(\\bm x^\\star,\\bm u, \\bm\\lambda^\\star),\\quad \\bm u \\in\\mathcal{U})\\\\\n\\bm x(t_\\mathrm{i}) &=\\mathbf x_\\mathrm{i}\\\\\n\\bm \\lambda(t_\\mathrm{f}) &= \\nabla\\phi(\\bm{x}(t_\\mathrm{f})).\n\\end{aligned}\n\nOne idea to solve this is to guess at the trajectory \\bm u(t) on a grid of the time interval, use it to solve the state and costate equations, and then with all the three variables \\bm x, \\bm u, and \\bm u evaluate how much the stationarity equation is actually not satisfied. Based on the this, modify \\bm u and go for another iteration. Formally this is expressed as the algorithm:\n\nSet some initial trajectory \\bm u(t),\\; t\\in[t_\\mathrm{i},t_\\mathrm{f}] on a grid of points in [t_\\mathrm{i},t_\\mathrm{f}].\nWith the chosen \\bm u(\\cdot) and the initial state \\bm x(t_\\mathrm{i}), solve the state equation \n\\dot{\\bm{x}} = \\nabla_{\\bm\\lambda} H(\\bm x,\\bm u,\\bm \\lambda) = \\mathbf f(\\bm x, \\bm u)\n for \\bm x(t) using a solver for initial value problem ODE, that is, on a grid of t\\in[t_\\mathrm{i},t_\\mathrm{f}].\nHaving the control and state trajectories, \\bm u(\\cdot) and \\bm x(\\cdot), solve the costate equation \n\\dot{\\bm{\\lambda}} = -\\nabla_{\\bm x} H(\\bm x,\\bm u,\\bm \\lambda)\n\nfor the costates \\bm \\lambda(t), starting at the final time t_\\mathrm{f}, invoking the boundary condition \\bm \\lambda(t_\\mathrm{f}) = \\nabla\\phi(\\bm{x}(t_\\mathrm{f})).\nEvaluate \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda) for all t\\in[t_\\mathrm{i}, t_\\mathrm{f}].\nIf \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda) \\approx  \\mathbf 0 for all t\\in[t_\\mathrm{i}, t_\\mathrm{f}], quit, otherwise modify \\bm u(\\cdot) and go to the step 2.\n\nThe question is, of course, how to modify \\bm u(t) for all t \\in [t_\\mathrm{i}, t_\\mathrm{f}] in the step 4. Recall that the variation of the (augmented) cost functional is \n\\begin{aligned}\n\\delta  J^\\text{aug} &= [\\nabla \\phi(\\bm x(t_\\mathrm{f})) - \\bm\\lambda(t_\\mathrm{f})]^\\top \\delta \\bm{x}(t_\\mathrm{f})\\\\\n& \\qquad + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\dot{\\bm{\\lambda}} +\\nabla_{\\bm x} H(\\bm x,\\bm u,\\bm \\lambda)]^\\top \\delta \\bm x(t)\\mathrm{d}t + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\dot{\\bm{x}} - \\nabla_{\\bm\\lambda} H(\\bm x,\\bm u,\\bm \\lambda)]^\\top \\delta \\bm\\lambda(t)  \\mathrm{d}t\\\\\n& \\qquad + \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)]^\\top \\delta \\bm u(t) \\mathrm{d}t\n\\end{aligned},\n and for state and costate variables satisfying the state and costate equations this variation simplifies to \n\\delta  J^\\text{aug} = \\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)]^\\top \\delta \\bm u(t) \\mathrm{d}t.\n\nSince our goal is to minimize J^\\text{aug}, we need to make \\Delta  J^\\text{aug}\\leq0. Provided the increment \n\\delta \\mathbf u(t)=\\bm u^{(i+1)}(t)-\\bm u^{(i)}(t)\n is small, we can consider the linear approximation \\delta  J^\\text{aug} instead. We choose \n\\delta \\bm u(t)  = -\\alpha \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)\n for \\alpha&gt;0, which means that the control trajectory in the next iteration is \\boxed\n{\\bm u^{(i+1)}(t)  = \\bm u^{(i)}(t) -\\alpha \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda),}\n and the variation of the augmented cost funtion is\n\n\\delta  J^\\text{aug} = -\\alpha\\int_{t_\\mathrm{i}}^{t_\\mathrm{f}} [\\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)]^2 \\mathrm{d}t \\leq 0,\n and it is zero only for \\nabla_{\\bm u} H(t,\\bm x,\\bm u,\\bm \\lambda) = \\mathbf 0 for all t\\in[t_\\mathrm{i}, t_\\mathrm{f}].",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for indirect approach"
    ]
  },
  {
    "objectID": "cont_numerical_indirect.html#methods-for-solving-tp-bvp-ode",
    "href": "cont_numerical_indirect.html#methods-for-solving-tp-bvp-ode",
    "title": "Numerical methods for indirect approach",
    "section": "Methods for solving TP-BVP ODE",
    "text": "Methods for solving TP-BVP ODE\nHere we assume that from the stationarity equation \n\\mathbf 0 = \\nabla_{\\bm u} H(\\bm x,\\bm u,\\bm \\lambda)\n we can express \\bm u(t) as a function of the the state and costate variables, \\bm x(t) and \\bm \\lambda(t), respectively. In fact, Pontryagin’s principles gives this expression as \\bm u^\\star(t) = \\text{arg} \\min_{\\bm u(t) \\in\\mathcal{U}} H(\\bm x^\\star(t),\\bm u(t), \\bm\\lambda^\\star(t)). And we substitute for \\bm u(t) into the state and costate equations. This way we eliminate \\bm u(t) from the system of DAEs and we are left with a system of ODEs for \\bm x(t) and \\bm \\lambda(t) only. Formally, the resulting Hamiltonian is a different function as it is now a functio of two variables only.\n\n\\begin{aligned}\n\\dot{\\bm{x}} &= \\nabla_{\\bm\\lambda} \\mathcal H(\\bm x,\\bm \\lambda) \\\\\n\\dot{\\bm{\\lambda}} &= -\\nabla_{\\bm x} \\mathcal H(\\bm x,\\bm \\lambda) \\\\\n\\bm x(t_\\mathrm{i}) &=\\mathbf x_\\mathrm{i}\\\\\n\\bm x(t_\\mathrm{f}) &= \\mathbf x_\\mathrm{f} \\qquad \\text{or} \\qquad \\bm \\lambda(t_\\mathrm{f}) = \\nabla\\phi(\\bm{x}(t_\\mathrm{f})).\n\\end{aligned}\n\nAlthough we now have an ODE system, it is still a BVP. Strictly speaking, from now on, arbitrary reference on numerical solution of boundary value problems can be consulted to get some overview – we no longer need to restrict ourselves to the optimal control literature and software. On the other hand, the right sides are not quite arbitrary – these are Hamiltonian equations – and this property could and perhaps even should be exploited by the solution methods.\nThe methods for solving general BVPs are generally divided into\n\nshooting and multiple shooting methods,\ndiscretization methods,\ncollocation methods.\n\n\nShooting methods\n\nShooting method outside optimal control\nHaving made the diclaimer that boundary value problems constitute a topic indenendent of the optimal control theory, we start their investigation within a control-unrelated setup. We consider a system of two ordinary differential equations in two variables with the value of the first variable specified at both ends while the value of the other variable is left unspecified \n\\begin{aligned}\n\\begin{bmatrix}\n  \\dot y_1(t)\\\\\n  \\dot y_2(t)\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\nf_1(\\bm y,t)\\\\\nf_2(\\bm y,t)\n\\end{bmatrix}\\\\\ny_1(t_\\mathrm{i}) &= \\mathrm y_{1\\mathrm{i}},\\\\\ny_1(t_\\mathrm{f}) &= \\mathrm y_{1\\mathrm{f}}.\n\\end{aligned}\n\nAn idea for a solution method is this:\n\nGuess at the missing (unspecified) value y_{2\\mathrm{i}} of y_2 at the initial time t_\\mathrm{i},\nUse an IVP solver (for example ode45 in Matlab) to find the values of both variables over the whole interval [t_\\mathrm{i},t_\\mathrm{f}].\nCompare the simulated value of the state variable y_1 at the final time t_\\mathrm{f} and compare it with the boundary value .\nBased on the error e = y_1(t_\\mathrm{f})-\\mathrm y_{1\\mathrm{f}}, update y_{2\\mathrm{i}} and go back to step 2.\n\nHow shall the update in the step 4 be realized? The value of y_1 at the final time t_\\mathrm{f} and therefore the error e are functions of the value y_{2\\mathrm{i}} of y_2 at the initial time t_\\mathrm{i}. We can formally express this upon introducing a map F such that e = F(y_{2\\mathrm{i}}). The problem now boils down to solving the nonlinear equation \\boxed\n{F(y_{2\\mathrm{i}}) = 0.}\n\nIf Newton’s method is to be used for solving this equation, the derivative of F is needed. Most often than not, numerical solvers for IVP ODE have to be called in order to evaluate the function F, in which case the derivative cannot be determined analytically. Finite difference (FD) and algorithmic/automatic differentiation (AD) methods are available.\nIn this example we only considered y_1 and y_2 as scalar variables, but in general these could be vector variables, in which case a system of equations in the vector variable has to be solved. Instead of a single scalar derivative, its matrix version – Jacobian matrix – must be determined.\nBy now the reason for calling this method shooting is perhaps obvious. Indeed, the analogy with aiming and shooting a cannon is illustrative.\nAs another example, we consider the BVP for a pendulum.\n\nExample 1 (BVP for pendulum) For an ideal pendulum described by the second-order model \\ddot \\theta + \\frac{b}{ml^2}\\dot \\theta + \\frac{g}{l} \\sin(\\theta) = 0 and for a final time t_\\mathrm{f}, at which some prescribed value of \\theta(t_\\mathrm{f}) must be achieved, compute by the shooting method the needed value of the initial angle \\theta_\\mathrm{i}, while assuming the initial angular rate \\omega_\\mathrm{i} is zero.\n\n\nShow the code\nusing DifferentialEquations\nusing Roots\nusing Plots\n\nfunction demo_shoot_pendulum()\n    θfinal = -0.2;\n    tfinal = 3.5;\n    tspan = (0.0,tfinal)\n    tol = 1e-5\n    function pendulum!(dx,x,p,t)\n        g = 9.81\n        l = 1.0;\n        m = 1.0;\n        b = 0.1;\n        a₁ = g/l\n        a₂ = b/(m*l^2)\n        θ,ω = x\n        dx[1] = ω\n        dx[2] = -a₁*sin(θ) - a₂*ω\n    end\n    prob = ODEProblem(pendulum!,zeros(Float64,2),tspan)\n    function F(θ₀::Float64)\n        xinitial = [θ₀,0.0]\n        prob = remake(prob,u0=xinitial)\n        sol = solve(prob,Tsit5(),reltol=tol/10,abstol=tol/10)\n        return θfinal-sol[end][1]\n    end\n    θinitial = find_zero(F,(-pi,pi))    # Solving the equation F(θ)=0 using Roots package. In general can find more solutions.\n    xinitial = [θinitial,0.0]\n    prob = remake(prob,u0=xinitial)     # Already solved in F(), but we solve it again for plotting.\n    sol = solve(prob,Tsit5())\n    p1 = plot(sol,lw=2,xlabel=\"Time\",ylabel=\"Angle\",label=\"θ\",idxs=(1))\n    scatter!([tfinal],[θfinal],label=\"Required terminal θ\")\n    p2 = plot(sol,lw=2,xlabel=\"Time\",ylabel=\"Angular rate\",label=\"ω\",idxs=(2))\n    display(plot(p1,p2,layout=(2,1)))\nend\n\ndemo_shoot_pendulum()\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: State responses for a pendulum on a given time interval, with zero initial angular rate and the initial angle solved for numerically so that the final angle attains a give value\n\n\n\n\n\nA few general comments to the above code:\n\nThe function F(\\theta_\\mathrm{i}) that defines the nonlinear equation F(\\theta_\\mathrm{i})=0 calles a numerical solver for an IVP ODE. The latter solver then should have the numerical tolerances set more stringent than the former.\nThe ODE problem should only be defined once and then in each iteration its parameters should be updated. In Julia, this is done by the remake function, but it may be similar for other languages.\n\n\n\nShooting method for indirect approach to optimal control\nWe finally bring the method into the realm of indirect approach to optimal control – it is the initial value \\lambda_\\mathrm{i} of the costate variable that serves as an optimization variable, while the initial value x_\\mathrm{i} of the state variable is known and fixed. The final values of both the state and costate variables are the outcomes of numerical simulation obtained using a numerical solver for an IVP ODE. Based on these, the residual is computed. Either as e = x(t_\\mathrm{f})-x_\\mathrm{f} if the final state is fixed, or as e = \\lambda(t_\\mathrm{f}) - \\nabla \\phi(x(t_\\mathrm{f})) if the final state is free. Based on this residual, the initial value of the costate is updated and another iteration of the algorithm is entered.\n\n\n\n\n\n\nFigure 2: Indirect shooting\n\n\n\n\nExample 2 (Shooting for indirect approach to LQR) Standard LQR optimal control for a second-order system on a fixed finite interval with a fixed final state.\n\n\nShow the code\nusing LinearAlgebra\nusing DifferentialEquations\nusing NLsolve\n\nfunction shoot_lq_fixed(A,B,Q,R,xinitial,xfinal,tfinal)\n    n = size(A)[1]\n    function statecostateeq!(dw,w,p,t)\n        x = w[1:n]\n        λ = w[(n+1):end]\n        dw[1:n] = A*x - B*(R\\B'*λ)\n        dw[(n+1):end] = -Q*x - A'*λ\n    end\n    λinitial = zeros(n)\n    tspan = (0.0,tfinal)\n    tol = 1e-5\n    function F(λinitial)\n        winitial = vcat(xinitial,λinitial)\n        prob = ODEProblem(statecostateeq!,winitial,tspan)\n        dsol = solve(prob,Tsit5(),abstol=tol/10,reltol=tol/10)\n        xfinalsolved = dsol[end][1:n]\n        return (xfinal-xfinalsolved)\n    end\n    nsol = nlsolve(F,λinitial,xtol=tol)                 # Could add autodiff=:forward.\n    λinitial = nsol.zero                                # Solving once again for plotting.\n    winitial = vcat(xinitial,λinitial)\n    prob = ODEProblem(statecostateeq!,winitial,tspan)\n    dsol = solve(prob,Tsit5(),abstol=tol/10,reltol=tol/10)\n    return dsol\nend\n\nfunction demo_shoot_lq_fixed()\n    n = 2                                               # Order of the system.\n    m = 1                                               # Number of inputs.\n    A = rand(n,n)                                       # Matrices modeling the system.\n    B = rand(n,m)\n        \n    Q = diagm(0=&gt;rand(n))                               # Weighting matrices for the quadratic cost function.\n    R = rand(1,1)\n\n    xinitial = [1.0, 2.0]\n    xfinal = [3.0, 4.0]\n    tfinal = 5.0 \n\n    dsol = shoot_lq_fixed(A,B,Q,R,xinitial,xfinal,tfinal)\n\n    p1 = plot(dsol,idxs=(1:2),lw=2,legend=false,xlabel=\"Time\",ylabel=\"State\")\n    p2 = plot(dsol,idxs=(3:4),lw=2,legend=false,xlabel=\"Time\",ylabel=\"Costate\")\n    display(plot(p1,p2,layout=(2,1)))\nend\n\ndemo_shoot_lq_fixed()\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Shooting method applied to the indirect approach to LQR problem\n\n\n\n\n\n\n\n\nMultiple shooting methods\nThe key deficiency of the shooting method is that the only source of the error is the error in the initial condition, this error then amplifies as it propagates over the whole time interval as the numerical integration proceeds, and consequently the residual is very sensitive to tiny changes in the initial value. The multiple shooting method is a remedy for this. The idea is to divide the interval [t_\\mathrm{i},t_\\mathrm{f}] into N subintervals [t_k,t_{k+1}] and to introduce the values of the state and co-state variable at the beginning of each subinterval as additional variables. Additional equations are then introduced that enforce the continuity of the variable at the end of one subinterval and at the beginning of the next subinterval.\n\n\n\n\n\n\nFigure 4: Indirect multiple shooting\n\n\n\n\n\nDiscretization methods\nShooting methods take advantage of availability of solvers for IVP ODEs. These solvers produce discret(ized) trajectories, proceeding (integration) step by step, forward in time. But they do this in a way hidden from users. We just have to set the initial conditions (possibly through numerical optimization) and the solver does the rest.\nAlternatively, the formulas for the discrete-time updates are not evaluated one by one, step by step, running forward in time, but are assembled to form a system of equations, in general nolinear ones. Appropriate boundary conditions are then added to these nonlinear equations and the whole system is then solved numerically, yielding a discrete approximation of the trajectories satisfying the BVP.\nSince all those equatins are solved simultaneously (as a system of equations), there is no advantage in using explicit methods for solving ODEs, and implicit methods are used instead.\nIt is now time to recall some crucial results from the numerical methods for solving ODEs. First, we start with the popular single-step methods known as the Runge-Kutta (RK) methods.\nWe consider the standard ODE \\dot x(t) = f(x(t),t).\nand we define the Butcher tableau as \n  \\begin{array}{ l | c c c c }\n    c_1 & a_{11} & a_{12} & \\ldots & a_{1s}\\\\\n    c_2 & a_{21} & a_{22} & \\ldots & a_{2s}\\\\\n    \\vdots & \\vdots\\\\\n    c_s & a_{s1} & a_{s2} & \\ldots & a_{ss}\\\\\n    \\hline\n      & b_{1} & b_{2} & \\ldots & b_{s}\n  \\end{array}.\n such that c_i = \\sum_{j=1}^s a_{ij}, and 1 = \\sum_{j=1}^s b_{j}.\nReffering to the particular Butcher table, a single step of the method is \n  \\begin{aligned}\n  f_{k1} &= f(x_k + h_k \\left(a_{11}f_{k1}+a_{12}f_{k2} + \\ldots + a_{1s}f_{ks}),t_k+c_1h_k\\right)\\\\\n  f_{k2} &= f(x_k + h_k \\left(a_{21}f_{k1}+a_{22}f_{k2} + \\ldots + a_{2s}f_{ks}),t_k+c_2h_k\\right)\\\\\n  \\vdots\\\\\n  f_{ks} &= f(x_k + h_k \\left(a_{s1}f_{k1}+a_{s2}f_{k2} + \\ldots + a_{ss}f_{ks}),t_k+c_sh_k\\right)\\\\\n  x_{k+1} &= x_k + h_k \\left(b_1 f_{k1}+b_2f_{k2} + \\ldots + b_sf_{ks}\\right).\n  \\end{aligned}\n\nIf the matrix A is strictly lower triangular, that is, if a_{ij} = 0 for all i&lt;j , the method belongs to explicit Runge-Kutta methods, otherwise it belongs to implicit Runge-Kutta methods.\nA prominent example of explicit RK methods is the 4-stage RK method (oftentimes referred to as RK4).\n\nExplicit RK4 method\nThe Buttcher table for the method is \n      \\begin{array}{ l | c c c c }\n        0 & 0 & 0 & 0 & 0\\\\\n        1/2 & 1/2 & 0 & 0 & 0\\\\\n        1/2 & 0 & 1/2 & 0 & 0\\\\\n        1 & 0 & 0 & 1 & 0\\\\\n        \\hline\n          & 1/6 & 1/3 & 1/3 & 1/6\n      \\end{array}.\n\nFollowing the Butcher table, a single step of this method is \n      \\begin{aligned}\n      f_{k1} &= f(x_k,t_k)\\\\\n      f_{k2} &= f\\left(x_k + \\frac{h_k}{2}f_{k1},t_k+\\frac{h_k}{2}\\right)\\\\\n      f_{k3} &= f\\left(x_k + \\frac{h_k}{2}f_{k2},t_k+\\frac{h_k}{2}\\right)\\\\\n      f_{k4} &= f\\left(x_k + h_k f_{k3},t_k+h_k\\right)\\\\\n      x_{k+1} &= x_k + h_k \\left(\\frac{1}{6} f_{k1}+\\frac{1}{3}f_{k2} + \\frac{1}{3}f_{k3} + \\frac{1}{6}f_{k4}\\right)\n      \\end{aligned}.\n\nBut as we have just mentions, explicit methods are not particularly useful for solving BVPs. We prefer implicit methods. One of the simplest is the implicit midpoint method.\n\n\nImplicit midpoint method\nThe Butcher tableau is \n  \\begin{array}{ l | c r }\n    1/2 & 1/2 \\\\\n    \\hline\n      & 1\n  \\end{array}\n\nA single step is then \\begin{aligned}\n  f_{k1} &= f\\left(x_k+\\frac{1}{2}f_{k1} h_k, t_k+\\frac{1}{2}h_k\\right)\\\\\n  x_{k+1} &= x_k + h_k f_{k1}.\n  \\end{aligned}\nBut adding to the last equation x_k we get x_{k+1} + x_k = 2x_k + h_k f_{k1}.\nDividing by two we get \\frac{1}{2}(x_{k+1} + x_k) = x_k + \\frac{1}{2}h_k f_{k1} and then it follows that \\boxed{x_{k+1} = x_k + h_k f\\left(\\frac{1}{2}(x_k+x_{k+1}),t_k+\\frac{1}{2}h_k\\right).}\nThe right hand side of the last equation explains the “midpoint” in the name. It can be viewed as a rectangular approximation to the integral in x_{k+1} = x_k + \\int_{t_k}^{t_{k+1}} f(x(t),t)\\mathrm{d}t as the integral is computed as an area of a rectangle with the height determined by f() evaluated in the middle point.\nAlthough we do not explain the details here, let’s just note that it is the simplest of the collocation methods. In particular it belongs to Gauss (also Gauss-Legandre) methods.\n\n\nImplicit trapezoidal method\nThe method can be viewed both as a single-step (RK) method and a multi-step method. When viewed as an RK method, its Butcher table is \n  \\begin{array}{ l | c r }\n    0 & 0 & 0 \\\\\n    1 & 1/2 & 1/2 \\\\\n    \\hline\n      & 1/2 & 1/2 \\\\\n  \\end{array}\n   Following the Butcher table, a single step of the method is then \n  \\begin{aligned}\n  f_{k1} &= f(x_k,t_k)\\\\\n  f_{k2} &= f(x_k + h_k \\frac{f_{k1}+f_{k2}}{2},t_k+h_k)\\\\\n  x_{k+1} &= x_k + h_k \\left(\\frac{1}{2} f_{k1}+\\frac{1}{2} f_{k2}\\right).\n  \\end{aligned}\n\nBut since the collocation points are identical with the nodes (grid/mesh points), we can relabel to \\begin{aligned}\n  f_{k} &= f(x_k,t_k)\\\\\n  f_{k+1} &= f(x_{k+1},t_{k+1})\\\\\n  x_{k+1} &= x_k + h_k \\left(\\frac{1}{2} f_{k}+\\frac{1}{2} f_{k+1}\\right).\n  \\end{aligned}\n\nThis possibility is a particular advantage of Lobatto and Radau methods, which contain both end points of the interval or just one of them among the collocation points. The two symbols f_k and f_{k+1} are really just shorthands for values of the function f at the beginning and the end of the integration interval, so the first two equations of the triple above are not really equations to be solved but rather definitions. And we can assemble it all into just one equation \\boxed{\n      x_{k+1} = x_k + h_k \\frac{f(x_k,t_k)+f(x_{k+1},t_{k+1})}{2}.\n      }\n\nThe right hand side of the last equation explains the “trapezoidal” in the name. It can be viewed as a trapezoidal approximation to the integral in x_{k+1} = x_k + \\int_{t_k}^{t_{k+1}} f(x(t),t)\\mathrm{d}t as the integral is computed as an area of a trapezoid.\nWhen it comes to building a system of equations within transcription methods, we typically move all the terms just on one side to obtain the defect equations x_{k+1} - x_k - h_k \\left(\\frac{1}{2} f(x_k,t_k)+\\frac{1}{2} f(x_{k+1},t_{k+1})\\right) = 0.\n\n\nHermite-Simpson method\nIt belongs to the family of Lobatto III methods, namely it is a 3-stage Lobatto IIIA method. Butcher tableau \n  \\begin{array}{ l | c c c c }\n    0 & 0 &0 & 0\\\\\n    1/2 & 5/24 & 1/3 & -1/24\\\\\n    1 & 1/6 & 2/3 & 1/6\\\\\n    \\hline\n      & 1/6 & 2/3 & 1/6\n  \\end{array}\n\nHermite-Simpson method can actually come in three forms (this is from Betts (2020)):\n\nPrimary form\nThere are two equations for the given integration interval [t_k,t_{k+1}] x_{k+1} = x_k + h_k \\left(\\frac{1}{6}f_k + \\frac{2}{3}f_{k2} + \\frac{1}{6}f_{k+1}\\right), x_{k2} = x_k + h_k \\left(\\frac{5}{24}f_k + \\frac{1}{3}f_{k2} - \\frac{1}{24}f_{k+1}\\right), where the f symbols are just shorthand notations for values of the function at a certain point f_k = f(x_k,u(t_k),t_k), f_{k2} = f(x_{k2},u(t_{k2}),t_{k2}), f_{k+1} = f(x_{k+1},u(t_{k+1}),t_{k+1}), and the off-grid time t_{k2} is given by t_{k2} = t_k + \\frac{1}{2}h_k.\nThe first of the two equations can be recognized as Simpson’s rule for computing a definite integral. Note that while considering the right hand sides as functions of the control inputs, we also correctly express at which time (the collocation time) we consider the value of the control variable.\nBeing this general allows considering general control inputs and not only piecewise constant control inputs. For example, if we consider piecewise linear control inputs, then u(t_{k2}) = \\frac{u_k + u_{k+1}}{2}. But if we stick to the (more common) piecewise constant controls, not surprisingly u(t_{k2}) = u_k. Typically we format the equations as defect equations, that is, with zero on the right hand side \n\\begin{aligned}\nx_{k+1} - x_k - h_k \\left(\\frac{1}{6}f_k + \\frac{2}{3}f_{k2} + \\frac{1}{6}f_{k+1}\\right) &= 0,\\\\\nx_{k2} - x_k - h_k \\left(\\frac{5}{24}f_k + \\frac{1}{3}f_{k2} - \\frac{1}{24}f_{k+1}\\right) &= 0.\n\\end{aligned}\n\nThe optimization variables for every integration interval are x_k,u_k,x_{k2}, u_{k2}.\n\n\nHermite-Simpson Separated (HSS) method\nAlternatively, we can express f_{k2} in the first equation as a function of the remaining terms and then substitute to the second equation. This will transform the second equation such that only the terms indexed with k and k+1 are present. \n\\begin{aligned}\nx_{k+1} - x_k - h_k \\left(\\frac{1}{6}f_k + \\frac{2}{3}f_{k2} + \\frac{1}{6}f_{k+1}\\right) &= 0,\\\\\nx_{k2} - \\frac{x_k + x_{k+1}}{2} - \\frac{h_k}{8} \\left(f_k - f_{k+1}\\right) &= 0.\n\\end{aligned}\n\nWhile we already know (from some paragraph above) that the first equation is Simpson’s rule, the second equation is an outcome of Hermite intepolation. Hence the name. The optimization variables for every integration interval are the same as before, that is, x_k,u_k,x_{k2}, u_{k2} .\n\n\nHermite-Simpson Condensed (HSC) method\nYet some more simplification can be obtained from HSS. Namely, the second equation can be actually used to directly prescribe x_{k2} x_{k2} = \\frac{x_k + x_{k+1}}{2} + \\frac{h_k}{8} \\left(f_k - f_{k+1}\\right), which is used in the first equation as an argument for the f() function (represented by the f_{k2} symbol), by which the second equation and the term x_{k2} are eliminated from the set of defect equations. The optimization variables for every integration interval still need to contain u_{k2} even though x_{k2} was eliminated, because it is needed to parameterize f_{k2} . That is, the optimization variables then are x_k,u_k, u_{k2} . Reportedly (by Betts) this has been widely used and historically one of the first methods. When it comes to using it in optimal control, it turns out, however, that the sparsity pattern is better for the HSS.\n\n\n\n\nCollocation methods\nYet another family of methods for solving BVP ODE \\dot x(t) = f(x(t),t) are collocation methods. They are also based on discretization of independent variable – the time t. That is, on the interval [t_\\mathrm{i}, t_\\mathrm{f}], discretization points (or grid points or nodes or knots) are chosen, say, t_0, t_1, \\ldots, t_N, where t_0 = t_\\mathrm{i} and t_N = t_\\mathrm{f}. The solution x(t) is then approximated by a polynomial p_k(t) of a certain degree s on each interval [t_k,t_{k+1}] of length h_k=t_{k+1}-t_k\np_k(t) = p_{k0} + p_{k1}(t-t_k) + p_{k2}(t-t_k)^2+\\ldots + p_{ks}(t-t_k)^s.\nThe degree of the polynomial is low, say s=3 or so, certainly well below 10. With N subintervals, the total number of coefficients to parameterize the (approximation of the) solution x(t) over the whole interval is then N(s+1). For example, for s=3 and N=10, we have 40 coefficients: p_{00}, p_{01}, p_{02}, p_{03}, p_{10}, p_{11}, p_{12}, p_{13},\\ldots, p_{90}, p_{91}, p_{92}, p_{93}.\n\n\n\n\n\n\nFigure 5: Indirect collocation\n\n\n\nFinding a solution amounts to determining all those coefficients. Once we have them, the (approximate) solution is given by a piecewise polynomial.\nHow to determine the coefficients? By interpolation. But we will see in a moment that two types of interpolation are needed – interpolation of the value of the solution and interpolation of the derivative of the solution.\nThe former is only performed at the beginning of each interval, that is, at every discretization point (or grid point or node or knot). The condition reads that the polynomial p_{k-1}(t) approximating the solution x(t) on the (k-1)th interval should attain the same value at the end of that interval, that is, at t_{k-1} + h_{k-1}, as the polynomial p_k(t) approximating the solution x(t) on the kth interval attains at the same point, which from its perspective is the beginning of the kth interval, that is, t_k. We express this condition formally as \\boxed{p_{k-1}(\\underbrace{t_{k-1}+h_{k-1}}_{t_{k}}) = p_k(t_k).}\nExpanding the two polynomials, we get p_{k-1,0} + p_{k-1,1}h_{k-1} + p_{k-1,2}h_{k-1}^2+\\ldots + p_{k-1,s}h_{k-1}^s = p_{k0}.\n\n\n\n\n\n\nSubscripts in the coefficients\n\n\n\nWe adopt the notational convention that a coefficient of a polynomial is indexed by two indices, the first one indicating the interval and the second one indicating the power of the corresponding term. For example, p_{k-1,2} is the coefficient of the quadratic term in the polynomial approximating the solution on the (k-1)th interval. For the sake of brevity, we omit the comma between the two subscripts in the cases such as p_{k1} (instead of writing p_{k,1}). But we do write p_{k-1,0}, because here omiting the comma would introduce ambiguity.\n\n\nGood, we have one condition (one equation) for each subinterval. But we need more, if polynomials of degree at least one are considered (we then parameterize them by two parameters, in which case one more equation is needed for each subinterval). Here comes the opportunity for the other kind of interpolation – interpolation of the derivative of the solution. At a given point (or points) that we call collocation points, the polynomial p_k(t) approximating the solution x(t) on the kth interval should satisfy the same differential equation \\dot x(t) = f(x(t),t) as the solution does. That is, we require that at\nt_{kj} = t_k + h_k c_{j}, \\quad j=1,\\ldots, s, which we call collocation points, the polynomial satisfies \\boxed{\\dot p_k(t_{kj}) = f(p_k(t_{kj}),t_{kj}), \\quad j=1,\\ldots, s.}\nExpressing the derivative of the polynomial on the left and expanding the polynomial itself on the right, we get \n\\begin{aligned}\np_{k1} + &2p_{k2}(t_{kj}-t_k)+\\ldots + s p_{ks}(t_{kj}-t_k)^{s-1} = \\\\ &f(p_{k0} + p_{k1}(t_{kj}-t_k) + p_{ks}(t_{kj}-t_k)^2 + \\ldots + p_{ks}(t_{kj}-t_k)^s), \\quad j=1,\\ldots, s.\n\\end{aligned}\n\nThis gives us the complete set of equations for each interval. For the considered example of a cubic polynomial, we have one interpolation condition at the beginning of the interval and then three collocation conditions at the collocation points. In total, we have four equations for each interval. The number of equations is equal to the number of coefficients of the polynomial. Before the system of equations can solved for the coefficients, we must specifies the collocation points. Based on these, the collocation methods split into three families:\n\nGauss or Gauss-Legendre methods – the collocation points are strictly inside each interval.\nLobatto methods – the collocation points include also both ends of each interval.\nRadau methods – the collocation points include just one end of the interval.\n\n\n\n\n\n\n\nFigure 6: Single (sub)interval in indirect collocation – a cubic polynomial calls for three collocation points, two of which coincide with the discretization points (discrete-times); the continuity is enforced at the discretization point at the beginning of the interval\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAlthough in principle the collocation points could be arbitrary (but distinct), within a given family of methods, and for a given number of collocation points, some clever options are known that maximize accuracy.\n\n\n\nLinear polynomials\nBesides the piecewise constant approximation, which is too crude, not to speak of the discontinuity it introduces, the next simplest approximation of a solution x(t) on the interval [t_k,t_{k+1}] of length h_k=t_{k+1}-t_k is a linear (actually affine) polynomial p_k(t) = p_{k0} + p_{k1}(t-t_k).\nOn the given kth interval it is parameterized by two parameters p_{k0} and p_{k1}, hence two equations are needed. The first equation enforces the continuity at the beginning of the interval \\boxed\n{p_{k-1,0} + p_{k-1,1}h_{k-1} = p_{k0}.}\n\nThe remaining single equation is the collocation condition at a single collocation point t_{k1} = t_k + h_k c_1, which remains to be chosen. One possible choice is c_1 = 1/2, that is \nt_{k1} = t_k + \\frac{h_k}{2}\n\nIn words, the collocation point is chosen in the middle of the interval. The collocation condition then reads \\boxed\n{p_{k1} = f\\left(p_{k0} + p_{k1}\\frac{h_k}{2}\\right).}\n\n\n\nQuadratic polynomials\nIf a quadratic polynomial is used to approximate the solution, the condition at the beginning of the interval is \\boxed\n{p_{k-1,0} + p_{k-1,1}h_{k-1} + p_{k-1,2}h_{k-1}^2 = p_{k0}.}\n\nTwo more equations – collocation conditions – are needed to specify all the three coefficients that parameterize the aproximating polynomial on a given interval [t_k,t_{k+1}]. One intuitive (and actually clever) choice is to place the collocation points at the beginning and the end of the interval, that is, at t_k and t_{k+1}. The coefficient that parameterize the relative position of the collocation points with respect to the interval are c_1=0 and c_2=1 The collocation conditions then read \\boxed\n{\\begin{aligned}\np_{k1}  &= f(p_{k0}),\\\\\np_{k1} + 2p_{k2}h_{k} &= f(p_{k0} + p_{k1}h_k + p_{k2}h_k^2).\n\\end{aligned}}\n\n\n\nCubic polynomials\nWhen a cubic polynomial is used, the condition at the beginning of the kth interval is \\boxed\n{p_{k-1,0} + p_{k-1,1}h_{k-1} + p_{k-1,2}h_{k-1}^2+p_{k-1,3}h_{k-1}^3 = p_{k0}.}\n\nThree more equations are needed to determine all the four coefficients of the polynomial. Where to place the collocations points? One intuitive (and clever too) option is to place them at the beginning, in the middle, and at the end of the interval. The relative positions of the collocation points are then given by c_1=0, c_2=1/2, and c_3=1. The collocation conditions then read \\boxed\n{\\begin{aligned}\np_{k1} &= f\\left(p_{k0} + p_{k1}(t_{k1}-t_k) + p_{k2}(t_{k1}-t_k)^2 + p_{k3}(t_{k1}-t_k)^3\\right),\\\\\np_{k1} + 2p_{k2}\\frac{h_k}{2} + 3 p_{k3}\\left(\\frac{h_k}{2}\\right)^{2} &= f\\left(p_{k0} + p_{k1}\\frac{h_k}{2} + p_{k2}\\left(\\frac{h_k}{2}\\right)^2 + p_{k3}\\left(\\frac{h_k}{2} \\right)^3\\right),\\\\\np_{k1} + 2p_{k2}h_k + 3 p_{k3}h_k^{2} &= f\\left(p_{k0} + p_{k1}h_k + p_{k2}h_k^2 + p_{k3}h_k^3\\right).\n\\end{aligned}}\n\n\n\n\nCollocation methods are implicit Runge-Kutta methods\nAn important observation that we are goint to make is that collocation methods can be viewed as implicit Runge-Kutta methods. But not all IRK methods can be viewed as collocation methods. In this section we show that the three implicit RK methods that we covered above are indeed (equivalent to) collocation methods. By the equivalence we mean that there is a linear relationship between the coefficients of the polynomials that approximate the solution on a given (sub)interval and the solution at the discretization point together with the derivative of the solution at the collocation points.\n\nImplicit midpoint method as a Radau collocation method\nFor the given integration interval [t_k,t_{k+1}], we write down two equations that relate the two coefficients of the linear polynomial p_k(t) = p_{k0} + p_{k1}(t-t_k) and an approximation x_k of x(t) at the beginning of the interval t_k, as well as an approximation of \\dot x(t) at the (single) collocation point t_{k1} = t_{k} + \\frac{h_k}{2}.\nIn particular, the first interpolation condition is p_k(t_k) = \\textcolor{red}{p_{k0} = x_k} \\approx x(t_k).\nThe second interpolation condition, the one on the derivative in the middle of the interval is \\dot p_k\\left(t_k + \\frac{h_k}{2}\\right) = \\textcolor{red}{p_{k1} = f(x_{k1},t_{k1})} \\approx f(x(t_{k1}),t_{k1}).\nNote that here we introduced yet another unknown – the approximation x_{k1} of x(t_{k1}) at the collocation point t_{k1}. We can write it using the polynomial p_k(t) as \nx_{k1} = p_k\\left(t_k + \\frac{h_k}{2}\\right) = p_{k0} + p_{k1}\\frac{h_k}{2}.\n\nSubstituting for p_{k0} and p_{k1}, we get \nx_{k1} = x_k + f(x_{k1},t_{k1})\\frac{h_k}{2}.\n\nWe also introduce the notation f_{k1} for f(x_{k1},t_{k1}) and we can write an equation \nf_{k1} = f\\left(x_k + f_{k1}\\frac{h_k}{2}\\right).\n\nBut we want to find x_{k+1}, which we can accomplish by evaluating the polynomial p_k(t) at t_{k+1} = t_k+h_k \nx_{k+1} = x_k + f_{k1}h_k.\n\nCollecting the last two equations, we rederived the good old friend – the implicit midpoint method.\n\n\nImplicit trapezoidal method as a Lobatto collocation method\nFor the given integration interval [t_k,t_{k+1}], we write down three equations that relate the three coefficients of the quadratic polynomial p_k(t) = p_{k0} + p_{k1}(t-t_k) + p_{k2}(t-t_k)^2 and an approximation x_k of x(t) at the beginning of the interval t_k, as well as approximations to \\dot x(t) at the two collocations points t_k and t_{k+1}.\nIn particular, the first interpolation condition is p_k(t_k) = \\textcolor{red}{p_{k0} = x_k} \\approx x(t_k).\nThe second interpolation condition, the one on the derivative at the beginning of the interval, the first collocation point, is \\dot p_k(t_k) = \\textcolor{red}{p_{k1} = f(x_k,t_k)} \\approx f(x(t_k),t_k).\nThe third interpolation condition, the one on the derivative at the second collocation point \\dot p_k(t_k+h_k) = \\textcolor{red}{p_{k1} + 2p_{k2} h_k = f(x_{k+1},t_{k+1})} \\approx f(x(t_{k+1}),t_{k+1}).\nAll the three conditions (emphasized in color above) can be written together as \n      \\begin{bmatrix}\n      1 & 0 & 0\\\\\n      0 & 1 & 0\\\\\n      0 & 1 & 2 h_k\\\\\n      \\end{bmatrix}\n      \\begin{bmatrix}\n      p_{k0} \\\\ p_{k1} \\\\ p_{k2}\n      \\end{bmatrix}\n      =\n      \\begin{bmatrix}\n      x_{k} \\\\ f(x_k,t_k) \\\\ f(x_{k+1},t_{k+1})\n      \\end{bmatrix}.\n\nThe above system of linear equations can be solved by inverting the matrix \n      \\begin{bmatrix}\n      p_{k0} \\\\ p_{k1} \\\\ p_{k2}\n      \\end{bmatrix}\n      =\n      \\begin{bmatrix}\n      1 & 0 & 0\\\\\n      0 & 1 & 0\\\\\n      0 & -\\frac{1}{2h_k} & \\frac{1}{2h_k}\\\\\n      \\end{bmatrix}\n      \\begin{bmatrix}\n      x_{k} \\\\ f(x_k,t_k) \\\\ f(x_{k+1},t_{k+1})\n      \\end{bmatrix}.\n\nWe can now write down the interpolating/approximating polynomial p_k(t) = x_{k} + f(x_{k},t_{k})(t-t_k) +\\left[-\\frac{1}{2h_k}f(x_{k},t_{k}) + \\frac{1}{2h_k}f(x_{k+1},t_{k+1})\\right](t-t_k)^2.\nThis polynomial can now be used to find an (approximation of the) value of the solution at the end of the interval x_{k+1} = p_k(t_k+h_k) = x_{k} + f(x_{k},t_{k})h_k +\\left[-\\frac{1}{2h_k}f(x_{k},t_{k}) + \\frac{1}{2h_k}f(x_{k+1},t_{k+1})\\right]h_k^2, which can be simplified nearly upon inspection to x_{k+1} = x_{k} + \\frac{f(x_{k},t_{k}) + f(x_{k+1},t_{k+1})}{2} h_k, but this is our good old friend, isn’t it? We have shown that the collocation method with a quadratic polynomial with the collocation points chosen at the beginning and the end of the interval is (equivalent to) the implicit trapezoidal method. The method belongs to the family of Lobatto IIIA methods, which are all known to be collocation methods.\n\n\nHermite-Simpson method as a Lobatto collocation method\nHere we show that Hermite-Simpson method also qualifies as a collocation method. In particular, it belongs to the family of Lobatto IIIA methods, similarly as implicit trapezoidal method. The first condition, the one on the value of the cubic polynomial p_k(t) = p_{k0} + p_{k1}(t-t_k) + p_{k2}(t-t_k)^2+ p_{k3}(t-t_k)^3 at the beginning of the interval is p_k(t_k) = \\textcolor{red}{p_{k0} = x_k} \\approx x(t_k).\nThe three remaining conditions are imposed at the collocation points, which for the integration interval [t_k,t_{k+1}] are t_{k1} = t_k , t_{k2} = \\frac{t_k+t_{k+1}}{2} , and t_{k3} = t_{k+1}. With the first derivative of the polynomial given by \\dot p_k(t) = p_{k1} + 2p_{k2}(t-t_k) + 3p_{k3}(t-t_k)^2, the first collocation condition \\dot p_k(t_k) = \\textcolor{red}{p_{k1} = f(x_k,t_k)} \\approx f(x(t_k),t_k).\nThe second collocation condition – the one on the derivative in the middle of the interval – is \\dot p_k\\left(t_k+\\frac{1}{2}h_k\\right) = \\textcolor{red}{p_{k1} + 2p_{k2} \\frac{h_k}{2} + 3p_{k3} \\left(\\frac{h_k}{2}\\right)^2 = f(x_{k2},t_{k2})} \\approx f\\left(x\\left(t_{k}+\\frac{h_k}{2}\\right),t_{k}+\\frac{h_k}{2}\\right).\nThe color-emphasized part can be simplified to \\textcolor{red}{p_{k1} + p_{k2} h_k + \\frac{3}{4}p_{k3} h_k^2 = f(x_{k2},t_{k2})}.\nFinally, the third collocation condition – the one imposed at the end of the interval – is \\dot p_k(t_k+h_k) = \\textcolor{red}{p_{k1} + 2p_{k2} h_k + 3p_{k3} h_k^2 = f(x_{k+1},t_{k+1})} \\approx f(x(t_{k+1}),t_{k+1}).\nAll the four conditions (emphasized in color above) can be written together as \n      \\begin{bmatrix}\n      1 & 0 & 0 & 0\\\\\n      0 & 1 & 0 & 0\\\\\n      0 & 1 & h_k & \\frac{3}{4} h_k^2\\\\\n      0 & 1 & 2 h_k & 3h_k^2\\\\\n      \\end{bmatrix}\n      \\begin{bmatrix}\n      p_{k0} \\\\ p_{k1} \\\\ p_{k2} \\\\p_{k3}\n      \\end{bmatrix}\n      =\n      \\begin{bmatrix}\n      x_{k} \\\\ f(x_k,t_k) \\\\ f(x_{k2},t_{k2}) \\\\ f(x_{k+1},t_{k+1}).\n      \\end{bmatrix}\n\nInverting the matrix analytically, we get \n      \\begin{bmatrix}\n      p_{k0} \\\\ p_{k1} \\\\ p_{k2}\\\\ p_{k3}\n      \\end{bmatrix}\n      =\n      \\begin{bmatrix}\n      1 & 0 & 0 & 0\\\\\n      0 & 1 & 0 & 0\\\\\n      0 & -\\frac{3}{2h_k} & \\frac{2}{h_k} & -\\frac{1}{2h_k}\\\\\n      0 & \\frac{2}{3h_k^2} & -\\frac{4}{3h_k^2} & \\frac{2}{3h_k^2}\n      \\end{bmatrix}\n      \\begin{bmatrix}\n      x_{k} \\\\ f(x_k,t_k) \\\\ f(x_{k2},t_{k2})\\\\ f(x_{k+1},t_{k+1}).\n      \\end{bmatrix}.\n\nWe can now write down the interpolating/approximating polynomial \n      \\begin{aligned}\n      p_k(t) &= x_{k} + f(x_{k},t_{k})(t-t_k) +\\left[-\\frac{3}{2h_k}f(x_{k},t_{k}) + \\frac{2}{h_k}f(x_{k2},t_{k2}) -\\frac{1}{2h_k}f(x_{k+1},t_{k+1}) \\right](t-t_k)^2\\\\\n      & +\\left[\\frac{2}{3h_k^2}f(x_{k},t_{k}) - \\frac{4}{3h_k^2}f(x_{k2},t_{k2}) +\\frac{2}{3h_k^2}f(x_{k+1},t_{k+1}) \\right](t-t_k)^3.\n      \\end{aligned}\n\nWe can use this prescription of the polynomial p_k(t) to compute the (approximation of the) value of the solution at the end of the kth interval \n      \\begin{aligned}\n      x_{k+1} = p_k(t_k+h_k) &= x_{k} + f(x_{k},t_{k})h_k +\\left[-\\frac{3}{2h_k}f(x_{k},t_{k}) + \\frac{2}{h_k}f(x_{k2},t_{k2}) -\\frac{1}{2h_k}f(x_{k+1},t_{k+1}) \\right]h_k^2\\\\\n      & +\\left[\\frac{2}{3h_k^2}f(x_{k},t_{k}) - \\frac{4}{3h_k^2}f(x_{k2},t_{k2}) +\\frac{2}{3h_k^2}f(x_{k+1},t_{k+1}) \\right]h_k^3,\n      \\end{aligned}\n which can be simplified to \n      \\begin{aligned}\n      x_{k+1} &= x_{k} + f(x_{k},t_{k})h_k +\\left[-\\frac{3}{2}f(x_{k},t_{k}) + \\frac{2}{1}f(x_{k2},t_{k2}) -\\frac{1}{2}f(x_{k+1},t_{k+1}) \\right]h_k\\\\\n      & +\\left[\\frac{2}{3}f(x_{k},t_{k}) - \\frac{4}{3}f(x_{k2},t_{k2}) +\\frac{2}{3}f(x_{k+1},t_{k+1}) \\right]h_k,\n      \\end{aligned}\n which further simplifies to \n      x_{k+1}  = x_{k} + h_k\\left[\\frac{1}{6}f(x_{k},t_{k}) + \\frac{2}{3}f(x_{k2},t_{k2}) + \\frac{1}{6}f(x_{k+1},t_{k+1}) \\right],\n which can be recognized as the Simpson integration that we have already seen in implicit Runge-Kutta method described above.\nObviously f_{k2} needs to be further elaborated on, namely, x_{k2} needs some prescription too. We know that it was introduced as an approximation to the solution x in the middle of the interval. Since the value of the polynomial in the middle is such an approximation too, we can set x_{k2} equal to the value of the polynomial in the middle. \n      \\begin{aligned}\n      x_{k2} = p_k\\left(t_k+\\frac{1}{2}h_k\\right) &= x_{k} + f(x_{k},t_{k})\\frac{h_k}{2} +\\left[-\\frac{3}{2h_k}f(x_{k},t_{k}) + \\frac{2}{h_k}f(x_{k2},t_{k2}) -\\frac{1}{2h_k}f(x_{k+1},t_{k+1}) \\right]\\left(\\frac{h_k}{2}\\right)^2\\\\\n      & +\\left[\\frac{2}{3h_k^2}f(x_{k},t_{k}) - \\frac{4}{3h_k^2}f(x_{k2},t_{k2}) +\\frac{2}{3h_k^2}f(x_{k+1},t_{k+1}) \\right]\\left(\\frac{h_k}{2}\\right)^3,\n      \\end{aligned}\n which without further ado simplifies to \n      x_{k2} = x_{k} + h_k\\left( \\frac{5}{24}f(x_{k},t_{k}) +\\frac{1}{3}f(x_{k2},t_{k2}) -\\frac{1}{24}f(x_{k+1},t_{k+1}) \\right),\n which can be recognized as the other equation in the primary formulation of Implicit trapezoidal method described above.\n\n\n\nPseudospectral collocation methods\nThey only consider a single polynomial over the whole interval. The degree of such polynomial, in contrast with classical collocation methods, rather high, therefore also the number of collocation points is high, but their location is crucial.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "Numerical methods for indirect approach"
    ]
  },
  {
    "objectID": "rocond_mu_synthesis.html",
    "href": "rocond_mu_synthesis.html",
    "title": "Mu synthesis",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "12. Robust control",
      "Mu synthesis"
    ]
  },
  {
    "objectID": "rocond_software.html",
    "href": "rocond_software.html",
    "title": "Software",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "12. Robust control",
      "Software"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "B(E)3M35ORR – Optimal and Robust Control",
    "section": "",
    "text": "This website constitutes the (work-in-progress) lecture notes for the graduate course Optimal and Robust Control (B3M35ORR, BE3M35ORR) taught within Cybernetics and Robotics graduate program at Faculty of Electrical Engineering, Czech Technical University in Prague, Czechia.\nEach chapter corresponds to a single weekly block (covered by a lecture, a seminar/exercise, and some homework), hence 14 chapters in total.\nOrganizational instructions, description of grading policy, assignments of homework problems and other course related material relevant for officially enrolled students are published on the course page within the FEL Moodle system.\n\n\n\n Back to top"
  },
  {
    "objectID": "cont_indir_trajectory_stabilization.html",
    "href": "cont_indir_trajectory_stabilization.html",
    "title": "Trajectory stabilization and neigboring extremals",
    "section": "",
    "text": "Indirect methods for optimal control reformulate the optimal control problem into a set of equtions – boundary value problems with differential and algebraic equations in the case of continuous-time systems – and by solving these (typically numerically) we obtain the optimal state and control trajectories. Practical usefullness of these is rather limited as such optimal control trajectory constitutes an open-loop control – there is certainly no need to advocate the importance of feedback in this advanced control course.\nOne way to introduce feedback is to regard the computed optimal state trajectory \\bm x^\\star(t) as a reference trajectory and design a feedback controller to track this reference. To our advantage, we already have the corresponding control trajectory \\bm u^\\star(t) too, and theferore we can formulate such reference tracking problem as a problem of regulating the deviation \\delta \\bm x(t) of the state from its reference by means of superposing a feedback control \\delta \\bm u(t) onto the (open-loop) optimal control.\nWhile this problem – also known as the problem of stabilization of a (reference) trajectory – can be solved by basically any feedback control scheme, one elegant way is to linearize the system around the reference trajectory and formulate the problem as the LQR problem for a time-varying linear system.\n\n\n\n\n\n\nLinearization around a trajectory\n\n\n\nDon’t forget that when linearizing a nonlinear system \\dot{\\bm x} = \\mathbf f(\\bm x,\\bm u) around a point that is not equilibrium – and this inevitably happens when linearizing along the state trajectory \\bm x^\\star(t) obtained from indirect approach to optimal control – the linearized system \\frac{\\mathrm d}{\\mathrm d t} \\delta \\bm x= \\mathbf A(t) \\delta \\bm x + \\mathbf B(t) \\delta \\bm u considers not only the state variables but also the control variables as increments \\delta \\bm x(t) and \\delta \\bm u(t) that must be added to the nominal values \\bm x^\\star(t) and \\bm u^\\star(t) of the state and control variables determining the operating point. That is, \\bm x(t) = \\bm x^\\star(t) + \\delta \\bm x(t) and \\bm u(t) = \\bm u^\\star(t) + \\delta \\bm u(t).\n\n\nHaving decided on an LQR framework, we can now come up with the three matrices \\mathbf Q, \\mathbf R and \\mathbf S that set the quadratic cost function. Once this choice is made, we can just invoke the solver for continuous-time Riccati equation with the ultimate goal of finding the time-varying state feedback gain \\mathbf K(t).\n\n\n\n\n\n\nLQR for trajectory stabilization cab be done in discrete time\n\n\n\nIf discrete-time feedback control is eventually desired, which it mostly is, the whole LQR design for a time-varying linear system will have to be done using just periodically sampled state and control trajectories and applying recursive formulas for the discrete-time Riccati equation and state feedback gain.\n\n\nThe three weighting matrices \\mathbf Q, \\mathbf R and \\mathbf S, if chosen arbitrarily, are not related to the original cost function that is minimized by the optimal state and control trajectories. The matrices just parameterize a new optimal control problem. It turns out, however, that there is a clever (and insightful) way of choosing these matrices so that the trajectory stabilization problem inherits the original cost function. In other words, even when the system fails to stay on the optimal trajectory perfectly, the LQR state-feedback controller will keep minimizing the same cost function when regulating the deviation from the optimal trajectory.\nRecall that using the conventional definition of Hamiltonian H(\\bm x, \\bm u, \\bm \\lambda) = L(\\bm x, \\bm u) + \\bm \\lambda^\\top \\mathbf f(\\bm x, \\bm u), in which we now assume time invariance of both the system and the cost function for notational simplicity, the necessary conditions of optimality are \n\\begin{aligned}\n\\dot{\\bm x} &= \\nabla_{\\bm\\lambda} H(\\bm x,\\bm u,\\boldsymbol \\lambda)  = \\mathbf f(\\bm x, \\bm u), \\\\\n\\dot{\\bm \\lambda} &= -\\nabla_{\\bm x} H(\\bm x,\\bm u,\\boldsymbol \\lambda), \\\\\n\\mathbf 0 &= \\nabla_{\\bm u} H(\\bm x,\\bm u,\\boldsymbol \\lambda),\\\\\n\\bm x(t_\\mathrm{i})&=\\mathbf x_\\mathrm{i},\\\\\n\\bm x(t_\\mathrm{f})&=\\mathbf x_\\mathrm{f} \\quad \\text{or}\\quad \\bm \\lambda(t_\\mathrm{f})=\\nabla\\phi(\\bm{x}(t_\\mathrm{f})),\n\\end{aligned}\n where the option on the last line is selected based on whether the state at the final time is fixed or free.\n\n\n\n\n\n\nThe state at the final time can also be restricted by a linear equation\n\n\n\nThe conditions of optimality stated above correspond to one of the two standard situations, in which the state in the final time is either fixed to a single value or completely free. The conditions can also be modified to consider the more general situation, in which the state at the final time is restricted to lie on a manifold defined by an equality constraint \\psi(\\bm x(t_\\mathrm{f})) = 0.\n\n\nLet’s now consider some tiny perturbation to the initial state \\bm x(t_\\mathrm{i}) from its prescribed nominal value \\mathbf x_\\mathrm{i}. It will give rise to deviations all all the variables in the above equations from their nominal – optimal – trajectories. Assuming the deviations are small, linear model suffices to describe them. In other words, what we are now after is linearization of the above equations\n\n\\begin{aligned}\n\\delta \\dot{\\bm x} &= \\nabla_{\\bm x} \\mathbf f \\; \\delta \\bm x + \\nabla_{\\bm u} \\mathbf f \\; \\delta \\bm u, \\\\\n\\delta \\dot{\\bm \\lambda} &= -\\nabla^2_{\\bm{xx}} H \\; \\delta \\bm x -\\nabla^2_{\\bm{xu}} H \\; \\delta \\bm u -\\underbrace{\\nabla^2_{\\bm{x\\lambda}} H}_{(\\nabla_{\\bm x} \\mathbf f)^\\top} \\; \\delta \\bm \\lambda, \\\\\n\\mathbf 0 &= \\nabla^2_{\\bm{ux}} H \\; \\delta \\bm x + \\nabla^2_{\\bm{uu}} H \\; \\delta \\bm u + \\underbrace{\\nabla^2_{\\bm{u\\lambda}} H}_{(\\nabla_{\\bm u} \\mathbf f)^\\top} \\; \\delta \\bm \\lambda,\\\\\n\\delta \\bm x(t_\\mathrm{i}) &= \\text{specified},\\\\\n\\delta \\bm \\lambda(t_\\mathrm{f}) &= \\nabla^2_{\\bm{xx}}\\phi(\\mathbf{x}(t_\\mathrm{f}))\\; \\delta \\bm x(t_\\mathrm{f}).\n\\end{aligned}\n\\tag{1}\n\n\n\n\n\n\nNote on notation\n\n\n\nLet’s recall for convenience here that since \\mathbf f(\\bm x, \\bm u) is a vector function of a vector argument(s), \\nabla_{\\bm x} \\mathbf f is a matrix whose columns are gradients of individual components of \\mathbf f. Equivalently, (\\nabla_{\\bm x} \\mathbf f)^\\top stands for the Jacobian of the function \\mathbf f with respect to \\bm x. Similarly, \\nabla_{\\bm{xx}} H is the Hessian of \\mathbf f with respect to \\bm x. That is, it is a matrix composed of second derivatives. It is a symmetric matrix, hence no need to transpose it. Finally, the terms \\nabla_{\\bm{ux}} H and \\nabla_{\\bm{xu}} H are matrices containing mixed second derivatives.\n\n\nWith hindsight we relabel the individual terms in Equation 1 as \n\\begin{aligned}\n\\mathbf A(t) &\\coloneqq (\\nabla_{\\bm x} \\mathbf f)^\\top\\\\\n\\mathbf B(t) &\\coloneqq (\\nabla_{\\bm u} \\mathbf f)^\\top\\\\\n\\mathbf Q(t) &\\coloneqq \\nabla^2_{\\bm{xx}} H\\\\\n\\mathbf R(t) &\\coloneqq \\nabla^2_{\\bm{uu}} H\\\\\n\\mathbf N(t) &\\coloneqq \\nabla^2_{\\bm{xu}} H\\\\\n\\mathbf S_\\mathrm{f} &\\coloneqq \\nabla^2_{\\bm{xx}}\\phi(\\mathbf{x}(t_\\mathrm{f})).\n\\end{aligned}\n\nLet’s rewrite the perturbed necessary conditions of optimality using these new symbols\n\n\\begin{aligned}\n\\delta \\dot{\\bm x} &= \\mathbf A(t) \\; \\delta \\bm x + \\mathbf B(t) \\; \\delta \\bm u, \\\\\n\\delta \\dot{\\bm \\lambda} &= - \\mathbf Q(t) \\; \\delta \\bm x - \\mathbf N(t) \\; \\delta \\bm u - \\mathbf A^\\top (t)\\; \\delta \\bm \\lambda, \\\\\n\\mathbf 0 &= \\mathbf N(t) \\; \\delta \\bm x + \\mathbf R(t) \\; \\delta \\bm u + \\mathbf B^\\top (t) \\; \\delta \\bm \\lambda,\\\\\n\\delta \\bm x(t_\\mathrm{i}) &= \\text{specified},\\\\\n\\delta \\bm \\lambda(t_\\mathrm{f}) &= \\mathbf S_\\mathrm{f}    \\; \\delta \\bm x(t_\\mathrm{f}).\n\\end{aligned}\n\nAssuming that \\nabla^2_{\\bm{uu}} H is nonsingular, which can solve the third equation for \\bm u \n\\bm u = -\\nabla^2_{\\bm{uu}} H^{-1} \\left( \\nabla^2_{\\bm{ux}} H \\; \\delta \\bm x + (\\nabla_{\\bm u} \\mathbf f)^\\top \\; \\delta \\bm \\lambda\\right ).\n\n\n\n\n Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Trajectory stabilization and neigboring extremals"
    ]
  },
  {
    "objectID": "rocond_H_infinity_control.html",
    "href": "rocond_H_infinity_control.html",
    "title": "Hinfinity-optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "12. Robust control",
      "Hinfinity-optimal control"
    ]
  },
  {
    "objectID": "opt_algo_unconstrained.html",
    "href": "opt_algo_unconstrained.html",
    "title": "Algorithms for unconstrained optimization",
    "section": "",
    "text": "Our motivation for studying numerical algorithms for unconstrained optimization remains the same as when we studied the conditions of optimality for such unconstrained problems – such algorithms constitute building blocks for constrained optimization problems. Indeed, many algorithms for constrained problems are based on reformulating the constrained problem into an unconstrained one and then applying the algorithms studied in this section.\nIt may be useful to recapitulate our motivation for studying optimization algorithms in general – after all, there are dozens of commercial or free&open-source software tools for solving optimization problems. Why not just use them? There are two answers beyond the traditional “at a grad school we should understand what we are using”:\nThere is certainly no shortage of algorithms for unconstrained optimization. In this crash course we can cover only a few. But the few we cover here certainly form a solid theoretical basis and provide practically usable tools.\nOne possible way to classify the algorithms is based on whether they use derivatives of the objective functions or not. In this course, we only consider the former approaches as they leads to more efficient algorithms. For the latter methods, we can refer to the literature (the prominent example is Nelder-Mead method).\nAll the relevant methods are iterative. Based on what happens within each iteration, we can classify them into two categories:",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_unconstrained.html#descent-methods",
    "href": "opt_algo_unconstrained.html#descent-methods",
    "title": "Algorithms for unconstrained optimization",
    "section": "Descent methods",
    "text": "Descent methods\nThe obvious quality that the search direction needs to satisfy, is that the cost function decreses along it, at least locally (for a small step length).\n\nDefinition 1 (Descent direction) At the current iterate \\bm x_k, the direction \\bm d_k is called a descent direction if \n\\nabla f(\\bm x_k)^\\top \\bm d_k &lt; 0,\n that is, the directional derivative is negative along the direction \\bm d_k.\n\nThe product above is an inner product of the two vectors \\bm d_k and \\nabla f(\\mathbf x_k). Recall that it is defined as \n\\nabla f(\\bm x_k)^\\top \\bm d_k = \\|\\nabla f(\\bm x_k)\\| \\|\\bm d_k\\| \\cos \\theta,\n where \\theta is the angle between the gradient and the search direction. This condition has a nice geometric interpretation in a contour plot for an optimization in \\mathbb R^2. Consider the line tangent to the function countour at \\bm x_k. A descent direction must be in the other half-plane generated by the tangent line than into which the gradient \\nabla f(\\bm x_k) points.\nBeware that it is only guaranteed that the cost function is reduced if the length of the step is sufficently small. For longer steps the higher-order terms in the Taylor’s series approximation of the cost function can dominate.\nBefore we proceed to the question of which descent direction to choose and how to find it, we adress the question of how far to go along the chosen direction. This is the problem of line search.\n\nStep length determination (aka line search)\nNote that once the search direction has been fixed (whether we used the negative of the gradient or any other descent direction), the problem of finding the step length \\alpha_k is just a scalar optimization problem. It turns out, however, that besides finding the true minimum along the search directions, it is often sufficient to find the minimum only approximately, or not aiming at minimization at all and work with a fixed step length instead.\n\nFixed length of the step\nHere we give a guidance on the choice of the lenght of the step. But we need to introduce a useful concept first.\n\nDefinition 2 (L-smoothness) For a continuously differentiable function f, the gradient \\nabla f is said to be L-smooth if there exists a constant L&gt;0 such that \n\\|\\nabla f(x) - \\nabla f(y)\\| \\leq L \\|x-y\\|.\n\n\nNot that if the second derivatives exist, L is an upper bound on the norm of the Hessian \n\\|\\nabla^2 f\\|\\leq L.\n\nFor quadratic functions, L is the largest eigenvalue of the Hessian \nL = \\max_i \\lambda_i (\\mathbf Q).\n\nThe usefulness of the concept of L-smoothness is that it provides a quadratic function that serves as an upper bound for the original function. This is formulated as the following lemma.\n\nLemma 1 (Descent lemma) Consider an L-smooth function f. Then for any \\mathbf x_k and \\mathbf x_{k+1}, the following inequality holds \nf(\\mathbf x_{k+1}) \\leq  f(\\mathbf x_{k}) + \\nabla f(\\mathbf x_k)^\\top (\\mathbf x_{k-1}-\\mathbf x_{k}) + \\frac{L}{2}\\|\\mathbf x_{k-1}-\\mathbf x_{k}\\|^2\n\n\nWhat implication does the result have on the determination of the step length?\n\n\\alpha = \\frac{1}{L}\n\n\n\nExact line search\nA number of methos exist: bisection, golden section, Newton, As finding true minium in each iteration is often too computationally costly and hardly needed, we do not have them here. The only exception the Newton’s method, which for vector variables constitutes another descent method on its own and we cover it later.\n\nExample 1 Here we develop a solution for an exact minimization of a quadratic functions f(\\bm x) = \\frac{1}{2} \\bm x^\\top\\mathbf Q \\bm x + \\mathbf c^\\top \\bm x along a given direction. We show that it leads to a closed-form formula. Although not particularly useful in practice, it is a good exercise in understanding the problem of line search. Furthermore, we will use it later to demonstrate the behaviour of the steepest descent method. The problem is to \\operatorname*{minimize}_{\\alpha_k} f(\\bm x_k + \\alpha_k \\bm d_k). We express the cost as a function of the current iterate, the direction, and step length. \n\\begin{aligned}\nf(\\bm x_k + \\alpha_k \\bm d_k) &= \\frac{1}{2}(\\bm x_k + \\alpha_k\\bm d_k)^\\top\\mathbf Q (\\bm x_k + \\alpha_k\\bm d_k) +\\mathbf c^\\top(\\bm x_k + \\alpha_k\\bm d_k)\\\\\n&= \\frac{1}{2} \\bm x_k^\\top\\mathbf Q \\bm x_k + \\bm d_k^\\top\\mathbf Q\\bm x_k \\alpha_k + \\frac{1}{2} \\bm d_k^\\top\\mathbf Q\\bm d_k \\alpha_k^2+ \\mathbf c^\\top(\\bm x_k + \\alpha_k\\bm d_k).\n\\end{aligned}\n\nDifferentiating the function with respect to the length of the step, we get \n\\frac{\\mathrm{d}f(\\bm x_k + \\alpha_k\\bm d_k)}{\\mathrm{d}\\alpha_k} = \\bm d_k^\\top \\underbrace{(\\mathbf Q\\bm x_k + \\mathbf c)}_{\\nabla f(\\bm x_k)} + \\bm d_k^\\top\\mathbf Q\\bm d_k \\alpha_k.\n\nAnd now setting the derivative to zero, we find the optimal step length \n\\boxed{\n\\alpha_k = -\\frac{\\bm d_k^\\top \\nabla f(\\bm x_k)}{\\bm d_k^\\top\\mathbf Q\\bm d_k} = -\\frac{\\bm d_k^\\top (\\mathbf Q\\bm x_k + \\mathbf c)}{\\bm d_k^\\top\\mathbf Q\\bm d_k}.}\n\n\n\n\nApproximate line search – backtracking\nThere are several methods for approximate line search. Here we describe the backtracking algorithm, which is based on the sufficient decrease condition (also known as Armijo condition), which reads \nf(\\bm x_k+\\alpha_k\\bm d_k) - f(\\bm x_k) \\leq \\gamma \\alpha_k \\mathbf d^T \\nabla f(\\bm x_k),\n where \\gamma\\in(0,1), typically \\gamma is very small, say \\gamma = 10^{-4}.\nThe term on the right can be be viewed as a linear function of \\alpha_k. Its negative slope is a bit less steep than the directional derivative of the function f at \\bm x_k. The condition of sufficient decrease thus requires that the cost function (as a function of \\alpha_k) is below the graph of this linear function.\nNow, the backtracking algorithm is parameterized by three parameters: the initial step lenght \\alpha_0&gt;0, the typically very small \\gamma\\in(0,1) that parameterizes the Armijo condition, and yet another parameter \\beta\\in(0,1).\nThe k-th iteration of the algorithm goes like this: failure of the sufficient decrease condition for a given \\alpha_k, or, equivalently satisfaction of the condition \nf(\\bm x_k) - f(\\bm x_k+\\alpha_k\\bm d_k) &lt; -\\gamma \\alpha_k \\mathbf d^T \\nabla f(\\bm x_k)\n sends the algorithm into another reduction of \\alpha_k by \\alpha_k = \\beta\\alpha_k. A reasonable choice for \\beta is 0.5, which corresponds to halving the step length upon failure to decrease sufficiently.\nThe backtracking algorithm can be implemented as follows\n\n\nShow the code\nfunction backtracking_line_search(f, ∇fₖ, xₖ, dₖ; α₀=1.0, β=0.5, γ=0.1)\n    αₖ = α₀\n    while f(xₖ)-f(xₖ+αₖ*dₖ) &lt; -γ*αₖ*dot(dₖ,∇fₖ)\n        αₖ *= β\n    end\n    return αₖ\nend\n\n\nbacktracking_line_search (generic function with 1 method)\n\n\nNow we are ready to proceed to the question of choosing a descent direction.\n\n\n\nSteepest descent (aka gradient descent) method\nA natural candidate for a descent direction is the negative of the gradient \n\\bm d_k = -\\nabla f(\\bm x_k).\n\nIn fact, among all descent directions, this is the one for which the descent is steepest (the gradient determines the direction of steepest ascent), though we will see later that this does not mean that the convergence of the method is the fastest.\nIn each iteration of the gradient method, this is the how the solution is updated\n\n\\boxed{\n\\bm x_{k+1} = \\bm x_{k} - \\alpha_k \\nabla f(\\bm x_{k}),}\n where determinatio of the step length \\alpha_k has already been discussed in the prevous section.\nLet’s now examine the behaviour of the method by applying it to minimization of a quadratic function. Well, for a quadratic function it is obviously an overkill, but we use it in the example because we can compute the step lenght exactly, which then helps the methods show its best.\n\nExample 2  \n\n\nShow the code\nusing LinearAlgebra         # For dot() function.\nusing Printf                # For formatted output.\n\nx0 = [2, 3]                 # Initial vector.\nQ = [1 0; 0 3]              # Positive definite matrix defining the quadratic form.\nc = [1, 2]                   # Vector defining the linear part.\n\nxs = -Q\\c                   # Stationary point, automatically the minimizer for posdef Q. \n\nϵ  = 1e-5                   # Threshold on the norm of the gradient.\nN  = 100;                   # Maximum number of steps .\n\nfunction gradient_descent_quadratic_exact(Q,c,x0,ϵ,N)\n    x = x0\n    iter = 0\n    f = 1/2*dot(x,Q*x)+dot(x,c)\n    ∇f = Q*x+c\n    while (norm(∇f) &gt; ϵ)\n        α = dot(∇f,∇f)/dot(∇f,Q*∇f)\n        x = x - α*∇f\n        iter = iter+1\n        f = 1/2*dot(x,Q*x)+dot(x,c)\n        ∇f = Q*x+c\n        @printf(\"i = %3d   ||∇f(x)|| = %6.4e   f(x) = %6.4e\\n\", iter, norm(∇f), f)\n        if iter &gt;= N\n            return f,x\n        end\n    end\n    return f,x\nend\n\nfopt,xopt = gradient_descent_quadratic_exact(Q,c,x0,ϵ,N)\n\n\ni =   1   ||∇f(x)|| = 2.0229e+00   f(x) = 7.8495e-01\ni =   2   ||∇f(x)|| = 9.0210e-01   f(x) = -1.0123e+00\ni =   3   ||∇f(x)|| = 1.6005e-01   f(x) = -1.1544e+00\ni =   4   ||∇f(x)|| = 7.1374e-02   f(x) = -1.1657e+00\ni =   5   ||∇f(x)|| = 1.2663e-02   f(x) = -1.1666e+00\ni =   6   ||∇f(x)|| = 5.6470e-03   f(x) = -1.1667e+00\ni =   7   ||∇f(x)|| = 1.0019e-03   f(x) = -1.1667e+00\ni =   8   ||∇f(x)|| = 4.4679e-04   f(x) = -1.1667e+00\ni =   9   ||∇f(x)|| = 7.9269e-05   f(x) = -1.1667e+00\ni =  10   ||∇f(x)|| = 3.5350e-05   f(x) = -1.1667e+00\ni =  11   ||∇f(x)|| = 6.2718e-06   f(x) = -1.1667e+00\n\n\n(-1.1666666666479069, [-0.9999939492423319, -0.6666672167355456])\n\n\nWe can also decorate the code a bit to visualize how the iterations proceeded.\n\n\nShow the code\nfunction gradient_descent_quadratic_exact_decor(Q,c,x0,ϵ,N)\n    x = x0\n    X = x\n    f = 1/2*dot(x,Q*x)+dot(x,c)\n    F = [f,]\n    ∇f = Q*x+c\n    iter = 0\n    while (norm(∇f) &gt; ϵ)\n        α = dot(∇f,∇f)/dot(∇f,Q*∇f)\n        x = x - α*∇f\n        iter = iter+1\n        f = 1/2*dot(x,Q*x)+dot(x,c)\n        ∇f = Q*x+c\n        X = hcat(X,x)\n        push!(F,f)\n        if iter &gt;= N\n         return F,X\n        end\n    end\n    return F,X\nend\n\nF,X = gradient_descent_quadratic_exact_decor(Q,c,x0,ϵ,N)\n\nx1_data = x2_data = -4:0.01:4;\nf(x) = 1/2*dot(x,Q*x)+dot(x,c)\nz_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ncontour(x1_data,x2_data,z_data)\nplot!(X[1,:],X[2,:],label=\"xk\",marker=:diamond,aspect_ratio=1)\nscatter!([x0[1],],[x0[2],],label=\"x0\")\nscatter!([xs[1],],[xs[2],],label=\"xopt\")\nxlabel!(\"x1\");ylabel!(\"x2\");\nxlims!(-4,4); ylims!(-4,4)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Zigzagging of the steepest descent method for a quadratic function\n\n\n\n\n\nAltough the number of iterations in the above example is acceptable, a major characteristic of the method is visible. Its convergence is slowing down as we are approaching a local minimum, which is visually recognizable oscillations or zig-zagging. But it can be much worse for some data.\n\nGradient method converges slowly for ill-conditioned problems\n\nExample 3 Consider minimization of the following cost function f(\\bm x) = 1000x_1^2 + 40x_1x_2 + x_2^2.\n\n\nShow the code\nQ = [1000 20; 20 1]\nc = [0, 0]\nx0 = [1,1000]\n\nF,X = gradient_descent_quadratic_exact(Q,c,x0,ϵ,N)\n\n\ni =   1   ||∇f(x)|| = 5.9951e+02   f(x) = 2.9939e+05\ni =   2   ||∇f(x)|| = 1.2093e+04   f(x) = 1.7221e+05\ni =   3   ||∇f(x)|| = 3.4484e+02   f(x) = 9.9052e+04\ni =   4   ||∇f(x)|| = 6.9560e+03   f(x) = 5.6974e+04\ni =   5   ||∇f(x)|| = 1.9835e+02   f(x) = 3.2771e+04\ni =   6   ||∇f(x)|| = 4.0011e+03   f(x) = 1.8850e+04\ni =   7   ||∇f(x)|| = 1.1409e+02   f(x) = 1.0842e+04\ni =   8   ||∇f(x)|| = 2.3014e+03   f(x) = 6.2364e+03\ni =   9   ||∇f(x)|| = 6.5623e+01   f(x) = 3.5872e+03\ni =  10   ||∇f(x)|| = 1.3237e+03   f(x) = 2.0633e+03\ni =  11   ||∇f(x)|| = 3.7746e+01   f(x) = 1.1868e+03\ni =  12   ||∇f(x)|| = 7.6141e+02   f(x) = 6.8264e+02\ni =  13   ||∇f(x)|| = 2.1711e+01   f(x) = 3.9265e+02\ni =  14   ||∇f(x)|| = 4.3796e+02   f(x) = 2.2585e+02\ni =  15   ||∇f(x)|| = 1.2488e+01   f(x) = 1.2991e+02\ni =  16   ||∇f(x)|| = 2.5191e+02   f(x) = 7.4722e+01\ni =  17   ||∇f(x)|| = 7.1831e+00   f(x) = 4.2980e+01\ni =  18   ||∇f(x)|| = 1.4490e+02   f(x) = 2.4722e+01\ni =  19   ||∇f(x)|| = 4.1317e+00   f(x) = 1.4220e+01\ni =  20   ||∇f(x)|| = 8.3344e+01   f(x) = 8.1791e+00\ni =  21   ||∇f(x)|| = 2.3765e+00   f(x) = 4.7046e+00\ni =  22   ||∇f(x)|| = 4.7939e+01   f(x) = 2.7061e+00\ni =  23   ||∇f(x)|| = 1.3670e+00   f(x) = 1.5565e+00\ni =  24   ||∇f(x)|| = 2.7574e+01   f(x) = 8.9529e-01\ni =  25   ||∇f(x)|| = 7.8627e-01   f(x) = 5.1497e-01\ni =  26   ||∇f(x)|| = 1.5861e+01   f(x) = 2.9621e-01\ni =  27   ||∇f(x)|| = 4.5226e-01   f(x) = 1.7038e-01\ni =  28   ||∇f(x)|| = 9.1229e+00   f(x) = 9.7999e-02\ni =  29   ||∇f(x)|| = 2.6014e-01   f(x) = 5.6369e-02\ni =  30   ||∇f(x)|| = 5.2474e+00   f(x) = 3.2423e-02\ni =  31   ||∇f(x)|| = 1.4963e-01   f(x) = 1.8649e-02\ni =  32   ||∇f(x)|| = 3.0183e+00   f(x) = 1.0727e-02\ni =  33   ||∇f(x)|| = 8.6065e-02   f(x) = 6.1701e-03\ni =  34   ||∇f(x)|| = 1.7361e+00   f(x) = 3.5490e-03\ni =  35   ||∇f(x)|| = 4.9504e-02   f(x) = 2.0414e-03\ni =  36   ||∇f(x)|| = 9.9859e-01   f(x) = 1.1742e-03\ni =  37   ||∇f(x)|| = 2.8475e-02   f(x) = 6.7539e-04\ni =  38   ||∇f(x)|| = 5.7439e-01   f(x) = 3.8848e-04\ni =  39   ||∇f(x)|| = 1.6378e-02   f(x) = 2.2345e-04\ni =  40   ||∇f(x)|| = 3.3038e-01   f(x) = 1.2853e-04\ni =  41   ||∇f(x)|| = 9.4207e-03   f(x) = 7.3928e-05\ni =  42   ||∇f(x)|| = 1.9003e-01   f(x) = 4.2523e-05\ni =  43   ||∇f(x)|| = 5.4188e-03   f(x) = 2.4459e-05\ni =  44   ||∇f(x)|| = 1.0931e-01   f(x) = 1.4069e-05\ni =  45   ||∇f(x)|| = 3.1168e-03   f(x) = 8.0922e-06\ni =  46   ||∇f(x)|| = 6.2873e-02   f(x) = 4.6546e-06\ni =  47   ||∇f(x)|| = 1.7928e-03   f(x) = 2.6773e-06\ni =  48   ||∇f(x)|| = 3.6164e-02   f(x) = 1.5400e-06\ni =  49   ||∇f(x)|| = 1.0312e-03   f(x) = 8.8578e-07\ni =  50   ||∇f(x)|| = 2.0801e-02   f(x) = 5.0949e-07\ni =  51   ||∇f(x)|| = 5.9314e-04   f(x) = 2.9306e-07\ni =  52   ||∇f(x)|| = 1.1965e-02   f(x) = 1.6856e-07\ni =  53   ||∇f(x)|| = 3.4117e-04   f(x) = 9.6958e-08\ni =  54   ||∇f(x)|| = 6.8821e-03   f(x) = 5.5769e-08\ni =  55   ||∇f(x)|| = 1.9624e-04   f(x) = 3.2078e-08\ni =  56   ||∇f(x)|| = 3.9585e-03   f(x) = 1.8451e-08\ni =  57   ||∇f(x)|| = 1.1288e-04   f(x) = 1.0613e-08\ni =  58   ||∇f(x)|| = 2.2769e-03   f(x) = 6.1045e-09\ni =  59   ||∇f(x)|| = 6.4925e-05   f(x) = 3.5113e-09\ni =  60   ||∇f(x)|| = 1.3097e-03   f(x) = 2.0197e-09\ni =  61   ||∇f(x)|| = 3.7345e-05   f(x) = 1.1617e-09\ni =  62   ||∇f(x)|| = 7.5331e-04   f(x) = 6.6821e-10\ni =  63   ||∇f(x)|| = 2.1480e-05   f(x) = 3.8435e-10\ni =  64   ||∇f(x)|| = 4.3330e-04   f(x) = 2.2107e-10\ni =  65   ||∇f(x)|| = 1.2355e-05   f(x) = 1.2716e-10\ni =  66   ||∇f(x)|| = 2.4923e-04   f(x) = 7.3142e-11\ni =  67   ||∇f(x)|| = 7.1068e-06   f(x) = 4.2071e-11\n\n\n(4.207097012941924e-11, [-2.371876542980443e-7, 1.1842143766107573e-5])\n\n\nWhile for the previous problem of the same kind and size the steepest descent method converged in just a few steps, for this particular data it takes many dozens of steps.\nThe culprit here are bad properties of the Hessian matrix Q. By ``bad properties’’ we mean the so-called , which is reflected in the very high . Recall that condition number \\kappa for a given matrix \\mathbf A is defined as \n\\kappa(\\mathbf A) = \\|\\mathbf A^{-1}\\|\\cdot \\|\\mathbf A\\|\n and it can be computed as ratio of the largest and smallest singular values, that is, \n\\kappa(\\mathbf A) = \\frac{\\sigma_{\\max}(\\mathbf A)}{\\sigma_{\\min}(\\mathbf A)}.\n\\end{equation}$$\nIdeally this number should be around 1. In the example above it is\n\n\nShow the code\ncond(Q)\n\n\n1668.0010671466664\n\n\nwhich is well above 1000. Is there anything that we can do about it? The answer is yes. We can scale the original date to improve the conditioning.\n\n\n\nScaled gradient method for ill-conditioned problems\nUpon introducing a matrix \\mathbf S that relates the original vector variable \\bm x with a new vector variable \\bm y according to \n\\bm x = \\mathbf S \\bm y,\n the optimization cost function changes from f(\\bm x) to f(\\mathbf S \\bm y). Let’s relabel the latter to g(\\bm y). And we will now examine how the steepest descent iteration changes. Straightforward application of a chain rule for finding derivatives of composite functions yields \ng'(\\bm y) = f'(\\mathbf S\\bm y) = f'(\\mathbf S\\bm y)\\mathbf S.\n\nKeeping in mind that gradients are transposes of derivatives, we can write \n\\nabla g(\\bm y) = \\mathbf S^\\top \\nabla f(\\mathbf S\\bm y).\n\nSteepest descent iterations then change accordingly\n\n\\begin{aligned}\n\\mathbf y_{k+1} &= \\mathbf y_k - \\alpha_k \\nabla g(\\mathbf y_k)\\\\\n\\mathbf y_{k+1} &= \\mathbf y_k - \\alpha_k \\mathbf S^T\\nabla f(\\mathbf S \\mathbf y_k)\\\\\n\\underbrace{\\mathbf S \\mathbf y_{k+1}}_{\\mathbf x_{k+1}} &= \\underbrace{\\mathbf S\\mathbf y_k}_{\\mathbf x_k} - \\alpha_k \\underbrace{\\mathbf S \\mathbf S^T}_{\\mathbf D}\\nabla f(\\underbrace{\\mathbf S \\mathbf y_k}_{\\mathbf x_k}).\n\\end{aligned}\n\nUpon defining the scaling matrix \\mathbf D as \\mathbf S \\mathbf S^T, a single iteration changes to \n\\boxed{\\bm x_{k+1} = \\bm x_{k} - \\alpha_k \\mathbf D_k\\nabla f(\\bm x_{k}).}\n\nThe question now is: how to choose the matrix \\mathbf D? We would like to make the Hessian matrix \\nabla^2 f(\\mathbf S \\bm y) (which in the case of a quadratic matrix form is the matrix \\mathbf Q as we used it above) better conditioned. Ideally, \\nabla^2 f(\\mathbf S \\bm y)\\approx \\mathbf I.\nA simple way for improving the conditioning is to define the scaling matrix \\mathbf D as a diagonal matrix whose diagonal entries are given by \n\\mathbf D_{ii} = [\\nabla^2 f(\\bm x_k)]^{-1}_{ii}.\n\nIn words, the diagonal entries of the Hessian matrix are inverted and they then form the diagonal of the scaling matrix.\nIt is worth emphasizing how the algorithm changed: the direction of steepest descent (the negative of the gradient) is premultiplied by some (scaling) matrix. We will see in a few moments that another method – Newton’s method – has a perfectly identical structure.\n\n\n\nNewton’s method\nNewton’s method is one of flagship algorithms in numerical computing. I am certainly not exaggerating if I include it in my personal Top 10 list of algorithms relevant for engineers. We may encounter the method in two settings: as a method for solving (sets of) nonlinear equations and as a method for optimization. The two are inherently related and it is useful to be able to see the connection.\n\nNewton’s method for rootfinding\nThe problem to be solved is that of finding x for which a given function g() vanishes. In other words, we solve the following equation \ng(x) = 0.\n\nThe above state scalar version has also its vector extension \n\\mathbf g(\\bm x) = \\mathbf 0,\n in which \\bm x stands for an n-tuple of variables and \\mathbf g() actually stands for an n-tuple of functions. Even more general version allows for different number of variables and equations.\nWe start with a scalar version. A single iteration of the method evaluates not only the value of the function g(x_k) at the given point but also its derivative g'(x_k). It then uses the two to approximate the function g() at x_k by a linear (actually affine) function and computes the intersection of this approximating function with the horizontal axis. This gives as x_{k+1}, that is, the (k+1)-th approximation to a solution (root). We can write this down as \n\\begin{aligned}\n\\underbrace{g(x_{k+1})}_{0} &= g(x_{k}) + g'(x_{k})(x_{k+1}-x_k)\\\\\n0 &= g(x_{k}) + g'(x_{k})x_{k+1}-g'(x_{k})x_k),\n\\end{aligned}\n from which the famous formula follows \n\\boxed{x_{k+1} = x_{k} - \\frac{g(x_k)}{g'(x_k)}.}\n\nIn the vector form, the formula is \n\\boxed{\\bm x_{k+1} = \\bm x_{k} - [\\nabla \\mathbf g(\\bm x_k)^\\top]^{-1}\\mathbf g(\\bm x_k),}\n where \\nabla \\mathbf g(\\bm x_k)^\\top is the (Jacobian) matrix of the first derivatives of \\mathbf g at \\bm x_k, that is, \\nabla \\mathbf g() is a matrix with the gradient of the g_i(\\bm x) function in its i-th column.\n\n\nNewton’s method for optimization\nOnce again, restrict ourselves to a scalar case first. The problem is \n\\operatorname*{minimize}_{x\\in\\mathbb{R}}\\quad f(x).\n\nAt the k-th iteration of the algorithm, the solution is x_k. The function to be minimized is approximated by a quadratic function m_k() in x. In order to find parameterization of this quadratic function, the function f() but also its first and second derivatives, f'() and f''(), respectively, need to be evaluated at x_k. Using these three, a function m_k(x) approximating f(x) at some x not too far from x_k can be defined \nm_k(x) = f(x_k) + f'(x_k)(x-x_k) + \\frac{1}{2}f''(x_k)(x-x_k)^2.\n\nThe problem of minimizing this new function in the k-th iteration is then formulated, namely,\n\n\\operatorname*{minimize}_{x_{k+1}\\in\\mathbb{R}}\\quad m_k(x_{k+1})\n and solved for some x_{k+1}. The way to find this solution is straightforward: find the derivative of m_k() and find the value of x_{k+1} for which this derivative vanishes. The result is \n\\boxed{x_{k+1} = x_{k} - \\frac{f'(x_k)}{f''(x_k)}.}\n\nThe vector version of the Newton’s step is \n\\boxed{\\bm x_{k+1} = \\bm x_{k} - [\\nabla^2 f(\\bm x_k)]^{-1} \\nabla f(\\bm x_k).}\n\nA few observations\n\nIf compared to the general prescription for descent direction methods (), the Newton’s method determines the direction and the step lenght at once (both \\alpha_k and \\mathbf d_k are hidden in - [\\nabla^2 f(\\mathbf x_k)]^{-1} \\nabla f(\\mathbf x_k)).\nIf compared with steepest descent (gradient) method, especially with its scaled version (), Newton’s method fits into the framework nicely because the inverse [\\nabla^2 f(\\mathbf x_k)]^{-1} of the Hessian can be regarded as a kind of a scaling matrix \\mathbf D. Indeed, you can find arguments in some textbooks that Newton’s method involves scaling that is optimal in some sense. We skip the details here because we only wanted to highlight the similarity in the structure of the two methods.\n\nThe great popularity of Newton’s method is mainly due to its nice convergence – quadratic. Although we skip any discussion of convergence rates here, note that for all other methods this is an ideal that aims to be approached.\nThe nice convergence rate of Newton’s method is compensated by a few disadvantages\n\nThe need to compute the Hessian. This is perhaps not quite clear with simple problems but can play some role with huge problems.\nOnce the Hessian is computed, it must be inverted (actually, a linear system must by solved). But this assumes that Hessian is nonsingular. How can we guarantee this for a given problem?\nIt is not only that Hessian must be nonsingular but it must also be positive (definite). Note that in the scalar case this corresponds to the situation when the second derivative is positive. Negativeness of the second derivative can send the algorithm in the opposite direction – away from the local minimum – , which which would ruin the convergence of the algorithm.\n\nThe last two issues are handled by some modification of the standard Newton’s method\n\nDamped Newton’s method\nA parameter \\alpha\\in(0,1) is introduced that shortens the step as in \n  \\bm x_{k+1} = \\bm x_{k} - \\alpha(\\nabla^2 f(\\bm x_k))^{-1} \\nabla f(\\bm x_k).\n\n\n\nFixed constant positive definite matrix instead of the inverse of the Hessian\nThe step is determined as \n  \\bm x_{k+1} = \\bm x_{k} - \\mathbf B \\nabla f(\\bm x_k).\n\nNote that the interpretation of the constant \\mathbf B in the position of the (inverse of the) Hessian in the rootfinding setting is that the slope of the approximating linear (affine) function is always constant.\nNow that we admitted to have something else then just the (inverse of the) Hessian in the formula for Newton’s method, we can explore further this new freedom. This will bring us into a family of methods called Quasi-Newton methods.\n\n\n\n\nQuasi-Newton’s methods\n…\n\n\nConjugate gradient method\n…",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_unconstrained.html#trust-region-methods",
    "href": "opt_algo_unconstrained.html#trust-region-methods",
    "title": "Algorithms for unconstrained optimization",
    "section": "Trust region methods",
    "text": "Trust region methods\n…",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for unconstrained optimization"
    ]
  },
  {
    "objectID": "discr_dir_mpc_solvers.html",
    "href": "discr_dir_mpc_solvers.html",
    "title": "Numerical solvers for MPC",
    "section": "",
    "text": "Essentially QP solvers with some extra features for MPC:\n\nwarmstarting requires fesibility of the previous solution. If only a fixed number of iterations is allowed (in favor of predictable timing), for some methods the iterations may temporarily lose feasibility.\n…\nqpOASES\nOSQP\nDAQP\nqpSWIFT\nProxQP\nPiQP\nECOS\nHPIPM\n\n\n\n\n Back to top",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Numerical solvers for MPC"
    ]
  },
  {
    "objectID": "roban_references.html",
    "href": "roban_references.html",
    "title": "References",
    "section": "",
    "text": "Even when restricted to control systems, the concept of robustness is quite broad and can be approached from many different angles. In our course we are restricting the focus to the approaches formulated in frequency domain. The main reference for this part of the course is the book (Skogestad and Postlethwaite 2005). The concepts and techniques introduced in our lecture are covered in Chapters 7 and 8 (up to 8.5) of the book. What we typically do not cover in the lecture is the topic of structured uncertainties and their analysis using structured singular value (SSV, 𝜇). These are treated in the section 8.6 through 8.11 of the book. Although the book is not freely available online (only the first three pages are downloadable on the authors’ web page), it is available in a decent number of copies in the university library.\n\n\n\n\n\n\nGet the second edition of Skogestad’s book\n\n\n\nIn case you are interested in getting the book in one way or another (perhaps even by purchasing it), make sure you get the second edition published 2005. The book contains some useful snippets of Matlab code and the first edition relies on some ancient version of Matlab toolboxes, which makes it useless these days.\n\n\nThe topic of modeling uncertainty in frequency domain using weighting filters plugged into additive or multiplicative structures is fairly classical now and as such can be found in numerous textbooks on robust control such as (Doyle, Francis, and Tannenbaum 2009), (Zhou, Doyle, and Glover 1995), (Dullerud and Paganini 2000), (Sánchez-Peña and Sznaier 1998). Although these are fine texts, frankly speaking they offer nearly no guidance for applying the highly advanced concepts to practical problems – they mostly focus on building up the theoretical framework. In this regard, Skogestad’s book is truly unique.\nThere are some newer texts that appear to be a bit more on the application side such as (Lavretsky and Wise 2024), (Yedavalli 2014) or (Gu, Petkov, and Konstantinov 2013), but I am not familiar them, to be honest.\n\n\n\n\n Back to topReferences\n\nDoyle, John C., Bruce A. Francis, and Allen R. Tannenbaum. 2009. Feedback Control Theory. Reprint of the 1990 edition by Macmillan Publishing. Dover Publications. https://www.control.utoronto.ca/people/profs/francis/dft.pdf.\n\n\nDullerud, Geir E., and Fernando Paganini. 2000. A Course in Robust Control Theory: A Convex Approach. Texts in Applied Mathematics. New York: Springer-Verlag. https://doi.org/10.1007/978-1-4757-3290-0.\n\n\nGu, Da-Wei, Petko H. Petkov, and Mihail M. Konstantinov. 2013. Robust Control Design with MATLAB. 2nd ed. Advanced Textbooks in Control and Signal Processing. New York: Springer. https://doi.org/10.1007/978-1-4471-4682-7.\n\n\nLavretsky, Eugene, and Kevin Wise. 2024. Robust and Adaptive Control: With Aerospace Applications. 2nd ed. Advanced Textbooks in Control and Signal Processing (C&SP). Cham: Springer. https://doi.org/10.1007/978-3-031-38314-4.\n\n\nSánchez-Peña, Ricardo S., and Mario Sznaier. 1998. Robust Systems Theory and Applications. 1st ed. Wiley-Interscience.\n\n\nSkogestad, Sigurd, and Ian Postlethwaite. 2005. Multivariable Feedback Control: Analysis and Design. 2nd ed. Wiley. https://folk.ntnu.no/skoge/book/.\n\n\nYedavalli, Rama K. 2014. Robust Control of Uncertain Dynamic Systems: A Linear State Space Approach. New York: Springer. https://doi.org/10.1007/978-1-4614-9132-3.\n\n\nZhou, Kemin, John C. Doyle, and Keith Glover. 1995. Robust and Optimal Control. 1st ed. Prentice Hall.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "References"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html",
    "href": "opt_algo_derivatives.html",
    "title": "Computing derivatives",
    "section": "",
    "text": "We have already argued that using derivatives within gives us a huge advantage in solving optimization problems.\nThere are three ways to compute derivatives:",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing derivatives"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html#symbolic-methods",
    "href": "opt_algo_derivatives.html#symbolic-methods",
    "title": "Computing derivatives",
    "section": "Symbolic methods",
    "text": "Symbolic methods\nThese are essentially the methods that we have all learnt to apply using a pen and paper. A bunch of rules. The outcome is an expression.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing derivatives"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html#numerical-finite-difference-fd-methods",
    "href": "opt_algo_derivatives.html#numerical-finite-difference-fd-methods",
    "title": "Computing derivatives",
    "section": "Numerical finite-difference (FD) methods",
    "text": "Numerical finite-difference (FD) methods\nThese methods approximate the derivative by computing differences between the function values at different points, hence the name finite-difference (FD) methods. The simplest FD methods follow from the definition of the derivative after omiting the limit:\n\n\\frac{\\mathrm d f(x)}{\\mathrm d x} \\approx \\frac{f(x+\\alpha)-f(x)}{\\alpha}\\qquad\\qquad \\text{forward difference}\n or \n\\frac{\\mathrm d f(x)}{\\mathrm d x} \\approx \\frac{f(x)-f(x-\\alpha)}{\\alpha}\\qquad\\qquad \\text{backward difference}\n or \n\\frac{\\mathrm d f(x)}{\\mathrm d x} \\approx \\frac{f(x+\\frac{\\alpha}{2})-f(x-\\frac{\\alpha}{2})}{\\alpha}\\qquad\\qquad \\text{central difference}\n\nFor functions of vector variables, the same idea applies, but now we have to compute the difference for each component of the vector.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing derivatives"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html#algorithmic-also-automatic-differentiation-methods",
    "href": "opt_algo_derivatives.html#algorithmic-also-automatic-differentiation-methods",
    "title": "Computing derivatives",
    "section": "Algorithmic (also Automatic) differentiation methods",
    "text": "Algorithmic (also Automatic) differentiation methods",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing derivatives"
    ]
  },
  {
    "objectID": "ext_H2.html",
    "href": "ext_H2.html",
    "title": "H2-optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "H2-optimal control"
    ]
  },
  {
    "objectID": "dynamic_programming_references.html",
    "href": "dynamic_programming_references.html",
    "title": "References",
    "section": "",
    "text": "Dynamic programming (DP) is a fairly powerful and yet general framework that finds its use in many disciplines. Optimal control is not the only one. But in this overview of the literature we deliberately focus on the DP references with optimal control flavour.\nOur introductory treatment was based almost exclusively on the (also just introductory) Chapter 6 in (Lewis, Vrabie, and Syrmo 2012). Electronic version of the book is freely available on the author’s webpage.\nComparable introduction is provided in (Kirk 2004). Although it does not appear to be legally available for free in an electronic form, its reprint by a low-cost publisher makes it an affordable (and recommendable) classic reference. Another classic (Anderson and Moore 2007) actually uses dynamic programming as the key technique to derive all those LQ-optimal regulation and tracking results. A few copies of this book are available in the faculty library at NTK. The authors also made an electronic version available for free on their website.\nFairly comprehensive treatment of control-oriented DP is in the two-volume monograph (Bertsekas 2017) and (Bertsekas 2012). It is not available online for free, but the book webpage contains links to other supporting materials including lecture notes. Furthermore, the latest book by the same author (Bertsekas 2023), which is available for free download, contains a decent introduction to dynamic programming.\nHaving just referenced a book on reinforcement learning (RL), indeed, this popular concept — or at least some of its flavours — is closely related to dynamic programming. In fact, it offers a way to overcome some of the limitations of dynamic programming. In our introductory lecture we are not covering RL, but an interested student can take advantage of availability of high-quality resources such as the the RL-related books and other resources by D. Bertsekas) and another recommendable introduction to RL from control systems perspective (Meyn 2022), which is also available for free download.\nThe book (Sutton and Barto 2018) often regarded as the bible of RL is nice (and freely available for download) but may be rather difficult to read for a control engineer because of major differences in terminology.\n\n\n\n\n Back to topReferences\n\nAnderson, Brian D. O., and John B. Moore. 2007. Optimal Control: Linear Quadratic Methods. Reprint of the 1989 edition. Dover Publications. http://users.cecs.anu.edu.au/~john/papers/BOOK/B03.PDF.\n\n\nBertsekas, Dimitri P. 2012. Dynamic Programming and Optimal Control. 4th ed. Vol. II. Belmont, Massachusetts: Athena Scientific. http://athenasc.com/dpbook.html.\n\n\n———. 2017. Dynamic Programming and Optimal Control. 4th ed. Vol. I. Belmont, Massachusetts: Athena Scientific. http://athenasc.com/dpbook.html.\n\n\n———. 2023. A Course in Reinforcement Learning. Belmont, Massachusetts: Athena Scientific. https://web.mit.edu/dimitrib/www/RLCOURSECOMPLETE.pdf.\n\n\nKirk, Donald E. 2004. Optimal Control Theory: An Introduction. Reprint of the 1970 edition. Dover Publications.\n\n\nLewis, Frank L., Draguna Vrabie, and Vassilis L. Syrmo. 2012. Optimal Control. 3rd ed. John Wiley & Sons. https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf.\n\n\nMeyn, Sean. 2022. Control Systems and Reinforcement Learning. Cambridge University Press. https://meyn.ece.ufl.edu/control-systems-and-reinforcement-learning/.\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. 2nd ed. Cambridge, Massachusetts: A Bradford Book. http://incompleteideas.net/book/the-book-2nd.html.",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming",
      "References"
    ]
  },
  {
    "objectID": "uncertainty.html",
    "href": "uncertainty.html",
    "title": "Uncertainty modelling",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "discr_dir_general.html",
    "href": "discr_dir_general.html",
    "title": "General finite-horizon nonlinear discrete-time optimal control as nonlinear program",
    "section": "",
    "text": "In this section we formulate a finite-horizon optimal control problem (OCP) for a discrete-time dynamical system as a mathematical optimization (also mathematical programming) problem, which can then be solved numerically by a suitable solvers for nonlinear programming (NLP), or possibly quadratic programming (QP). The outcome of such numerical optimization is an optimal control trajectory (a sequence of controls), which is why this approach is called direct – we optimize directly over the trajectories.\nIn the next chapter we then present an alternative – indirect – approach, wherein the conditions of optimality are formulated first. These come in the form of a set of equations, some of them recurrent/recursive, hence indirect approach amounts to solving such equations.\nBut now back to the direct approaches. We will start with a general nonlinear discrete-time optimal control problem in this section and then specialize to the linear quadratic regulation (LQR) problem in the next section. Finally, since the computed control trajectory constitutes an open-loop control scheme, something must be done about it if feedback scheme is preferred – we introduce the concept of a receding horizon control (RHC), perhaps bettern known as model predictive control (MPC), that turns the direct approach into a feedback control scheme.\nWe start by considering a nonlinear discrete-time system modelled by the state equation \n\\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\n where\n\n\\bm x_k\\in \\mathbb R^n is the state at the discrete time k\\in \\mathbb Z,\n\\bm u_k\\in \\mathbb R^m is the control at the discrete time k,\n\\mathbf f_k: \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb Z \\to \\mathbb{R}^n is a state transition function (in general not only nonlinear but also time-varying, with the convention that the dependence on k is expressed through the lower index).\n\nA general nonlinear discrete-time optimal control problem (OCP) is then formulated as \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_i,\\ldots, \\bm u_{N-1}, \\bm x_{i},\\ldots, \\bm x_N}&\\quad \\left(\\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k) \\right)\\\\\n\\text{subject to}  &\\quad \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm u_k \\in \\mathcal U_k,\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm x_k \\in \\mathcal X_k,\\quad k=i, \\ldots, N,\n\\end{aligned}\n where\n\ni is the initial discrete time,\nN is the final discrete time,\n\\phi() is a terminal cost function that penalizes the state at the final time,\nL_k() is a running (also stage) cost function,\nand \\mathcal U_k and \\mathcal X_k are sets of feasible controls and states – these sets are typically expressed using equations and inequalities. Should they be constant, the notation is just \\mathcal U and \\mathcal X.\n\nOftentimes it is convenient to handle the constraints of the initial and final states separately: \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_i,\\ldots, \\bm u_{N-1}, \\bm x_{i},\\ldots, \\bm x_N}&\\quad \\left(\\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k) \\right)\\\\\n\\text{subject to}  &\\quad \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm u_k \\in \\mathcal U_k,\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm x_k \\in \\mathcal X_k,\\quad k=i+1, \\ldots, N-1,\\\\\n                    &\\quad \\bm x_i \\in \\mathcal X_\\mathrm{init},\\\\\n                    &\\quad \\bm x_N \\in \\mathcal X_\\mathrm{final}.\n\\end{aligned}\n\nIn particular, at the initial time just one particular state is often considered. At the final time, the state might be required to be equal to some given value, it might be required to be in some set defined through equations or inequalities, or it might be left unconstrained. Finally, the constraints on the control and states typically (but not always) come in the form of lower and upper bounds. The optimal control problem then specializes to \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_i,\\ldots, \\bm u_{N-1}, \\bm x_{i},\\ldots, \\bm x_N}&\\quad \\left(\\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k) \\right)\\\\\n\\text{subject to}  &\\quad \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm u_{\\min} \\leq \\bm u_k \\leq \\bm u_{\\max},\\\\\n                    &\\quad \\bm x_{\\min} \\leq \\bm x_k \\leq \\bm x_{\\max},\\\\\n                    &\\quad\\bm x_i = \\mathbf x^\\text{init},\\\\\n                    &\\quad \\left(\\bm x_N = \\mathbf x^\\text{ref}, \\; \\text{or} \\; \\mathbf h_\\text{final}(\\bm x_N) =  \\mathbf 0, \\text{or} \\; \\mathbf g_\\text{final}(\\bm x_N) \\leq  \\mathbf 0\\right),                    \n\\end{aligned}\n where\n\n\\bm u_{\\min} and \\bm u_{\\max} are lower and upper bounds on the control, respectively,\n\\bm x_{\\min} and \\bm x_{\\max} are lower and upper bounds on the state, respectively,\n\\mathbf x^\\text{init} is a fixed initial state,\n\\mathbf x^\\text{ref} is a required (reference) final state,\nand the functions \\mathbf g_\\text{final}() and \\mathbf h_\\text{final}() can be used to define the constraint set for the final state.\n\nThis optimal control problem is an instance of a general nonlinear programming (NLP) problem \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm x}\\in\\mathbb{R}^{n(N-i)},\\bar{\\bm u}\\in\\mathbb{R}^{m(N-i)}} &\\quad J(\\bar{\\bm x},\\bar{\\bm u})\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bar{\\bm x},\\bar{\\bm u}) =0,\\\\\n&\\quad \\mathbf g(\\bar{\\bm x},\\bar{\\bm u}) \\leq \\mathbf 0,\n\\end{aligned}\n where \\bar{\\bm u} and \\bar{\\bm x} are vectors obtained by stacking control and state vectors for individual times\n\n\\begin{aligned}\n\\bar{\\bm u} &= \\operatorname*{vec}(\\bm u_i,\\ldots, \\bm u_{N-1}) = \\begin{bmatrix}\\bm u_i\\\\ \\bm u_{i+1}\\\\ \\vdots \\\\ \\bm u_{N-1} \\end{bmatrix},\\\\\n\\bar{\\bm x} &= \\operatorname*{vec}(\\bm x_{i+1},\\ldots, \\bm x_N) = \\begin{bmatrix}\\bm x_{i+1}\\\\ \\bm x_{i+2}\\\\ \\vdots \\\\ \\bm x_{N} \\end{bmatrix}.\n\\end{aligned}\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlthought there may be applications where it is desirable to optimize over the initial state \\bm x_i as well, mostly the initial state \\bm x_i is fixed, and it does not have to be considered as an optimization variable. This can be even emphasized through the notation J(\\bar{\\bm x},\\bar{\\bm u}; \\bm x_i), where the semicolon separates the variables from (fixed) parameters.\nThe last control that affects the state trajectory on the interval [i,N] is \\bm u_{N-1}.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "General finite-horizon nonlinear discrete-time optimal control as nonlinear program"
    ]
  },
  {
    "objectID": "ext_stochastic_LQR.html",
    "href": "ext_stochastic_LQR.html",
    "title": "LQR for stochastic systems",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQR for stochastic systems"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html",
    "href": "opt_theory_reformulations.html",
    "title": "Problem reformulations",
    "section": "",
    "text": "Given a function f(\\bm x), we can maximize it by minimizing -f(\\bm x).",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#maximization-into-minimization",
    "href": "opt_theory_reformulations.html#maximization-into-minimization",
    "title": "Problem reformulations",
    "section": "",
    "text": "Given a function f(\\bm x), we can maximize it by minimizing -f(\\bm x).",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#equality-into-inequality-constraints",
    "href": "opt_theory_reformulations.html#equality-into-inequality-constraints",
    "title": "Problem reformulations",
    "section": "Equality into inequality constraints",
    "text": "Equality into inequality constraints\nAs a matter of fact, we could declare as the most general format of an NLP problem the one with only inequality constraints. This is because we can always transform an equality constraint into two inequality constraints. Given an equality constraint h(\\bm x) = 0, we can write it as h(\\bm x) \\leq 0 and -h(\\bm x) \\leq 0, that is,\n\n\\underbrace{\\begin{bmatrix}\nh(\\bm x) \\\\\n-h(\\bm x)\n\\end{bmatrix}}_{\\mathbf g(\\bm x)} \\leq \\mathbf 0.\n\nOn the other hand, it may be useful to keep the equality constraints explicit in the problem formulation for the benefit of theoretical analysis, numerical methods and convenience of the user/modeller.",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#inequality-into-sort-of-equality-constraints",
    "href": "opt_theory_reformulations.html#inequality-into-sort-of-equality-constraints",
    "title": "Problem reformulations",
    "section": "Inequality into “sort-of” equality constraints",
    "text": "Inequality into “sort-of” equality constraints\nConsider the inequality constraint g(\\bm x) \\leq 0. By introducing a slack variable s and imposing the nonnegativity condition, we can turn the inequality into the equality g(\\bm x) + s = 0. Well, we have not completely discarded an inequality because now we have s \\geq 0. But this new problem may be better suited for some theoretical analysis or numerical methods.\nIt is also possible to express the nonnegativity constraint implicitly by considering an unrestricted variable s and using it within the inequality through its square s^2:\n\ng(\\bm x) + s^2 = 0.",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#linear-cost-function-always-possible",
    "href": "opt_theory_reformulations.html#linear-cost-function-always-possible",
    "title": "Problem reformulations",
    "section": "Linear cost function always possible",
    "text": "Linear cost function always possible\nGiven a cost function f(\\bm x) to be minimized, we can always upper-bound it by a new variable \\gamma accompanied by a new constraint f(\\bm x) \\leq \\gamma and then minimize just \\gamma \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm{x}\\in\\mathbb R^n, \\gamma\\in\\mathbb R} & \\quad \\gamma \\\\\n\\text{subject to} & \\quad f(\\bm x) \\leq \\gamma.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#absolute-value",
    "href": "opt_theory_reformulations.html#absolute-value",
    "title": "Problem reformulations",
    "section": "Absolute value",
    "text": "Absolute value\nConsider an optimization problem in which the cost function contains the absolute value of a variable \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\sum_i c_i|x_i|\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x \\geq \\mathbf b.\n\\end{aligned}\n\nWe also impose the restriction that all the coefficients c_i are nonnegative. The cost function is then a sum of piecewise linear convex function, which can be shown to be convex.\nThe trouble with the absolute value function is that it is not linear, it is not even smooth. And yet, as we will see below, this optimization with the absolute value can be reformulated as a linear program.\nOne possible reformulation introduces two new nonnegative (vector) variables \\bm x^+\\geq 0 and \\bm x^-\\geq 0 and with which the original variables can be expressed as x_i = x_i^+ - x_i^-, \\; i=1, \\ldots, n. The cost function can then be written as \\sum c_i|x_i| = \\sum_i c_i (x_i^+ + x_i^-).\nThis may look surprising (and incorrect) at first, but we argue that at an optimum, x_i^+ or x_i^- must be zero. Otherwise we could subtract (in case c_i&gt;0) the same amount from/to both, which would not change the satisfaction of the constraints (this modification cancels in x_i = x_i^+ - x_i^-), and the cost would be further reduced.\nThe LP in the standard form then changes to\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x^+\\in \\mathbb R^n, \\bm x^-\\in \\mathbb R^n} &\\quad \\mathbf c^\\top (\\bm x^+ + \\bm x^-)\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x^+ - \\mathbf A \\bm x^- \\geq \\mathbf b,\\\\\n&\\quad \\bm x^+ \\geq \\mathbf 0,\\\\\n&\\quad \\bm x^- \\geq \\mathbf 0.\n\\end{aligned}\n\nAnother possibility is to exploit the reformulation of z_i = |x_i| as x_i\\leq z and -x_i\\leq z. The original problem then transforms into\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm z\\in \\mathbb R^n, \\bm x\\in \\mathbb R^n} &\\quad \\mathbf c^\\top \\bm z\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x \\geq \\mathbf b,\\\\\n&\\qquad \\bm x \\leq \\bm z,\\\\\n&\\quad -\\bm x \\leq \\bm z.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#piecewise-linear",
    "href": "opt_theory_reformulations.html#piecewise-linear",
    "title": "Problem reformulations",
    "section": "Piecewise linear",
    "text": "Piecewise linear",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#quadratic",
    "href": "opt_theory_reformulations.html#quadratic",
    "title": "Problem reformulations",
    "section": "Quadratic",
    "text": "Quadratic",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html",
    "href": "opt_theory_unconstrained.html",
    "title": "Theory for unconstrained optimization",
    "section": "",
    "text": "Here we are going to analyze the optimization problem with no constraints \n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad  f(\\bm x).\nWhy are we considering such an unrealistic problem? After all, every engineering problem is subject to some constraints.\nBesides the standard teacher’s answer that we should always start with easier problems, there is another answer: it is common for analysis and algorithms for constrained optimization problems to reformulate them as unconstrained ones and then apply tools for unconstrained problems.",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html#local-vs-global-optimality",
    "href": "opt_theory_unconstrained.html#local-vs-global-optimality",
    "title": "Theory for unconstrained optimization",
    "section": "Local vs global optimality",
    "text": "Local vs global optimality\nFirst, let’s define carefully what we mean by a minimum in the unconstrained problem.\n\n\n\n\n\n\nCaution\n\n\n\nFor those whose mother tongue does not use articles such as the and a/an in English, it is worth emphasizing that there is a difference between “the minimum” and “a minimum”. In the former we assume that there is just one minimum, in the latter we make no such assumption.\n\n\nConsider a (scalar) function of a scalar variable for simplicity\n\nWe say, that the function has a local minimum at x^\\star if f(x)\\geq f(x^\\star) in an \\varepsilon neighbourhood. All the red dots in the above figure are local minima. Similarly, of course, the function has a local maximum at x^\\star if f(x)\\leq f(x^\\star) in an \\varepsilon neighbourhood. Such local maxima are the green dots in the figure. The smallest and the largest of these are global minima and maxima, respectively.",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html#conditions-of-optimality",
    "href": "opt_theory_unconstrained.html#conditions-of-optimality",
    "title": "Theory for unconstrained optimization",
    "section": "Conditions of optimality",
    "text": "Conditions of optimality\nHere we consider two types of conditions of optimality for unconstrained minimization problems: necessary and sufficient conditions. Necessary conditions must be satisfied at the minimum, but even when they are, the optimality is not guaranteed. On the other hand, the sufficient conditions need not be satisfied at the minimum, but if they are, the optimality is guaranteed. We show the necessary conditions of the first and second order, while the sufficient condition only of the second order.\n\nScalar optimization variable\nYou may want to have a look at the video, but below we continue with the text that summarizes the video.\n\nWe recall here the fundamental assumption made at the beginning of our introduction to optimization – we only consider optimization variables that are real-valued (first, just scalar x \\in \\mathbb R, later vectors \\bm x \\in \\mathbb R^n), and objective functions f() that are sufficiently smooth – all the derivatives exist. Then the conditions of optimality can be derived upon inspecting the Taylor series approximation of the cost function around the minimum.\n\nTaylor series approximation around the optimum\nDenote x^\\star as the (local) minimum of the function f(x). The Taylor series expansion of f(x) around x^\\star is\n\nf(x^\\star+\\alpha) = f(x^\\star)+\\left.\\frac{\\mathrm{d}f(x)}{\\mathrm{d} x}\\right|_{x=x^\\star}\\alpha + \\frac{1}{2}\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star}\\alpha^2 + {\\color{blue}\\mathcal{O}(\\alpha^3)},\n where \\mathcal{O}() is called Big O and has the property that \n\\lim_{\\alpha\\rightarrow 0}\\frac{\\mathcal{O}(\\alpha^3)}{\\alpha^3} \\leq M&lt;\\infty.\n\nAlternatively, we can write the Taylor series expansion as \nf(x^\\star+\\alpha) = f(x^\\star)+\\left.\\frac{\\mathrm{d}f(x)}{\\mathrm{d} x}\\right|_{x=x^\\star}\\alpha + \\frac{1}{2}\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star}\\alpha^2 + {\\color{red}o(\\alpha^2)},\n using the little o with the property that \n\\lim_{\\alpha\\rightarrow 0}\\frac{o(\\alpha^2)}{\\alpha^2} = 0.\n\nWhether \\mathcal{O}() or \\mathcal{o}() concepts are used, it is just a matter of personal preference. They both express that the higher-order terms in the expansion tend to be negligible compare to the first- and second-order term as \\alpha is getting smaller. \\mathcal O(\\alpha^3) goes to zero at least as fast as a cubic function, while o(\\alpha^2) goes to zero faster than a quadratic function.\nIt is indeed important to understand that this negligibility of the higher-order terms is only valid asymptotically – for a particular \\alpha it may easily happend that, say, the third-order term is still dominating.\n\n\nFirst-order necessary conditions of optimality\nFor \\alpha sufficiently small, the first-order Taylor series expansion is a good approximation of the function f(x) around the minimum. Since \\alpha enters this expansion linearly, the cost function can increase or decrease with \\alpha, depending on the sign of the first derivative. The only way to ensure that the function as a (local) minimu at x^\\star is to have the first derivative equal to zero, that is \\boxed{\n\\left.\\frac{\\mathrm{d}f(x)}{\\mathrm{d} x}\\right|_{x=x^\\star} = 0.}\n\n\n\nSecond-order necessary conditions of optimality\nOnce the first-order necessary condition of optimality is satisfied, the dominating term (as \\alpha is getting smalle) is the second-order term \\frac{1}{2}\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star}\\alpha^2. Since \\alpha is squared, it is the sign of the second derivative that determines the contribution of the whole second-order term to the cost function value. For the minimum, the second derivative must be nonnegative, that is\n\n\\boxed{\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star} \\geq 0.}\n\nFor completeness we state that the sign must be nonpositive for the maximum.\n\n\nSecond-order sufficient condition of optimality\nFollowing the same line of reasoning as above, the if the second derivative is positive, the miniumum is guaranteed, that is, the sufficient condition of optimality is \n\\boxed{\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star} &gt; 0.}\n\nIf the second derivative fails to be positive and is just zero (thus still satisfying the necessary condition), does it mean that the point is not a minimum? No. We must examine higher order terms.\n\n\n\nVector optimization variable\nOnce again, should you prefer watching a video, here it is, but below we continue with the text that covers the content of the video.\n\n\nFirst-order necessary conditions of optimality\nOne way to handle the vector variables is to convert the vector problem into a scalar one by fixing a direction to an arbitrary vector \\bm d and then considering the scalar function of the form f(\\bm x^\\star + \\alpha \\bm d). For convenience we define a new function \ng(\\alpha) \\coloneqq f(\\bm x^\\star + \\alpha \\bm d)\n and from now on we can invoke the results for scalar functions. Namely, we expand the g() function around zero as \ng(\\alpha) = g(0) + \\frac{\\mathrm{d}g(\\alpha)}{\\mathrm{d}\\alpha}\\bigg|_{\\alpha=0}\\alpha + \\frac{1}{2}\\frac{\\mathrm{d}^2 g(\\alpha)}{\\mathrm{d}\\alpha^2}\\bigg|_{\\alpha=0}\\alpha^2 + \\mathcal{O}(\\alpha^3),\n and argue that the first-order necessary condition of optimality is \n\\frac{\\mathrm{d}g(\\alpha)}{\\mathrm{d}\\alpha}\\bigg|_{\\alpha=0} = 0.\n\nNow, invoking the chain rule, we go back from g() to f() \n\\frac{\\mathrm{d}g(\\alpha)}{\\mathrm{d}\\alpha}\\bigg|_{\\alpha=0} = \\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star} \\frac{\\partial(\\bm x^\\star + \\alpha \\bm d)}{\\partial\\alpha}\\bigg|_{\\alpha=0} = \\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star}\\,\\bm d = 0,\n where \\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star} is a row vector of partial derivatives of f() evaluated at \\bm x^\\star. Since the vector \\bm d is arbitrary, the necessary condition is that\n\n\\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star} = \\mathbf 0,  \n\nMore often than not we use the column vector to store partial derivatives. We call it the gradient of the function f() and denoted it as \n\\nabla f(\\bm x) \\coloneqq \\begin{bmatrix}\\frac{\\partial f(\\bm x)}{\\partial x_1} \\\\ \\frac{\\partial f(\\bm x)}{\\partial x_n} \\\\ \\vdots \\\\ \\frac{\\partial f(\\bm x)}{\\partial x_n}\\end{bmatrix}.  \n\nThe first-order necessary condition of optimality using gradients is then \n\\boxed{\\left.\\nabla f(\\bm x)\\right|_{x=x^\\star} = \\mathbf 0.  }\n\n\n\n\n\n\n\nGradient is a column vector\n\n\n\nIn some literature the gradient \\nabla f(\\bm x) is defined as a row vector. For the condition of optimality it does not matteer since all we require is that all partial derivatives vanish. But for other purposes in our text we regard the gradient as a vector living in the same vector space \\mathbb R^n as the optimization variable. The row vector is sometimes denoted as \\mathrm Df(\\bm x).\n\n\n\nComputing the gradient of a scalar function of a vector variable\nA convenient way is to compute the differential fist and then to identify the derivative in it. Recall that the differential is the first-order approximation to the increment of the function due to a change in the variable\n\n\\Delta f \\approx \\mathrm{d}f = \\nabla f(x)^\\top \\mathrm d \\bm x.\n\nFinding the differential of a function is conceptually easier than finding the derivative since it is a scalar quantity. When searching for the differential of a composed function, we follow the same rules as for the derivative (such as that the one for finding the differential of a product). Let’s illustrate it using an example.\n\nExample 1 For the function \nf(\\mathbf x) = \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{r}^\\top\\bm{x},\n where \\mathbf Q is symetric, the differential is \n\\mathrm{d}f = \\frac{1}{2}\\mathrm d\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\mathrm d\\bm{x} + \\mathbf{r}^\\top\\mathrm{d}\\bm{x},\n in which the first two terms can be combined thanks to the fact that they are scalars \n\\mathrm{d}f = \\left(\\bm{x}^\\top\\frac{\\mathbf{Q} + \\mathbf{Q}^\\top}{2} + \\mathbf{r}^\\top\\right)\\mathrm{d}\\bm{x},\n and finally, since we assumed that \\mathbf Q is a symmetric matrix, we get \n\\mathrm{d}f = \\left(\\mathbf{Q}\\bm{x} + \\mathbf{r}\\right)^\\top\\mathrm{d}\\bm{x},\n from which we can identify the gradient as \n\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\mathbf{x} + \\mathbf{r}.\n\nThe first-order condition of optimality is then \n\\boxed{\\mathbf{Q}\\mathbf{x} = -\\mathbf{r}.}\n\nAlthough this was just an example, it is actually a very useful one. Keep this result in mind – necessary condition of optimality of a quadratic function comes in the form of a set of linear equations.\n\n\n\n\nSecond-order necessary conditions of optimality\nAs before, we fix the direction \\bm d and consider the function g(\\alpha) = f(\\bm x^\\star + \\alpha \\bm d). We expand the expression for the first derivative as \n\\frac{\\mathrm d g(\\alpha)}{\\mathrm d \\alpha} = \\sum_{i=1}^{n}\\frac{\\partial f(\\bm x)}{\\partial x_i}\\bigg|_{\\bm x = \\bm x^\\star} d_i,\n and differentiating this once again, we get the second derivative \n\\frac{\\mathrm d^2 g(\\alpha)}{\\mathrm d \\alpha^2} = \\sum_{i,j=1}^{n}\\frac{\\partial^2 f(\\bm x)}{\\partial x_ix_j}\\bigg|_{\\bm x = \\bm x^\\star}d_id_j\n\n\n\\begin{aligned}\n\\frac{\\mathrm d^2 g(\\alpha)}{\\mathrm d \\alpha^2}\\bigg|_{\\alpha=0} &= \\sum_{i,j=1}^{n}\\frac{\\text{d}^2}{\\text{d}x_ix_j}f(\\bm x)\\bigg|_{\\bm x = \\bm x^\\star}d_id_j\\\\\n&= \\mathbf d^\\text{T} \\underbrace{\\nabla^\\text{2}f(\\mathbf x)\\bigg|_{\\bm x = \\bm x^\\star}}_\\text{Hessian} \\mathbf d.\n\\end{aligned}\n where \\nabla^2 f(\\mathbf x) is the Hessian (the symmetrix matrix of the second-order mixed partial derivatives) \n\\nabla^2 f(x) = \\begin{bmatrix}\n                 \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_1^2} && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_1\\partial x_2} && \\ldots && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_1\\partial x_n}\\\\\n                 \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_2\\partial x_1} && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_2^2} && \\ldots && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_2\\partial x_n}\\\\\n                 \\vdots\\\\\n                 \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_n\\partial x_1} && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_n\\partial x_2} && \\ldots && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_n\\partial x_n}.\n                \\end{bmatrix}\n\nSince \\bm d is arbitrary, the second-order necessary condition of optimality is then \n\\boxed{\\nabla^\\text{2}f(\\mathbf x)\\bigg|_{\\bm x = \\bm x^\\star} \\succeq 0,}\n where, once again, the inequality \\succeq reads that the matrix is positive semidefinite.\n\n\nSecond-order sufficient condition of optimality\n\n\\boxed{\\nabla^2 f(\\mathbf x)\\bigg|_{\\bm x = \\bm x^\\star} \\succ 0,}\n where, once again, the inequality \\succ reads that the matrix is positive definite.\n\nExample 2 For the quadratic function f(\\mathbf x) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{Q}\\mathbf{x} + \\mathbf{r}^\\mathrm{T}\\mathbf{x}, the Hessian is \n\\nabla^2 f(\\mathbf{x}) = \\mathbf{Q}\n and the second-order necessary condition of optimality is \n\\boxed{\\mathbf{Q} \\succeq 0.}\n\nSecond-order sufficient condition of optimality is then \n\\boxed{\\mathbf{Q} \\succ 0.}\n\nOnce again, this was more than just an example – quadratic functions are so important for us that it is worth remembering this result.",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html#classification-of-stationary-points",
    "href": "opt_theory_unconstrained.html#classification-of-stationary-points",
    "title": "Theory for unconstrained optimization",
    "section": "Classification of stationary points",
    "text": "Classification of stationary points\nFor a stationary (also critical) \\bm x^\\star, that is, one that satisfies the first-order necessary condition \n\\nabla f(\\bm x^\\star) = 0,\n\nwe can classify it as\n\nMinimum: \\nabla^2 f(x^\\star)\\succ 0\nMaximum: \\nabla^2 f(x^\\star)\\prec 0\nSaddle point: \\nabla^2 f(x^\\star) indefinite\nSingular point (we cannot decide): \\nabla^2 f(x^\\star)=0\n\n\nExample 3 (Minimum of a quadratic function) We consider a quadratic function f(\\mathbf x) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{Q}\\mathbf{x} + \\mathbf{r}^\\mathrm{T}\\mathbf{x} for a particular \\mathbf{Q} and \\mathbf{r}.\n\n\nCode\nQ = [1 1; 1 2]\nr = [0, 1]\n\nusing LinearAlgebra\nx_stationary = -Q\\r\neigvals(Q) \n\n\n2-element Vector{Float64}:\n 0.38196601125010515\n 2.618033988749895\n\n\nThe matrix is positive definite, so the stationary point is a minimum. In fact, the minimum. Surface and contour plots of the function are shown below.\n\nCode\nf(x) = 1/2*dot(x,Q*x)+dot(x,r)\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ncontour(x1_data,x2_data,f_data)\ndisplay(plot!([x_stationary[1]], [x_stationary[2]], marker=:circle, label=\"Stationary point\"))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 1: Minimum of a quadratic function\n\n\n\n\n\nExample 4 (Saddle point of a quadratic function)  \n\n\nCode\nQ = [-1 1; 1 2]\nr = [0, 1]\n\nusing LinearAlgebra\nx_stationary = -Q\\r\neigvals(Q)\n\n\n2-element Vector{Float64}:\n -1.3027756377319946\n  2.302775637731995\n\n\nThe matrix is indefinite, so the stationary point is a saddle point. Surface and contour plots of the function are shown below.\n\nCode\nf(x) = 1/2*dot(x,Q*x)+dot(x,r)\n\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ncontour(x1_data,x2_data,f_data)\ndisplay(plot!([x_stationary[1]], [x_stationary[2]], marker=:circle, label=\"Stationary point\"))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 2: Saddle point of a quadratic function\n\n\n\n\n\nExample 5 (Singular point of a quadratic function)  \n\n\nCode\nQ = [2 0; 0 0]\nr = [3, 0]\n\nusing LinearAlgebra\neigvals(Q)\n\n\n2-element Vector{Float64}:\n 0.0\n 2.0\n\n\nThe matrix Q is singular, which has two consequences:\n\nWe cannot compute the stationary point since Q is not invertible. In fact, there is a whole line (a subspace) of stationary points.\nThe matrix Q is positive semidefinite, which generally means that optimality cannot be concluded. But in this particular case of a quadratic function, there are no higher-order terms in the Taylor series expansion, so the stationary point is a minimum.\n\nSurface and contour plots of the function are shown below.\n\nCode\nf(x) = 1/2*dot(x,Q*x)+dot(x,r)\n\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ndisplay(contour(x1_data,x2_data,f_data))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 3: Singular point of a quadratic function\n\n\n\n\n\nExample 6 (Singular point of a non-quadratic function) Consider the function f(\\bm x) = x_1^2 + x_2^4. Its gradient is \\nabla f(\\bm x) = \\begin{bmatrix}2x_1\\\\ 4x_2^3\\end{bmatrix} and it vanishes at \\bm x^\\star = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}. The Hessian is \\nabla^2 f(\\bm x) = \\begin{bmatrix}2 & 0\\\\ 0 & 12x_2^2\\end{bmatrix}, which when evaluated at the stationary point is \\nabla^2 f(\\bm x)\\bigg|_{\\bm x=\\mathbf 0} = \\begin{bmatrix}2 & 0\\\\ 0 & 0\\end{bmatrix}, which is positive semidefinite. We cannot conclude if the function attains a minimum at \\bm x^\\star.\nWe need to examine higher-order terms in the Taylor series expansion. The third derivatives are \n\\frac{\\partial^3 f}{\\partial x_1^3} = 0, \\quad \\frac{\\partial^3 f}{\\partial x_1^2\\partial x_2} = 0, \\quad \\frac{\\partial^3 f}{\\partial x_1\\partial x_2^2} = 0, \\quad \\frac{\\partial^3 f}{\\partial x_2^3} = 24x_2,\n and when evaluated at zero, they all vanish.\nAll but one fourth derivatives also vanish. The one that does not is \n\\frac{\\partial^4 f}{\\partial x_2^4} = 24,\n which is positive, and since the associated derivative is of the even order, the power si also even, hence the function attains a minimum at \\bm x^\\star = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}.\nThis can also be visually confirmed by the surface and contour plots of the function.\n\nCode\nf(x) = x[1]^2 + x[2]^4\n\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ncontour(x1_data,x2_data,f_data)\ndisplay(plot!([x_stationary[1]], [x_stationary[2]], marker=:circle, label=\"Stationary point\"))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 4: Singular point of a non-quadratic function",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_problems.html",
    "href": "opt_theory_problems.html",
    "title": "Optimization problems",
    "section": "",
    "text": "We formulate a general optimization problem (also a mathematical program) as \n\\begin{aligned}\n\\operatorname*{minimize} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\bm x \\in \\mathcal{X},\n\\end{aligned}\n where f is a scalar function, \\bm x can be a scalar, a vector, a matrix or perhaps even a variable of yet another type, and \\mathcal{X} is a set of values that \\bm x can take, also called the feasible set.\n\n\n\n\n\n\nNote\n\n\n\nThe term “program” here has nothing to do with a computer program. Instead, it was used by the US military during the WWII to refer to plans or schedules in training and logistics.\n\n\nIf maximization of the objective function f() is desired, we can simply multiply the objective function by -1 and minimize the resulting function.\nTypically there are two types of constraints that can be imposed on the optimization variable \\bm x:\n\nexplicit characterization of the set such as \\bm x \\in \\mathbb{R}^n or \\bm x \\in \\mathbb{Z}^n, possibly even a direction enumeration such as \\bm x \\in \\{0,1\\}^n in the case of binary variables,\nimplicit characterization of the set using equations and inequalities such as g_i(\\bm x) \\leq 0 and h_i(\\bm x) = 0 for i = 1, \\ldots, m and j = 1, \\ldots, p.\n\nAn example of a more structured and yet sufficiently general optimization problems over several real and integer variables is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^{n_x}, \\, \\bm y \\in \\mathbb{Z}^{n_y}} \\quad & f(\\bm x, \\bm y) \\\\\n\\text{subject to} \\quad & g_i(\\bm x, \\bm y) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_j(\\bm x, \\bm y) = 0, \\quad j = 1, \\ldots, p.\n\\end{aligned}\n\nIndeed, for named sets such as \\mathbb R or \\mathbb Z, it is common to place the set constraints directly underneath the word “minimize”. But it is just one convention, and these constraints could be listed into the “subject to” section as well.\n\n\n\n\n\n\nInteger optimization not in this course\n\n\n\nIn this course we are only going to consider optimization problems with real-valued variables. This decision does not suggest that optimization with integer variables is less relevant for optimal control, quite the opposite! It is just that the theory and algorithms for integer or mixed integer optimization are based on different principles than those for real variables. And they can hardly fit into a single course. Good news for the interested students is that a graduate course named Combinatorial algorithms (B3B35KOA) covering integer optimization in detail is offered by the Cybernetics and Robotics study program at CTU FEE. Applications of integer optimization to optimal control are part of the course on Hybrid systems (B3B39HYS).\n\n\nThe form of an optimization problem that we are going to use in our course most often than not is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & g_i(\\bm x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_i(\\bm x) = 0, \\quad i = 1, \\ldots, p,\n\\end{aligned}\n which can also be written using vector-valued functions (reflected in the use of the bold face for their names) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\mathbf g(\\bm x) \\leq 0,\\\\\n& \\mathbf h(\\bm x) = 0.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_theory_problems.html#optimization-problem-formulation",
    "href": "opt_theory_problems.html#optimization-problem-formulation",
    "title": "Optimization problems",
    "section": "",
    "text": "We formulate a general optimization problem (also a mathematical program) as \n\\begin{aligned}\n\\operatorname*{minimize} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\bm x \\in \\mathcal{X},\n\\end{aligned}\n where f is a scalar function, \\bm x can be a scalar, a vector, a matrix or perhaps even a variable of yet another type, and \\mathcal{X} is a set of values that \\bm x can take, also called the feasible set.\n\n\n\n\n\n\nNote\n\n\n\nThe term “program” here has nothing to do with a computer program. Instead, it was used by the US military during the WWII to refer to plans or schedules in training and logistics.\n\n\nIf maximization of the objective function f() is desired, we can simply multiply the objective function by -1 and minimize the resulting function.\nTypically there are two types of constraints that can be imposed on the optimization variable \\bm x:\n\nexplicit characterization of the set such as \\bm x \\in \\mathbb{R}^n or \\bm x \\in \\mathbb{Z}^n, possibly even a direction enumeration such as \\bm x \\in \\{0,1\\}^n in the case of binary variables,\nimplicit characterization of the set using equations and inequalities such as g_i(\\bm x) \\leq 0 and h_i(\\bm x) = 0 for i = 1, \\ldots, m and j = 1, \\ldots, p.\n\nAn example of a more structured and yet sufficiently general optimization problems over several real and integer variables is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^{n_x}, \\, \\bm y \\in \\mathbb{Z}^{n_y}} \\quad & f(\\bm x, \\bm y) \\\\\n\\text{subject to} \\quad & g_i(\\bm x, \\bm y) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_j(\\bm x, \\bm y) = 0, \\quad j = 1, \\ldots, p.\n\\end{aligned}\n\nIndeed, for named sets such as \\mathbb R or \\mathbb Z, it is common to place the set constraints directly underneath the word “minimize”. But it is just one convention, and these constraints could be listed into the “subject to” section as well.\n\n\n\n\n\n\nInteger optimization not in this course\n\n\n\nIn this course we are only going to consider optimization problems with real-valued variables. This decision does not suggest that optimization with integer variables is less relevant for optimal control, quite the opposite! It is just that the theory and algorithms for integer or mixed integer optimization are based on different principles than those for real variables. And they can hardly fit into a single course. Good news for the interested students is that a graduate course named Combinatorial algorithms (B3B35KOA) covering integer optimization in detail is offered by the Cybernetics and Robotics study program at CTU FEE. Applications of integer optimization to optimal control are part of the course on Hybrid systems (B3B39HYS).\n\n\nThe form of an optimization problem that we are going to use in our course most often than not is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & g_i(\\bm x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_i(\\bm x) = 0, \\quad i = 1, \\ldots, p,\n\\end{aligned}\n which can also be written using vector-valued functions (reflected in the use of the bold face for their names) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\mathbf g(\\bm x) \\leq 0,\\\\\n& \\mathbf h(\\bm x) = 0.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_theory_problems.html#properties-of-optimization-problems",
    "href": "opt_theory_problems.html#properties-of-optimization-problems",
    "title": "Optimization problems",
    "section": "Properties of optimization problems",
    "text": "Properties of optimization problems\nIt is now the presence/absence and the properties of individual components in the optimization problem defined above that characterize classes of optimization problems. In particular, we can identify the following properties:\n\nUnconstrained vs constrained\n\nPractically relevant problems are almost always constrained. But still there are good reasons to study unconstrained problems too, as many theoretical results and algorithms for constrained problems are based on transformations to unconstrained problems.\n\nLinear vs nonlinear\n\nBy linear problems we mean problems where the objective function and all the functions defining the constraints are linear (or actually affine) functions of the optimization variable \\bm x. Such problems constitute the simplest class of optimization problems, are very well understood, and there are efficient algorithms for solving them. In contrast, nonlinear problems are typically more difficult to solve (but see the discussion of the role of convexity below).\n\nSmooth vs nonsmooth\n\nEfficient algorithms for optimization over real variables benefit heavily from knowledge of the derivatives of the objective and constraint functions. If the functions are differentiable (aka smooth), we say that the whole optimization problem is smooth. Nonsmooth problems are typically a more difficult to analyze and solve (but again, see the discussion of the role of convexity below).\n\nConvex vs nonconvex\n\nIf the objective function and the feasible set are convex (the latter holds when the functions defining the inequality constraints are convex and the functions defining the equality constrains are affine), the whole optimization problem is convex. Convex optimization problems are very well understood and there are efficient algorithms for solving them. In contrast, nonconvex problems are typically more difficult to solve. It turns out that convexity is a lot more important property than linearity and smoothness when it comes to solving optimization problems efficiently.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_theory_problems.html#classes-of-optimization-problems",
    "href": "opt_theory_problems.html#classes-of-optimization-problems",
    "title": "Optimization problems",
    "section": "Classes of optimization problems",
    "text": "Classes of optimization problems\nBased on the properties discussed above, we can identify the following distinct classes of optimization problems:\n\nLinear program (LP)\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\mathbf A_\\mathrm{ineq}\\bm x \\leq \\mathbf b_\\mathrm{ineq},\\\\\n& \\mathbf A_\\mathrm{eq}\\bm x = \\mathbf b_\\mathrm{eq}.\n\\end{aligned}\n\nAn LP is obviously linear, hence it is also smooth and convex.\nSome theoretical results and numerical algorithms require a linear program in a specific form, called the standard form: \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\mathbf A\\bm x = \\mathbf b,\\\\\n& \\bm x \\geq \\mathbf 0,\n\\end{aligned}\n where the inequality \\bm x \\geq \\mathbf 0 is understood componentwise, that is, x_i \\geq 0 for all i = 1, \\ldots, n.\n\n\nQuadratic program (QP)\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\bm x^\\top \\mathbf Q \\bm x +  \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\mathbf A_\\mathrm{ineq}\\bm x \\leq \\mathbf b_\\mathrm{ineq},\\\\\n& \\mathbf A_\\mathrm{eq}\\bm x = \\mathbf b_\\mathrm{eq}.\n\\end{aligned}\n\nEven though the QP is nonlinear, it is smooth and if the matrix \\mathbf Q is positive semidefinite, it is convex. Its analysis and numerical solution are not much more difficult than those of an LP problem.\n\nQuadratically constrained quadratic program (QCQP)\nIt is worth emphasizing that for the standard QP the constraints are still given by a system of linear equations and inequalities. Sometimes we can encounter problems in which not only the cost function but also the functions defining the constraints are quadratic as in \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\bm x^\\top \\mathbf Q \\bm x +  \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\bm x^\\top \\mathbf A_i\\bm x + \\mathbf b_i \\bm x + c_i \\leq \\mathbf 0, \\quad i=1, \\ldots, m.\n\\end{aligned}\n\nA QCQP problem is convex if and only if the the constraints define a convex feasible set, which is the case when all the matrices \\mathbf A_i are positive semidefinite.\n\n\n\nConic program (CP)\nFirst, what is a cone? It is a set such that if something is in the cone, then a multiple of it by a nonnegative number is still in the set. We are going to restrict ourselves to regular cones, which are are pointed, closed and convex. An example of such regular cone in a plane is in Figure 1 below.\n\n\n\n\n\n\nFigure 1: Regular (pointed, convex, closed) cone in a plane\n\n\n\nNow, what is the point in using cones in optimization? Reformulation of nonlinear optimization problems using cones constitutes a systematic way to identify what these (conic) optimization problems have in common with linear programs, for which powerful theory and efficient algorithms exist.\nNote that an LP in the standard form can be written as \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad \\bm x\\in \\mathbb{R}_+^n,\n\\end{aligned}\n where \\mathbb R_+^n is a positive orthant. Now, the positive orthant is a convex cone! We can then see the LP as an instance of a general conic optimization problem (conic program)\n\n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad \\bm x\\in \\mathcal{K},\n\\end{aligned}\n where \\mathcal{K} is a cone in \\mathbb R^n.\n\n\n\n\n\n\nInequality as belonging to a cone\n\n\n\nA fundamental idea unrolled here: the inequality \\bm x\\geq 0 can be interpreted as \\bm x belonging to a componentwise nonegative cone, that is \\bm x \\in \\mathbb R_+^n. What if some other cone \\mathcal K is considered? What would be the interpretation of the inequality then?\n\n\nSometimes in order to emphasize that the inequality is induced by the cone \\mathcal K, we write it as \\geq_\\mathcal{K}. Another convention – the one we actually adopt here – is to use another symbol for the inequality \\succeq to distinguish it from the componentwise meaning, assuming that the cone is understood from the context. We then interpret conic inequalities such as \n\\mathbf A_\\mathrm{ineq}\\bm x \\succeq \\mathbf b_\\mathrm{ineq}\n in the sense that \n\\mathbf A_\\mathrm{ineq}\\bm x - \\mathbf b_\\mathrm{ineq} \\in \\mathcal{K}.\n\nIt is high time to explore some concrete cones (other than the positive orthant). We consider just two, but there are a few more, see the literature.\n\nSecond-order cone program (SOCP)\nThe most immediate cone in \\mathbb R^n is the second-order cone, also called the Lorentz cone or even the ice cream cone. We explain it in \\mathbb R^3 for the ease of visualization, but generalization to \\mathbb R^n is straightforward. The second-order cone in \\mathbb R^3 is defined as \n\\mathcal{K}_\\mathrm{SOC}^3 = \\left\\{ \\bm x \\in \\mathbb R^3 \\mid \\sqrt{x_1^2 + x_2^2} \\leq x_3 \\right\\}.\n\nand is visualized in Figure 2 below.\n\n\n\n\n\n\n\n\nFigure 2: A second-order cone in 3D\n\n\n\n\n\nWhich of the three axes plays the role of the axis of symmetry for the cone must be agreed beforhand. Singling this direction out, the SOC in \\mathbb R^n can also be formulated as \n\\mathcal{K}_\\mathrm{SOC}^n = \\left\\{ (\\bm x, t) \\in \\mathbb R^{n-1} \\times \\mathbb R \\mid \\|\\bm x\\|_2 \\leq t \\right\\}.\n\nA second-order conic program in standard form is then \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad \\bm x\\in \\mathcal{K}_\\mathrm{SOC}^n,\n\\end{aligned}\n\nwhich can be written explicitly as \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad x_1^2 + \\cdots + x_{n-1}^2 - x_n^2 \\leq 0.\n\\end{aligned}\n\nA second-order conic program can also come in non-standard form such as \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A_\\mathrm{ineq}\\bm x \\succeq  \\mathbf b_\\mathrm{ineq}.\n\\end{aligned}\n\nAssuming the data is structured as \n\\begin{bmatrix}\n\\mathbf A\\\\\n\\mathbf c^\\top\n\\end{bmatrix}\n\\bm x \\succeq\n\\begin{bmatrix}\n\\mathbf b\\\\\nd\n\\end{bmatrix},\n the inequality can be rewritten as \n\\begin{bmatrix}\n\\mathbf A\\\\\n\\mathbf c^\\top\n\\end{bmatrix}\n\\bm x -\n\\begin{bmatrix}\n\\mathbf b\\\\\nd\n\\end{bmatrix} \\in \\mathcal{K}_\\mathrm{SOC}^n,\n which finally gives \n\\|\\mathbf A \\bm x - \\mathbf b\\|_2 \\leq \\mathbf c^\\top \\bm x + d.\n\nTo summarize, another form of a second-order cone program (SOCP) is\n\n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A_\\mathrm{eq}\\bm x = \\mathbf b_\\mathrm{eq},\\\\\n&\\quad \\|\\mathbf A \\bm x - \\mathbf b\\|_2 \\leq \\mathbf c^\\top \\bm x + d.\n\\end{aligned}\n\nWe can see that the SOCP contains both linear and quadratic constraints, hence it generalizes LP and QP, including convex QCQP. To see the latter, expand the square of \\|\\mathbf A \\bm x - \\mathbf b\\|_2 into (\\bm x^\\top \\mathbf A^\\top  - \\mathbf b^\\top)(\\mathbf A \\bm x - \\mathbf b) = \\bm x^\\top \\mathbf A^\\top \\mathbf A \\bm x + \\ldots\n\n\nSemidefinite program (SDP)\nAnother cone of great importance the control theory is the cone of positive semidefinite matrices. It is commonly denoted as \\mathcal S_+^n and is defined as \n\\mathcal S_+^n = \\left\\{ \\bm X \\in \\mathbb R^{n \\times n} \\mid \\bm X = \\bm X^\\top, \\, \\bm z^\\top \\bm X \\bm z \\geq 0\\; \\forall \\bm z\\in \\mathbb R^n \\right\\},\n and with this cone the inequality \\mathbf X \\succeq 0 is a common way to express that \\mathbf X is positive semidefinite.\nUnlike the previous classes of optimization problems, this one is formulated with matrix variables instead of vector ones. But nothing prevents us from collecting the components of a symmetric matrix into a vector and proceed with vectors as usual, if needed:\n\n\\bm X = \\begin{bmatrix} x_1 & x_2 & x_3 \\\\ x_2 & x_4 & x_5\\\\ x_3 & x_5 & x_6 \\end{bmatrix},\n\\quad\n\\bm x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_6 \\end{bmatrix}.\n\nAn optimization problem with matrix variables constrained to be in the cone of semidefinite matrices (or their vector representations) is called a semidefinite program (SDP). As usual, we start with the standard form, in which the cost function is linear and the optimization is subject to an affine constraint and a conic constraint. In the following, in place of the inner products of two vectors \\mathbf c^\\top x we are going to use inner products of matrices defined as \n\\langle \\mathbf C, \\bm X\\rangle = \\operatorname{Tr} \\mathbf C \\bm X,\n where \\operatorname{Tr} is a trace of a matrix defined as the sum of the diagonal elements.\nThe SDP program in the standard form is then \n\\begin{aligned}\n\\operatorname{minimize}_{\\bm X} &\\quad \\operatorname{Tr} \\mathbf C \\bm X\\\\\n\\text{subject to} &\\quad \\operatorname{Tr} \\mathbf A_i \\bm X = \\mathbf b_i, \\quad i=1, \\ldots, m,\\\\\n&\\quad \\bm X \\in \\mathcal S_+^n,\n\\end{aligned}\n where the latter constraint is more often than not written as \\bm X \\succeq 0, understanding from the context that the cone of positive definite matrices is assumed.\n\n\nOther conic programs\nWe are not going to cover them here, but we only enumerate a few other cones useful in optimization: rotated second-order cone, exponential cone, power cone, … A concise overview is in (“MOSEK Modeling Cookbook” 2024)\n\n\n\nGeometric program (GP)\n\n\nNonlinear program (NLP)\nFor completeness we include here once again the general nonlinear programming problem:\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\mathbf g(\\bm x) \\leq 0,\\\\\n& \\mathbf h(\\bm x) = 0.\n\\end{aligned}\n\nSmoothness of the problem can easily be determined based on the differentiability of the functions. Convexity can also be determined by inspecting the functions, but this is not necessarily easy. One way to check convexity of a function is to view it as a composition of simple functions and exploit the knowledge about convexity of these simple functions. See (Boyd and Vandenberghe 2004, sec. 3.2)",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_algo_solvers.html",
    "href": "opt_algo_solvers.html",
    "title": "Numerical solvers",
    "section": "",
    "text": "The number of numerical solvers is huge. First, we give a short biased list of solvers which we may use within this course.\n\nOptimization Toolbox for Matlab: fmincon, fminunc, linprog, quadpro, … Available within the all-university Matlab license for all students and employees at CTU.\nGurobi Optimizer: LP, QP, SOCP, MIP, commercial (but free academic license available).\nIBM ILOG CPLEX: LP, QP, SOCP, MIP, commercial (but free academic license available).\nMOSEK: LP, QP, MIP, SOCP, SDP, commercial (but free academic license available).\nHIGHS: LP, QP, MIP, open source.\nKnitro: NLP, commercial.\nIpopt: NLP, open source.\nSEDUMI: SOCP, SDP, open source.\n…\n\nSecond, for a reasonably comprehensive and well maintained list of solvers, consult the NEOS Guide to Optimization web page (in particular the link at the bottom of that page). Similar list is maintained within Hans Mittelman’s Decision Tree for Optimization Software web page.\nWorking in Matlab and using Yalmip for defining and solving optimization problems, the list of optimization solvers supported by Yalmip shows what is available.\nSimilarly, users of Julia and JuMP will find the list of solvers supported by JuMP useful. The list is worth consulting even if Julia is not the tool of choice, as many solvers are indepdenent of Julia.\n\n\n\n Back to top",
    "crumbs": [
      "2. Optimization – algorithms",
      "Numerical solvers"
    ]
  },
  {
    "objectID": "cont_indir_time_optimal.html",
    "href": "cont_indir_time_optimal.html",
    "title": "Time-optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Time-optimal control"
    ]
  },
  {
    "objectID": "discr_dir_references.html",
    "href": "discr_dir_references.html",
    "title": "References",
    "section": "",
    "text": "The crucial message of this chapter — the concept of model predictive control (MPC) — has been described in a number of dedicated monographs and textbooks. Particularly recommendable are (Rawlings, Mayne, and Diehl 2017) and (Borrelli, Bemporad, and Morari 2017). They are not only reasonably up-to-date, written by leaders in the field, but they are also available online.\nSome updates as well as additional tutorial are in (Raković and Levine 2019), which seems to be available to CTU students through the institutional access.\nThere seems to be no shortage of lecture notes and slides as well. Particularly recommendable are the course slides (Bemporad 2021) and (Boyd n.d.).\nExtensions towards nonlinear systems are described in (Grüne and Pannek 2017), which also seems to be available to CTU students through the institutional access.\nSince MPC essentially boils down to solving optimization problems in real time on some industrial device, the topic of embedded optimization is important. Nice overview is given in (Ferreau et al. 2017).\n\n\n\n\n Back to topReferences\n\nBemporad, Alberto. 2021. “Model Predictive Control.” Lecture Slides. http://cse.lab.imtlucca.it/~bemporad/teaching/mpc/imt/1-linear_mpc.pdf.\n\n\nBorrelli, Francesco, Alberto Bemporad, and Manfred Morari. 2017. Predictive Control for Linear and Hybrid Systems. Cambridge, New York: Cambridge University Press. http://cse.lab.imtlucca.it/~bemporad/publications/papers/BBMbook.pdf.\n\n\nBoyd, Stephen. n.d. “Model Predictive Control (EE364b - Convex Optimization II.).” Lecture Slides. Stanford University. Accessed February 25, 2019. https://stanford.edu/class/ee364b/lectures/mpc_slides.pdf.\n\n\nFerreau, H. J., S. Almér, R. Verschueren, M. Diehl, D. Frick, A. Domahidi, J. L. Jerez, G. Stathopoulos, and C. Jones. 2017. “Embedded Optimization Methods for Industrial Automatic Control.” IFAC-PapersOnLine, 20th IFAC World Congress, 50 (1): 13194–209. https://doi.org/10.1016/j.ifacol.2017.08.1946.\n\n\nGrüne, Lars, and Jürgen Pannek. 2017. Nonlinear Model Predictive Control: Theory and Algorithms. 2nd ed. Communications and Control Engineering. Cham: Springer. https://doi.org/10.1007/978-3-319-46024-6.\n\n\nRaković, Saša V., and William S. Levine, eds. 2019. Handbook of Model Predictive Control. Control Engineering. Birkhäuser Basel. https://www.springer.com/us/book/9783319774886.\n\n\nRawlings, James B., David Q. Mayne, and Moritz M. Diehl. 2017. Model Predictive Control: Theory, Computation, and Design. 2nd ed. Madison, Wisconsin: Nob Hill Publishing, LLC. http://www.nobhillpublishing.com/mpc-paperback/index-mpc.html.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "References"
    ]
  },
  {
    "objectID": "rocond_references.html",
    "href": "rocond_references.html",
    "title": "References",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "12. Robust control",
      "References"
    ]
  },
  {
    "objectID": "opt_algo_references.html",
    "href": "opt_algo_references.html",
    "title": "References",
    "section": "",
    "text": "Pretty much identical to the literature recommended in the previous section on optimization theory.\nSome practical aspects are discussed in Guidelines for Numerical Issues for Gurobi Optimizer.\n\n\n\n Back to top",
    "crumbs": [
      "2. Optimization – algorithms",
      "References"
    ]
  },
  {
    "objectID": "cont_indir_CARE.html",
    "href": "cont_indir_CARE.html",
    "title": "Continuous-time Riccati equation",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Continuous-time Riccati equation"
    ]
  },
  {
    "objectID": "roban_uncertainty.html",
    "href": "roban_uncertainty.html",
    "title": "Uncertainty (in) modelling",
    "section": "",
    "text": "Through this chapter we are stepping into the domain of robust control. We need to define a few keywords first.\nWhile these two terms are used in many other fields, here we are tailoring them to the discipline of control systems, in particular their model-based design.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Uncertainty (in) modelling"
    ]
  },
  {
    "objectID": "roban_uncertainty.html#origins-of-uncertainty-in-models",
    "href": "roban_uncertainty.html#origins-of-uncertainty-in-models",
    "title": "Uncertainty (in) modelling",
    "section": "Origins of uncertainty in models?",
    "text": "Origins of uncertainty in models?\n\nPhysical parameters are not known exactly (say, they are known to be within ±10% or ±3σ interval around the nominal value).\nEven if the physical parameters are initially known with a high accuracy, they can evolve in time, unmeasured.\nThere may be variations among the individual units of the same product.\nIf a nonlinear system is planned to be operated around a given operating point, it can be linearized aroud that operating point, which gives a nominal linear model. If the system is then operated in a significantly different operating point, the corresponding linear model is different from the nominal one.\nOur understanding of the underlying physics (or chemistry or biology or …) is imperfect, hence our model is imperfect too. In fact, our understading can even be incorrect, in which case the model contains some discrepancies too. The imperfections of the model are typically observed at higher frequencies (referring to the frequency-domain modeling such as transfer functions).\nEven if we are able to eventually capture full dynamics of the system in a model, we may opt not to do so. We may want to keep the model simple, even if less accurate, because time invested into modelling is not for free.\nEven if we can get a high-fidelity model with a reasonable effort, we may still prefer using a simpler (and less accurate) model for a controller design. The reason is that very often the complexity of the model used for model-based control design is reflected by the complexity of the controller – and high-complexity controllers are not particularly appreciated in industry.",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Uncertainty (in) modelling"
    ]
  },
  {
    "objectID": "roban_uncertainty.html#models-of-uncertainty",
    "href": "roban_uncertainty.html#models-of-uncertainty",
    "title": "Uncertainty (in) modelling",
    "section": "Models of uncertainty",
    "text": "Models of uncertainty\nThere are several approaches to model the uncertainty (or, in other words, to characterize the uncertainty in the model). They all aim – in one way or another – to express that the controller has to deal no only with the single nominal system, for which it was designed, but a family of systems. Depending on the mathematical frameworks used for characterization of such a family, there are two major classes of approaches.\n\nWorst-case models of uncertainty\nProbabilistic models of uncertainty\n\nThe former assumes sets of systems with no additional information about the structure of such sets. The latter imposes some probability structure on the set of systems – in other words, although in principle any member of the set possible, some may be more probable than the others. In this course we are focusing on the former, which is also the mainstream in the robust control literature, but note that the latter we already encountered while considering control for systems exposed to random disturbances, namely the LQG control. A possible viewpoint is that as a consequence of the random disturbance, the controller has to deal with a family of systems.\nAnother classification of models of uncertainty is according to the actual quantity that is uncertain. We distinguish these two\n\nParametric uncertainty\nFrequency-dependent (aka dynamical) uncertainty\n\nUnstructured uncertainty\nStructured uncertainty\n\n\n\nParametric uncertainty\nThis is obviously straightforward to state: some (real/physical) parameters are uncertain. The conceptually simplest way to characterize such uncertain parameters is by considering intervals instead of just single (nominal) values.\n\nExample 1 (A pendulum on a cart) \n\\begin{aligned}\n{\\color{red} m_\\mathrm{l}} & \\in [m_\\mathrm{l}^{-},m_\\mathrm{l}^{+}],\\\\\n{\\color{red} l} & \\in [l^{-}, l^{+}],\n\\end{aligned}\n\n\n\\dot{\\bm x}(t) =\n\\begin{bmatrix}\n0 & 1 & 0 & 0\\\\\n0 & 0 & \\frac{\\textcolor{red}{m_\\mathrm{l}}}{m_\\mathrm{c}} g & 0\\\\\n0 & 0 & 0 & 1\\\\\n0 & 0 & -\\frac{(\\textcolor{red}{m_\\mathrm{l}}+m_\\mathrm{c})g}{m_\\mathrm{c}\\textcolor{red}{l}} & 0\n\\end{bmatrix}\n\\bm x(t)\n+\n\\begin{bmatrix}\n0\\\\\n\\frac{1}{m_\\mathrm{c}}\\\\\n0\\\\\n-\\frac{1}{m_\\mathrm{c}\\textcolor{red}{l}}\n\\end{bmatrix}\nu(t).\n\n\n\n\nUnstructured frequency-dependent uncertainty\nNot only some enumerated physical parameters but even the order of the system can be uncertain. In other words, there may be some phenomena exhibitted by the system that is not captured by the model at all. Possibly some lightly damped modes, possibly some time delay here and there. The system contains uncertain dynamics. In the linear case, all this can be expressed by regarding the magnitude and phase responses uncertain without mapping these to actual physical parameters.\n\n\n\n\n\n\nFigure 1: A whole subsystem is uncertain\n\n\n\nA popular model for the uncertain subsystem is that of a transfer function \\Delta(s), about which we know only that it is stable and that its magnitude is bounded by 1 \\boxed\n{\\sup_{\\omega}|\\Delta(j\\omega)|\\leq 1,\\;\\;\\Delta \\;\\text{stable}. }\n\nBut typically the uncertainty is higher at higher frequencies. This can be expressed by using some weighting function w(\\omega).\nFor later theoretical and computational purposes we approximate the real weighting function using a low-order rational stable transfer function W(s). That is, W(j\\omega)\\approx w(\\omega) for \\omega \\in \\mathbb R, that is for s=j\\omega on the imaginary axis.\nThe ultimate transfer function model of the uncertainty is then\n\\boxed{\nW(s)\\;\\Delta(s),\\quad \\max_{\\omega}|\\Delta(j\\omega)|\\leq 1,\\;\\;\\Delta\\; \\text{stable}. }\n\n\n\\mathcal H_\\infty norm of an LTI system\n\nH-infinity norm of an LTI system interpreted in frequency domain\n\nDefinition 4 (\\mathcal H_\\infty norm of a SISO LTI system) For a stable LTI system G with a single input and single output, the \\mathcal H_\\infty norm is defined as \n\\|G\\|_{\\infty} = \\sup_{\\omega\\in\\mathbb{R}}|G(j\\omega)|.\n\n\nHaving just defined the \\mathcal H_\\infty norm, the uncertainty model can be expressed compactly as \\boxed{\nW(s)\\;\\Delta(s),\\quad \\|\\Delta(j\\omega)\\|\\leq 1. }\n\n\n\n\n\n\n\n\\mathcal H_\\infty as a space of functions\n\n\n\n\\mathcal H_\\infty denotes a normed vector space of functions that are analytic in the closed extended right half plane (of the complex plane). In parlance of control systems, \\mathcal H_\\infty is the space of proper and stable transfer functions. Poles on the imaginary axis are not allowed. The functions do not need to be rational, but very often we do restrict ourselves to rational functions, in which case we typically write such space as \\mathcal{RH}_\\infty.\n\n\nWe now extend the concept of the \\mathcal H_\\infty norm to MIMO systems. The extension is perhaps not quite intuitive – certainly it is not computed as the maximum of the norms of individual transfer functions, which may be the first guess.\n\nDefinition 5 (\\mathcal H_\\infty norm of a MIMO LTI system) For a stable LTI system \\mathbf G with multiple inputs and/or multiple outputs, the \\mathcal H_\\infty norm is defined as \n\\|\\mathbf G\\|_{\\infty} = \\sup_{\\omega\\in\\mathbb{R}}\\bar{\\sigma}(\\mathbf{G}(j\\omega))\n where \\bar\\sigma is the largest singular value.\n\nHere we include a short recap of singular values and singular value decomposition (SVD) of a matrix. Consider a matrix \\mathbf M, possibly a rectangular one. It can be decomposed as a product of three matrices \n\\mathbf M = \\mathbf U\n\\underbrace{\n\\begin{bmatrix}\n\\sigma_1 & & & &\\\\\n  & \\sigma_2 & & &\\\\\n  & &\\sigma_3 & &\\\\\n\\\\\n  & & & & \\sigma_n\\\\\n\\end{bmatrix}\n}_{\\boldsymbol\\Sigma}\n\\mathbf V^{*}.\n\nThe two square matrices \\mathbf V and \\mathbf U are unitary, that is, \n\\mathbf V\\mathbf V^*=\\mathbf I=\\mathbf V^*\\mathbf V\n and \n\\mathbf U\\mathbf U^*=\\mathbf I=\\mathbf U^*\\mathbf U.\n\nThe nonnegative diagonal entries \\sigma_i \\in \\mathbb R_+, \\forall i of the (possibly rectangular) matrix \\Sigma are called singular values. Commonly they are ordered in a nonincreasing order, that is \n\\sigma_1\\geq \\sigma_2\\geq \\sigma_3\\geq \\ldots \\geq \\sigma_n.\n\nIt is also a common notation to denote the largest singular value as \\bar \\sigma, that is, \\bar \\sigma \\coloneqq \\sigma_1.\n\n\n\\mathcal{H}_{\\infty} norm of an LTI system interpreted in time domain\nWe can also view the dynamical system G with inputs and outputs as an operator mapping from some chosen space of functions to another space of functions. A popular model for these spaces are the spaces of square-integrable functions, denoted as \\mathcal{L}_2, and sometimes interpreted as bounded-energy signals \nG:\\;\\mathcal{L}_2\\rightarrow \\mathcal{L}_2.\n\nIt is a powerful fact that the \\mathcal{H}_{\\infty} norm of the system is then defined as the induced norm of the corresponding operator \\boxed{\n\\|G(s)\\|_{\\infty} = \\sup_{u(t)\\in\\mathcal{L}_2\\setminus 0}\\frac{\\|y(t)\\|_2}{\\|u(t)\\|_2}}.\n\nWith the energy interpretation of the input and output variables, this system norm can also be interpreted as the worst-case energy gain of the system.\nScaling necessary to get any useful info from MIMO models! See Skogestad’s book, section 1.4, pages 5–8. \n\n\n\nHow does the uncertainty enter the model of the system?\n\nAdditive uncertainty\nThe transfer function of an uncertain system can be written as a sum of a nominal system and an uncertainty \nG(s) = \\underbrace{G_0(s)}_{\\text{nominal model}}+\\underbrace{W(s)\\Delta(s)}_{\\text{additive uncertainty}}.\n\nThe block diagram interpretation is in\n\n\n\n\n\n\nFigure 2: Additive uncertainty\n\n\n\nThe magnitude frequency response of the weighting filter W(s) then serves as an upper bound on the absolute error in the magnitude frequency responses \n|G(j\\omega)-G_0(j\\omega)|&lt;|W(j\\omega)|\\quad \\forall \\omega\\in\\mathbb R.\n\n\n\nMultiplicative uncertainty\n\nG(s) = (1+W(s)\\Delta(s))\\,G_0(s).\n\nThe block diagram interpretation is in\n\n\n\n\n\n\nFigure 3: Multiplicative uncertainty\n\n\n\n\n\n\n\n\n\nFor SISO transfer functions no need to bother about the order of terms in the products\n\n\n\nSice we are considering SISO transfer functions, the order of terms in the products is not important. We will have to be more alert to the order of terms when we move to MIMO systems.\n\n\nThe magnitude frequency response of the weighting filter W(s) then serves as an upper bound on the relative error in the magnitude frequency responses \\boxed\n{\\frac{|G(j\\omega)-G_0(j\\omega)|}{|G_0(j\\omega)|}&lt;|W(j\\omega)|\\quad \\forall \\omega\\in\\mathbb R.}\n\n\nExample 2 (Uncertain first-order delayed system) We consider a first-order system with a delay described by \nG(s) = \\frac{k}{T s+1}e^{-\\theta s}, \\qquad 2\\leq k,\\tau,\\theta,\\leq 3.\n\n\n\n\\boxed{\nW(s) = \\frac{\\tau s+r_0}{(\\tau/r_{\\infty})s+1}}\n where r_0 is uncertainty at steady state, 1/\\tau is the frequency, where the relative uncertainty reaches 100%, r_{\\infty} is relative uncertainty at high frequencies, often r_{\\infty}\\geq 2.\n\n\nInverse additive uncertainty\n…\n\n\nInverse multiplicative uncertainty\n…\n\n\nLinear fractional transformation (LFT)\nFor a matrix \\mathbf P sized (n_1+n_2)\\times(m_1+m_2) and divided into blocks like \n\\mathbf P=\n\\begin{bmatrix}\n\\mathbf P_{11} & \\mathbf P_{12}\\\\\n\\mathbf P_{21} & \\mathbf P_{22}\n\\end{bmatrix},\n and a matrix \\mathbf K sized m_2\\times n_2, the lower LFT of \\mathbf P with respect to \\mathbf K is \n\\boxed{\n\\mathcal{F}_\\mathbf{l}(\\mathbf P,\\mathbf K) = \\mathbf P_{11}+\\mathbf P_{12}\\mathbf K(\\mathbf I-\\mathbf P_{22}\\mathbf K)^{-1}\\mathbf P_{21}}.\n\nIt can be viewed as a feedback interconnection of the plant \\mathbf P and the controller \\mathbf K, in which not all plant inputs are used as control inputs and not all plant outputs are measured, as depicted in Figure 4\n\n\n\n\n\n\nFigure 4: Lower LFT of \\mathbf P with respect to \\mathbf K\n\n\n\nSimilarly, for a matrix \\mathbf N sized (n_1+n_2)\\times(m_1+m_2) and a matrix \\boldsymbol\\Delta sized m_1\\times n_1, the upper LFT of \\mathbf N with respect to \\mathbf K is \n\\boxed{\n\\mathcal{F}_\\mathbf{u}(\\mathbf N,\\boldsymbol\\Delta) = \\mathbf N_{22}+\\mathbf N_{21}\\boldsymbol\\Delta(\\mathbf I-\\mathbf N_{11}\\boldsymbol\\Delta)^{-1}\\mathbf N_{12}}.\n\nIt can be viewed as a feedback interconnection of the nominal plant \\mathbf N and the uncertainty block \\boldsymbol\\Delta, as depicted in Figure 5\n\n\n\n\n\n\nFigure 5: Upper LFT of \\mathbf N with respect to \\boldsymbol \\Delta\n\n\n\nHere we already anticipated MIMO uncertainty blocks. One motivation for them is explained in the very next section on structured uncertainties, another one is appearing once we start formulating robust performance within the same analytical framework as robust stability.\n\n\n\n\n\n\nWhich is lower and which is upper is a matter of convention, but a useful one\n\n\n\nOur usage of the lower LFT for a feedback interconnection of a (generalized) plant and a controller and the upper LFT for a feedback interconnection of a nominal system and and uncertainty is completely arbitrary. We could easily use the lower LFT for the uncertainty. But it is a convenient convention to adhere to. The more so that it allows for the combination of both as in the diagram Figure 6 below, which corresponds to composition of the two LFTs.\n\n\n\n\n\n\nFigure 6: Combination of the lower and upper LFT\n\n\n\n\n\n\n\n\n\nStructured frequency-domain uncertainty\nNot just a single \\Delta(s) but several \\Delta_i(s), i=1,\\ldots,n are considered. Some of them scalar-valued, some of them matrix-valued.\nIn the upper LFT, all the individual \\Delta_is are collected into a single overall \\boldsymbol \\Delta, which then exhibits some structure. Typically it is block-diagonal as in \n\\boldsymbol\\Delta =\n\\begin{bmatrix}\n\\Delta_1& 0 & \\ldots & 0\\\\\n0 & \\Delta_2 & \\ldots & 0\\\\\n\\vdots\\\\\n0 & 0 & \\ldots & \\boldsymbol\\Delta_n\n\\end{bmatrix},\n with each block (including the MIMO blocks) satisfying the usual condition \n\\|\\Delta_i\\|_{\\infty}\\leq 1, \\; i=1,\\ldots, n.\n\n\nStructured singular value (SSV, \\mu, mu)\nWith this structured uncertainty, how does the small gain theorem look like?",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Uncertainty (in) modelling"
    ]
  },
  {
    "objectID": "discr_dir_mpc_recursive_feasibility.html",
    "href": "discr_dir_mpc_recursive_feasibility.html",
    "title": "Recursive feasibility",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "6. More on MPC",
      "Recursive feasibility"
    ]
  },
  {
    "objectID": "dynamic_programming_LQR.html",
    "href": "dynamic_programming_LQR.html",
    "title": "Solving LQR via dynamic programming",
    "section": "",
    "text": "In the previous section we have used dynamic programming as a numerical algorithm for solving a general discrete-time optimal control problem. We now show how to use dynamic programming to solve the discrete-time LQR problem. We consider a linear discrete-time system modelled by \n\\bm x_{k+1} = \\mathbf A\\bm x_k + \\mathbf B\\bm u_k,\n for which we want to minimize the quadratic cost given by \nJ_0(\\bm x_0, \\bm u_0, \\bm u_1, \\ldots, \\bm u_{N-1}) = \\frac{1}{2}\\bm x_N^\\top \\mathbf S_N \\bm x_N + \\frac{1}{2}\\sum_{k=0}^{N-1}\\left(\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k\\right),\n with \\mathbf S_N\\succeq 0, \\mathbf Q\\succeq 0, \\mathbf R\\succ 0, as usual.\nWe now invoke the principle of optimality, that is, we start at the end of the time interval and evaluate the optimal cost \nJ_N^\\star(\\bm x_N) = \\frac{1}{2}\\bm x_N^\\top \\mathbf S_N \\bm x_N.\n\nObviously, here we did not even have to do any optimization since at the and of the interval the cost can no longer be influenced by any control. We just evaluated the cost.\nWe then proceed backwards in time, that is, we decrease the time to k=N-1. Here do have to optimize: \nJ^\\star_{N-1}(\\bm x_{N-1}) = \\min_{\\bm u_{N-1}\\in\\mathbb R^m} J_{N-1}(\\bm x_{N-1},\\bm u_{N-1}) = \\min_{\\bm u_{N-1}\\in\\mathbb R^m} \\left[L(\\bm x_{N-1},\\bm u_{N-1}) + J^\\star_{N}(\\bm x_{N}) \\right].\n\nWe now expand the expression for the cost\n\n\\begin{aligned}\nJ_{N-1}(\\bm x_{N-1},\\bm u_{N-1}) &= \\frac{1}{2} \\left(\\mathbf x_{N-1}^\\top \\mathbf Q \\mathbf x_{N-1} + \\mathbf u_{N-1}^\\top \\mathbf R \\mathbf u_{N-1} \\right) + J^\\star_{N}(\\bm x_{N}) \\\\\n&= \\frac{1}{2} \\left(\\mathbf x_{N-1}^\\top \\mathbf Q \\mathbf x_{N-1} + \\mathbf u_{N-1}^\\top \\mathbf R \\mathbf u_{N-1} \\right) + \\frac{1}{2}\\mathbf x_N^\\top \\mathbf S_N \\mathbf x_N\\\\\n&= \\frac{1}{2} \\left( \\mathbf x_{N-1}^\\top \\mathbf Q \\mathbf x_{N-1} + \\mathbf u_{N-1}^\\top \\mathbf R \\mathbf u_{N-1} + \\mathbf x_N^\\top \\mathbf S_N \\mathbf x_N \\right)\\\\\n&= \\frac{1}{2} \\left[ \\mathbf x_{N-1}^\\top \\mathbf Q \\mathbf x_{N-1} + \\mathbf u_{N-1}^\\top \\mathbf R \\mathbf u_{N-1} + (\\mathbf x_{N-1}^\\top \\mathbf A^\\top + \\mathbf u_{N-1}^\\top \\mathbf B^\\top) \\mathbf S_N (\\mathbf A\\mathbf x_{N-1} + \\mathbf B\\mathbf u_{N-1}) \\right]\\\\\n&= \\frac{1}{2} \\left[\\mathbf x_{N-1}^\\top (\\mathbf Q  + \\mathbf A^\\top\\mathbf S_N \\mathbf A)\\mathbf x_{N-1} + 2\\mathbf x^\\top_{N-1}\\mathbf A ^\\top \\mathbf S_N \\mathbf B  \\mathbf u_{N-1} + \\mathbf u^\\top_{N-1}(\\mathbf R + \\mathbf B^\\top \\mathbf S_n \\mathbf B)\\mathbf u_{N-1} \\right].\n\\end{aligned}\n\nWe assumed no constraint on \\mathbf u_{N-1}, hence finding the minimum of J_{N-1} is as easy as setting its gradient to zero \n\\mathbf 0 = \\nabla_{\\bm u_{N-1}} J_{N-1} = (\\mathbf R + \\mathbf B^\\top \\mathbf S_n \\mathbf B)\\bm u_{N-1} + \\mathbf B^\\top \\mathbf S_N\\mathbf A\\bm x_{N-1},\n which leads to \n\\bm u_{N-1}^\\star = -\\underbrace{(\\mathbf B^\\top \\mathbf S_N\\mathbf B + \\mathbf R)^{-1}\\mathbf B^\\top \\mathbf S_N \\mathbf A}_{\\mathbf K_{N-1}} \\bm x_{N-1},\n which amounts to solving a system of linear equations. We can also recognize the Kalman gain matrix \\mathbf K_{N-1}, which we derived using the indirect approach in the previous chapter.\nThe optimal cost J^\\star_{N-1} can be obtained by substituting \\bm u_{N-1}^\\star into J_{N-1} \nJ_{N-1}^\\star = \\frac{1}{2}\\bm x_{N-1}^\\top \\underbrace{\\left[(\\mathbf A-\\mathbf B\\mathbf K_{N-1})^\\top \\mathbf S_N(\\mathbf A-\\mathbf B\\mathbf K_{N-1}) + \\mathbf K_{N-1}^\\top \\mathbf R \\mathbf K_{N-1} + \\mathbf Q\\right]}_{\\mathbf S_{N-1}} \\bm x_{N-1}.\n\nNote that the optimal cost J^\\star_{N-1} is also a quadratic function of the state as is the cost J^\\star_{N}. We denote the matrix that defines this quadratic function as \\mathbf S_{N-1}. We do this in anticipation of continuation of this recursive procedure to k = N-2, N-3, \\ldots, which will give \\mathbf S_{N-2}, \\mathbf S_{N-3}, \\ldots. The rest of the story is quite predictable, isn’t it? Applying the Bellman’s principle of optimality we (re)discovered the discrete-time Riccati equation in the Joseph stabilized form \n\\mathbf S_k = (\\mathbf A-\\mathbf B\\mathbf K_{N-1})^\\top \\mathbf S_N(\\mathbf A-\\mathbf B\\mathbf K_{N-1}) + \\mathbf K_{N-1}^\\top \\mathbf R \\mathbf K_{N-1} + \\mathbf Q,\n together with the prescription for the state feedback (Kalman) gain \n\\mathbf K_{k} = (\\mathbf B^\\top \\mathbf S_N\\mathbf B + \\mathbf R)^{-1}\\mathbf B^\\top \\mathbf S_N \\mathbf A.\n\n\n\n\n Back to top",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming",
      "Solving LQR via dynamic programming"
    ]
  },
  {
    "objectID": "cont_indir_LQR_inf_horizon.html",
    "href": "cont_indir_LQR_inf_horizon.html",
    "title": "Indirect approach to LQR on an infinite horizon",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Indirect approach to LQR on an infinite horizon"
    ]
  },
  {
    "objectID": "discr_indir_DARE.html",
    "href": "discr_indir_DARE.html",
    "title": "Discrete-time algebraic Riccati equation (DARE)",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time algebraic Riccati equation (DARE)"
    ]
  },
  {
    "objectID": "roban_structured.html#robust-performance-with-a-structured-uncertainty",
    "href": "roban_structured.html#robust-performance-with-a-structured-uncertainty",
    "title": "Robustness analysis for structured uncertainty",
    "section": "Robust performance with a structured uncertainty",
    "text": "Robust performance with a structured uncertainty",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Robustness analysis for structured uncertainty"
    ]
  },
  {
    "objectID": "reduction_order_model.html",
    "href": "reduction_order_model.html",
    "title": "Model order reduction",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "14. Model and controller order reduction",
      "Model order reduction"
    ]
  },
  {
    "objectID": "ext_LQG.html",
    "href": "ext_LQG.html",
    "title": "LQG control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQG control"
    ]
  },
  {
    "objectID": "discr_indir_references.html",
    "href": "discr_indir_references.html",
    "title": "References",
    "section": "",
    "text": "While the indirect approaches to optimal control constitute the classical core of the optimal control theory, most treatments of the subject consider continuous-time systems. Our treatment was based on Chapter 2 in (Lewis, Vrabie, and Syrmo 2012), which is one of a few resources that discuss discrete-time optimal control too.\n\n\n\n\n Back to topReferences\n\nLewis, Frank L., Draguna Vrabie, and Vassilis L. Syrmo. 2012. Optimal Control. 3rd ed. John Wiley & Sons. https://lewisgroup.uta.edu/FL%20books/Lewis%20optimal%20control%203rd%20edition%202012.pdf.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "References"
    ]
  },
  {
    "objectID": "cont_indir_LQR_fin_horizon.html",
    "href": "cont_indir_LQR_fin_horizon.html",
    "title": "Indirect approach to LQR on a finite horizon",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Indirect approach to LQR on a finite horizon"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html",
    "href": "opt_algo_constrained.html",
    "title": "Algorithms for constrained optimization",
    "section": "",
    "text": "We keep adhering to our previous decision to focus on the algorithms that use derivatives. But even then the number of derivative-based algorithms for constrained optimization – and we consider both equality and inequality constraints – is large. They can be classified in many ways.\nOne way to classify the derivative-based algorithms for constrained optimization is based on is to based on the dimension of the space in which they work. For an optimization problem with n variables and m constraints, we have the following possibilities: n-m, n, m, and n+m.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for constrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html#primal-methods",
    "href": "opt_algo_constrained.html#primal-methods",
    "title": "Algorithms for constrained optimization",
    "section": "Primal methods",
    "text": "Primal methods\n\nWith m equality constraints, they work in the space of dimension n-m.\nThree advantages\n\neach point generated by the iterative algoritm is feasible – if terminated early, such point is feaible.\nif they generate a converging sequence, it typically converges at least to a local constrained minimum.\nit does not rely on a special structure of the problem, it can be even nonconvex.\n\nbut it needs a feasible initial point.\nThey may fail for inequality constraints.\n\nThey are particularly useful for linear/affine constraints or simple nonlinear constraints (norm balls or ellipsoids).\n\nProjected gradient method\n\n\nActive set methods\n\n\nSequential quadratic programming (SQP)\nKKT conditions for a nonlinear program with equality constraints solved by Newton’s method.\nInterpretation: at each iteration, we solve a quadratic program (QP) with linear constraints.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for constrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html#penalty-and-barrier-methods",
    "href": "opt_algo_constrained.html#penalty-and-barrier-methods",
    "title": "Algorithms for constrained optimization",
    "section": "Penalty and barrier methods",
    "text": "Penalty and barrier methods",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for constrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html#dual-methods",
    "href": "opt_algo_constrained.html#dual-methods",
    "title": "Algorithms for constrained optimization",
    "section": "Dual methods",
    "text": "Dual methods",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for constrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html#primal-dual-methods",
    "href": "opt_algo_constrained.html#primal-dual-methods",
    "title": "Algorithms for constrained optimization",
    "section": "Primal-dual methods",
    "text": "Primal-dual methods",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for constrained optimization"
    ]
  },
  {
    "objectID": "roban_unstructured.html",
    "href": "roban_unstructured.html",
    "title": "Robustness analysis for unstructured uncertainty",
    "section": "",
    "text": "When we introduced the concept of robustness, we only vaguely hinted that it is always related to some property of interest. Now comes the time to specify these two properties:\n\nDefinition 1 (Robust stability) Guaranteed stability of the closed feedback loop with a given controller for all admissible (=considered apriori) deviations of the model from the reality.\n\n\nDefinition 2 (Robust performance) Robustness of some performance characteristics such as steady-state regulation error, attenuation of some specified disturbance, insensitivity to measurement noise, fast response, ….\n\n\nInternal stability\nBefore we start discussing robust stability, we need to discuss one fine issue related to stability of a nominal system. We do it through the following example.\n\nExample 1 (Internal stability) Consider the following feedback system with a nominal plant G(s) and a nominal controller K(s).\n\n\n\n\n\n\n\n\n\nThe question is: is this closed-loop system stable? We determine stability by looking at the denominator of a closed-loop transfer function. But which one? There are several. Perhaps the most immediate one is the transfer function from the reference r to the plant output y. With the open-loop transfer function L(s) = G(s)K(s) = \\frac{s-1}{s+1} \\frac{k(s+1)}{s(s-1)} = \\frac{k}{s}, the closed-loop transfer function is \nT(s) = \\frac{\\frac{k}{s}}{1+\\frac{k}{s}} = \\frac{k}{s+k},  \n which is perfectly stable. But note that for practical purposes, all possible closed-loop transfer functions must be stable. How about the one from the output disturbance d to the plant output y? \nS(s) = \\frac{1}{1+\\frac{k}{s}} = \\frac{s}{s+k},\n which is stable too. Isn’t this a signal that we can stop worrying? Not yet. Consider now the closed-loop transfer function from the reference r to the control u. The closed-loop transfer function is \nK(s)S(s) = \\frac{\\frac{k(s+1)}{s(s-1)}}{1+\\frac{k}{s}} = \\frac{k(s+1)}{{\\color{red}(s-1)}(s+k)}.\n\nOops! This closed-loop transfer function is not stable. Obviously the culprit here is our cancelling the zero in the RHP with an unstable pole in the controller. But let’s emphasize that the trouble is not in imperfectness of this cancellation due to numerical errors. The trouble is in the very cancelling the zero in the RHP by the controller. Identical problem would arise if an unstable pole of the plant is cancelled by the RHP zero of the controller as we can see by modifying the assignment accordingly.\n\nThe example taught (or perhaps reminded) us that in order to guarantee stability of all closed-loop transfer functions, no cancellation of poles and zeros in the right half plane is allowed. The resulting closed-loop system is then called internally stable. Checking just (arbitrary) one closed-loop transfer function for stability is then enough to conclude that all of them are stable too.\n\n\nRobust stability for a multiplicative uncertainty\nWe consider a feedback system with a plant G(s) and a controller K(s), where the uncertainty in the plant modelled as multiplicative uncertainty, that is, G(s) = (1+W(s)\\Delta(s))\\,G_0(s).\nThe technique for analyzing closed-loop stability is based on Nyquist criterion. Instead of analyzing the Nyquist plot for the nominal plant G_0(s), we analyze the Nyquist plot for the uncertain plant G(s). The corresponding open-loop transfer function is \nL(s) = G(s)K(s) = (1+W(s)\\Delta(s))\\,G_0(s)K(s) = L_0(s) + W(s)L_0(s)\\Delta(s).\n\nWhen trying to figure out the conditions, under which this family of Nyquist curves avoids the point -1, it is useful to interpret the last equation at a given frequency \\omega as a disc with the center at L_0(j\\omega) and the radius W(j\\omega)L_0(j\\omega). To see this, note that \\Delta(j\\omega) represents a complex number with a magnitude up to one, and with an arbitrary angle.\n\n\n\n\n\n\nFigure 2: Robust stability for multiplicative uncertainty\n\n\n\nThe geometric formulation of the condition is then that the distance from -1 to the nominal Nyquist plot of L_0(j\\omega) is greater than the radius W(j\\omega)L_0(j\\omega) of the disc centered at the nominal Nyquist curve With the distance from the point -1 to the nominal Nyquist plot of L_0(s) evaluated at a particular frequency \\omega a |-1-L_0(j\\omega)| = |1+L_0(j\\omega)|, the condition can be written as\n\n|W(j\\omega)L_0(j\\omega)| &lt; |1+L_0(j\\omega)|, \\;\\forall \\omega.\n\nDividing both sides by 1+L_0(j\\omega) we get \n\\frac{W(j\\omega)L_0(j\\omega)}{1+L_0(j\\omega)} &lt; 1, \\;\\forall \\omega.\n\nBut recalling the definition of the complementary sensitivity function, and dividing both sides by W, we can rewrite the condition as \\boxed\n{|T_0(j\\omega)| &lt; 1/|W(j\\omega)|, \\;\\; \\forall \\omega.}\n\nThis condition has clear interpretation in terms of the magnitude of the complementary sensitivity function – it must be smaller than the reciprocal of the magnitude of the uncertainty weight at all frequencies.\nFinally, we can also invoke the definition of the \\mathcal H_\\infty norm and reformulate the condition as \\boxed\n{\\|WT\\|_{\\infty}&lt; 1.}\n\nTo appreciate usefulness of the this format of the robust stability condition beyond mere notational compactness, we mention that \\mathcal H_\\infty norm of an LTI system can be reliably computed. Robust stability can then be then checked by computing a single number.\nIn fact, it is even better than that – there are methods for computing a feedback controller that minimizise the \\mathcal H_\\infty norm of a specified closed-loop transfer function, which suggests an optimization-based approach to design of robustly stabilizing controllers. We are going to build on this in the next chapter. But let’s stick to the analysis for now.\n\n\nRobust stability for an LFT – small gain theorem\nWe consider the upper LFT as in Figure 3.\n\n\n\n\n\n\nFigure 3: Upper LFT with the \\mathbf N term corresponding to the nominal closed-loop system structured into blocks\n\n\n\nThe term corresponding to the nominal closed-loop system is structured into blocks. It is only the N_{11} block that captures the interaction with the uncertainty in the model. For convenience we rename this block as \nM \\coloneqq N_{11}.\n\nThe open-loop transfer function is then M \\Delta. Following the same Nyquist criterion based reasoning as before, that is, asking for the conditions under which this open-loop transfer function does not touch the point -1, while the \\Delta term can introduce an arbitrary phase, we arrive at the robust stability condition for the LFT as \\boxed\n{|M(j\\omega)|&lt;1,\\;\\;\\forall \\omega.}\n\nOnce again, invoking the definition of the \\mathcal H_\\infty norm, we can rewrite the condition compactly as \\boxed\n{\\|M\\|_{\\infty}&lt;1.}\n\nOnce again, the formulation as an inequality over all frequencies can be useful for visualization and interpretation, while the inequality with the \\mathcal H_\\infty norm can be used for computation and optimization.\nThis condition of robust stability belongs to the most fundamental results in control theory. It is known as the small gain theorem.\n\n\n\n\n\n\nSmall gain theorem works for MIMO too\n\n\n\nSmall gain theorem works for a MIMO uncertainty \\boldsymbol \\Delta and a block \\mathbf N_{11} (or \\mathbf M) too \n\\|\\mathbf M\\|_{\\infty}&lt;1.\n\nBut we discuss in the next section that it is typically too conservative as the \\boldsymbol \\Delta block has typically some structure (block diagonal) and it should be exploited. More on this in the section dedicated to structured uncertainty.\n\n\n\n\nNominal performance\nHaving discussed stability (and its robustness), it is now time to turn to performance (and its robustness). Performance can mean difference things for different people, and it can be specified in a number of ways, but we would like to formulate performance requirements in the same frequency domain setting as we did for (robust) stability. Namely, we would like to specify the performance requirements in terms of the frequency response of some closed-loop transfer function. The sensitivity function seems to be a natural choice for this purpose. It turns out that by imposing upper bound constraints on |S(\\omega)| (actually |S_0(\\omega)| as we now focus on the nominal case with no uncertainty) we can specify a number of performance requirements:\n\nUp to which frequency the feedback controller attenuates the disturbance, that is, the bandwidth \\omega_\\mathrm{BW} of the system.\nHow much the feedback controller attenuates the disturbances over the bandwidth.\nHow does it behave at very low frequencies, that is, how well it regulates the steady-state error.\nWhat is the maximum amplification of the disturbance, that is, the resonance peak.\n\nThese four types of performance requirements can be pointed at in Figure 4 below.\n\n\n\n\n\n\nFigure 4: Performance specifications through the shape of the magnitude frequency response of the sensitivity function\n\n\n\nBut these requirements can also be compactly expressed throug the performance weighting filter W_\\mathrm{p}(s) as \\boxed\n{|S_0(j\\omega)| &lt; 1/|W_\\mathrm{p}(j\\omega)|,\\;\\;\\forall \\omega,}\n\\tag{1}\nwhere S_0 = \\frac{1}{1+L_0} is the sensitivity function of the nominal closed-loop system. which can again be compactly written as \\boxed\n{\\|W_\\mathrm{p}S_0\\|_{\\infty}&lt;1.}\n\nIt lends some insight if we visualize this condition in the complex plane. First, recall that S_0 = \\frac{1}{1+L_0}. Equation 1 then translates to \n|W_\\mathrm{p}(j\\omega)|&lt;|1+L_0(j\\omega)|\\;\\;\\forall \\omega,\n which can be visualized as in\n\n\n\n\n\n\nFigure 5: Nominal performance condition\n\n\n\n\n\nRobust performance for a multiplicative uncertainty\nSo far we have the condition of robust stability and the condition of nominal performance. Simultaneous satisfaction of both gives… just robust stability and nominal performance. Robust performance obviously needs a stricter condition.\n\n\n\n\n\n\nFigure 6: Robust performance condition\n\n\n\n\n\\boxed{\n|W_\\mathrm{p}(j\\omega)S_0(j\\omega)| + |W(j\\omega)T_0(j\\omega)| &lt; 1\\;\\;\\forall \\omega.}\n\nIn the SISO case, this is equivalent to \\boxed\n{\\left\\|\n\\begin{bmatrix}\nW_\\mathrm{p}S_0\\\\\nWT_0\n\\end{bmatrix}\n\\right\\|_{\\infty}\n&lt;\\frac{1}{\\sqrt{2}},}\n where the augmented closed-loop system \\begin{bmatrix} W_\\mathrm{p}S\\\\ WT_0 \\end{bmatrix} is called mixed sensitivity function.\nIn the MIMO case we do not have a useful upper bound, but at least we have received a hint that it may be useful to minimize the \\mathcal H_\\infty norm of the mixed sensitivity function. This observation will directly lead to a control design method.\n\n\n\n\n Back to top",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Robustness analysis for unstructured uncertainty"
    ]
  },
  {
    "objectID": "dynamic_programming.html",
    "href": "dynamic_programming.html",
    "title": "Dynamic programming and discrete-time optimal control",
    "section": "",
    "text": "In the previous two chapters we explained direct and indirect approaches to discrete-time optimal control. While the former conveniently allows incorporating almost arbitrary constraints, it only provides a control trajectory (a finite sequence of values of the control variable); if feedback is needed, the optimization must be performed in every sampling period (thus implementing the concept of receding horizon or model predictive control, MPC). The latter, in contrast, can lead to a (state) feedback control law, but this only happens in special cases such as a regulation of a linear system minimizing a quadratic cost (LQR) while assuming no bound constraints on the the control or state variables; in the general case it leads to a two-point boundary value problem, which can only be solved numerically for trajectories.\nIn this chapter we present yet another approach — dynamic programming DP. It also allows imposing constraints (in fact, even constraints such as integrality of variables, which are not compatible with our derivative-based optimization toolset exploited so far), and yet it directly leads to feedback controllers.\nWhile in the case of linear systems with a quadratic cost function, dynamic programming provides another route to the theoretical results that we already know — Riccati equation based solution to the LQR problem —, in the the case of general nonlinear dynamical systems with general cost functions, the feedback controllers come in the form of look-up tables. This format of a feedback controller gives some hint about disadvantages of DP, namely, both computation and then the use of these look-up tables do not scale well with the dimension of the state space (aka curse of dimensionality). Various approximation schemes exist — one promising branch is known as reinforcement learning.",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming",
      "Dynamic programming and discrete-time optimal control"
    ]
  },
  {
    "objectID": "dynamic_programming.html#bellmans-principle-of-optimality-and-dynamic-programming",
    "href": "dynamic_programming.html#bellmans-principle-of-optimality-and-dynamic-programming",
    "title": "Dynamic programming and discrete-time optimal control",
    "section": "Bellman’s principle of optimality and dynamic programming",
    "text": "Bellman’s principle of optimality and dynamic programming\nWe start by considering the following example.\n\nExample 1 (Reusing the plan for a trip from Prague to Ostrava) We are planning a car trip from Prague to Ostrava and you are searching for a route that minimizes the total time. Using the online planner we learn that the fastest route from Prague to Ostrava is — as bizarre as it sounds — via (actually around) Brno.\n\n\nNow, is it possible to reuse this plan for our friends from Brno who are also heading for Ostrava?\n\n\nThe answer is yes, as the planner confirms. Surely did not even need the planner to answer such trivial question. And yet it demonstrates the key wisdom of the whole chapter — the Bellman’s principle of optimality —, which we now state formally.\n\n\nTheorem 1 (Bellman’s principle of optimality) An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.\n\nWe now investigate this idea a bit more quantitatively using a simple computational example of finding a shortest path in a graph.\n\nExample 2 (Shortest path in a graph) We consider a directional graph with nodes A, B, C, D, and E and edges with the prescribed lengths as in the figure below.\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n\n\n\nB\n\nB\n\n\n\nA-&gt;B\n\n\n3\n\n\n\nD\n\nD\n\n\n\nA-&gt;D\n\n\n1\n\n\n\nC\n\nC\n\n\n\nB-&gt;C\n\n\n2\n\n\n\nE\n\nE\n\n\n\nB-&gt;E\n\n\n1\n\n\n\nD-&gt;E\n\n\n3\n\n\n\nG\n\nG\n\n\n\nD-&gt;G\n\n\n2\n\n\n\nF\n\nF\n\n\n\nC-&gt;F\n\n\n3\n\n\n\nE-&gt;F\n\n\n3\n\n\n\nH\n\nH\n\n\n\nE-&gt;H\n\n\n2\n\n\n\nG-&gt;H\n\n\n4\n\n\n\nI\n\nI\n\n\n\nF-&gt;I\n\n\n4\n\n\n\nH-&gt;I\n\n\n2\n\n\n\n\n\n\n\n\nThe task is now to find the shortest path from A to I. What are possible solution strategies? We can start enumerating all the possible paths and calculate their costs (by summing the costs of the participating edges). Needless to say, this strategy based on enumeration scales very badly with the growing number of nodes.\nAlternatively, we solve the problem using dynamic programming and relying on Bellman’s principle of optimality. Before we proceed, we need to define the concept of a stage. It is perhaps less common and natural when it comes to solving graph problems, but we introduce it with anticipation of discrete-time optimal control problems. By the kth stage we understand the node at which the kth decision needs to be made. In our case, starting at A, 4 decisions need to be made to reach the final node. But let’s agree that we also denote the final node as the stage, the 5th one, even if no decision is to be made here. The total number of stages is then N=5.\nThe crucial attribute of the strategy based on dynamic programming is that we proceed backwards. We start at the very final stage. At this stage, there is just one node and there is nothing we can do, but note that it also makes sense to formulate problems with several possible nodes at the final stage, each with a different (terminal) costs — we will actually use once we switch to the optimal control setting. Now we proceed backwards to the last but one, that is, the (N-1)th stage.\nThese are F and H nodes at this 4the stage. In these two nodes there is again no freedom as for the actions, but for each of them we can record their respective cost to go: 4 for the F node and 2 for the H node. These costs reflect how costly it is to reach the terminal node from them.\nThings are only getting interesting if we now proceed to the 3rd stage. We now have to consider three possible nodes: C, E and G. For the C and G nodes there is still just one action and we can only record their costs to go. The cost for the C node can be computed as the cost for the immediate transition from C to F plus the cost for the F node, which we recorded previously, that is, 3+4=7. We record the value of 7 with the C node. Similarly for the G node. For the E node there are two possible actions — two possible decisions to be made, two possible paths to choose from. Either to the left (or, actually, up in our orientation of the graph), which would bring us to the node F, or to the right (or down), which would bring us to the node H. We compute the costs to go for both decisions and choose the decision with a smaller cost. Here the cost of the decision to go to the left is composed of the cost of the transition to F plus the cost to go from F, that is, 3+4=7. The cost to go for the decision to go right is composed of the transition cost from E to H plus the cost to go from H, that is, 2+2=4. Obviously, the optimal decision is to go right, that is, to the node H. Here, on top of the value of the optimal (smallest) cost to go from the node we also record the optimal decision (go to the right/down). We do it by coloring the edge in blue.\nNote that in principle we should have highlighted the edges from F to I, from C to F, and from G to H. It was unnecessary here since there were the only possible edges emanating from these nodes.\nWe proceed backwards to the 2nd stage, and we compute the costs to go for the nodes B and D. Again we record their optimal values and the actual optimal decisions.\nOne last shift backwards and we are at the initial node A, for which we can do the same computation of the costs to go. Note that here coincidently both decisions have the same cost to go, hence both possible decisions/actions are optimal and we can just toss a coin.\n\n\n\n\n\n\n\nG\n\n\n\nA\n\nA\n8\n\n\n\nB\n\nB\n5\n\n\n\nA-&gt;B\n\n\n3\n\n\n\nD\n\nD\n7\n\n\n\nA-&gt;D\n\n\n1\n\n\n\nC\n\nC\n7\n\n\n\nB-&gt;C\n\n\n2\n\n\n\nE\n\nE\n4\n\n\n\nB-&gt;E\n\n\n1\n\n\n\nD-&gt;E\n\n\n3\n\n\n\nG\n\nG\n6\n\n\n\nD-&gt;G\n\n\n2\n\n\n\nF\n\nF\n4\n\n\n\nC-&gt;F\n\n\n3\n\n\n\nE-&gt;F\n\n\n3\n\n\n\nH\n\nH\n2\n\n\n\nE-&gt;H\n\n\n2\n\n\n\nG-&gt;H\n\n\n4\n\n\n\nI\n\nI\n0\n\n\n\nF-&gt;I\n\n\n4\n\n\n\nH-&gt;I\n\n\n2\n\n\n\n\n\n\n\n\nMaybe it is not immediately clear from the graph, but when viewed as an itinerary for a trip, it provides a feedback controller. Even if for whichever reason we find ourselves out of the optimal path, we can always have a look at the graph — it will guide us along the path that is optimal from that given node. For example, if we happen to be in node C, we do have a plan. Well, here is misleadingly simple as there is no decision to be made, but you get the point.",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming",
      "Dynamic programming and discrete-time optimal control"
    ]
  },
  {
    "objectID": "dynamic_programming.html#bellmans-principle-of-optimality-applied-to-the-discrete-time-optimal-control-problem",
    "href": "dynamic_programming.html#bellmans-principle-of-optimality-applied-to-the-discrete-time-optimal-control-problem",
    "title": "Dynamic programming and discrete-time optimal control",
    "section": "Bellman’s principle of optimality applied to the discrete-time optimal control problem",
    "text": "Bellman’s principle of optimality applied to the discrete-time optimal control problem\nLet’s recapitulate here the problem of optimal control for a discrete-time system. In particular, we consider the system modelled by \n\\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\n defined on the discrete time interval [i,N], with the initial state \\bm x_i fixed (\\bm x_i = \\mathbf x_i) We aim at minimizing the cost function \nJ_i^N\\left(\\bm x_i, \\bm u_i, \\bm u_{i+1}, \\ldots, \\bm u_{N-1}\\right) = \\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1}L_k(\\bm x_k,\\bm u_k).\n\nBefore we proceed, some comments on the notation are in order. Indeed, a well tuned and systematically used notation is instrumental in dynamic programming.\n\n\n\n\n\n\nWe omit the final time from the notation for the cost function\n\n\n\nWhile the cost function does depend on the final time too, in most if not all our analyses we assume that it is fixed and understood from the context. Hence we will not explicitly indicate the dependence on the final time. We will write just J_i(\\ldots). This may help reduce the notational clutter as we are going to need the upper index for something else soon.\n\n\n\n\n\n\n\n\nWe omit the state trajectory from the notation for the cost function and leave just the initial state\n\n\n\nThe cost function is clearly a function of the full sequence \\bm x_i, \\bm x_{i+1},\\ldots, \\bm x_N of the state vectors too. In the previous chapters we handled it systematically (either by considering them as optimization variables in the simultaneous direct approach or by introducing Lagrange multipliers in the indirect approache). But here we want to emphasize the fact that starting with \\bm x_{i+1}, the whole state trajectory is uniquelly determined by the initial state \\bm x_i and the corresponding control trajectory \\bm u_i, \\bm u_{i+1},\\ldots, \\bm u_{N-1}. Therefore, we write the cost function as a function of the initial state, the initial time (we already agreed above not to emphasize the final time), and the sequence of controls.\n\n\n\n\n\n\n\n\nWe use the lower index to display dependence on time\n\n\n\nThe dependence on the discrete time is reflected by the lower indices: not only in \\bm x_k and \\bm u_k but also in \\mathbf f_k(), L_k() and J_k(). We could perhaps write these as \\mathbf f(\\cdot,\\cdot,k), L(\\cdot,\\cdot,k) and J(\\cdot,\\cdot,k) to better indicate that k is really an argument for these functions, but we prefer making it compatible with the way we indicate the time dependence of \\bm x_k and \\bm u_k.\n\n\nHaving introduced the cost function parameterized by the initial state, initial time and the full sequence of controls, we now introduce the optimal cost function\n\n\\boxed{\n    J^\\star_i(\\bm x_i) = \\min_{\\bm u_i,\\ldots, \\bm u_{N-1}} J_i\\left(\\bm x_i, \\bm u_i, \\bm u_{i+1}, \\ldots, \\bm u_{N-1}\\right).}\n\\tag{1}\nThe sequence of controls in the above minimization may be subject to some constraints, but we do not indicate them here for the sake of notational simplicity.\n\n\n\n\n\n\nDifference between the J_i and J^\\star_i functions\n\n\n\nUnderstanding the difference is crucial. While the cost function J_i depends on the (initial) state, the (initial) time and the sequence of controls applied over the whole interval, the optimal cost function J^\\star_i only depends on the (initial) state and the (initial) time.\n\n\nAssume now that we can find an optimal control sequence from any given state \\bm x_{k+1} at time k+1 on, i.e., we can find \\bm u_{k+1}^\\star,\\bm u_{k+2}^\\star,\\ldots, \\bm u_{N-1}^\\star yielding the optimal cost J_{k+1}^\\star(\\bm x_{k+1}). We will soon show how to actually find it, but for the time being we just assume we can have it. We now show how it can be used to find the optimal cost J_k^\\star(\\bm x_k) at state \\bm x_k and time k.\nLet’s now consider the following strategy: with the system at state \\bm x_k and time k we apply some control \\bm u_k, not necessarily an optimal one, which brings the system to the state \\bm x_{k+1} in the next time k+1. But from then on we use the control sequence \\bm u_{k+1}^\\star,\\bm u_{k+2}^\\star,\\ldots, \\bm u_{N-1}^\\star that is optimal from \\bm x_{k+1}. The corresponding cost is \nL_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1}).\n\\tag{2}\nBellman’s principle of optimality states that if we optimize the above expression over \\bm u_k, we get the optimal cost J_k^\\star(\\bm x_k) at time k \n\\boxed{J_k^\\star(\\bm x_k) = \\min_{\\bm u_k}\\left(L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1})\\right).}\n\\tag{3}\nHence, at a given state \\bm x_{k} and time k, the optimization is performed over only one (possibly vector) control \\bm u_k and not the whole trajectory as the definition of the optimal cost in Equation 1 suggests! What a simplification!\n\n\n\n\n\n\nImportant\n\n\n\nThe minimization needs to be performed over the whole sum L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1}), because \\bm x_{k+1} is a function of \\bm u_k (recall that \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k)). We can also write Equation 3 as \nJ_k^\\star(\\bm x_k) = \\min_{\\bm u_k}\\left(L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\mathbf f_k(\\bm x_k,\\bm u_k))\\right),\n which makes it more apparent.\n\n\nOnce we have the optimal cost function J^\\star_{k}, the optimal control \\bm u_k^\\star(x_k) at a given time k and state \\bm x_k is obtained by \n\\boxed{\n    \\bm u_k^\\star(\\bm x_k) = \\arg \\min_{\\bm u_k}\\left(L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\mathbf f_k(\\bm x_k,\\bm u_k))\\right).}\n\n\nAlternative formulation of dynamic programming using Q-factors\nThe cost function in Equation 2 is sometimes called Q-factor and we denote it Q_k(\\bm x_k,\\bm u_k). We write its definition here for convenience \nQ^\\star_k(\\bm x_k,\\bm u_k) = L_k(\\bm x_k,\\bm u_k) + J_{k+1}^\\star(\\bm x_{k+1}).\n\nThe optimal cost function J_k^\\star(\\bm x_k) can be recovered from the optimal Q-factor Q_k^\\star(\\bm x_k,\\bm u_k) by taking the minimum over \\bm u_k \nJ_k^\\star(\\bm x_k) = \\min_{\\bm u_k} Q_k^\\star(\\bm x_k,\\bm u_k).\n\nBellman’s principle of optimality can be then expressed using the optimal Q-factor as \n\\boxed{Q_k^\\star(\\bm x_k,\\bm u_k) = L_k(\\bm x_k,\\bm u_k) + \\min_{\\bm u_{k+1}} Q_{k+1}^\\star(\\bm x_{k+1},\\bm u_{k+1})}.\n\nOptimal control is then obtained from the optimal Q-factor as the minimizing control \n\\boxed{\\bm u_k^\\star(\\bm x_k) = \\arg \\min_{\\bm u_k} Q_k^\\star(\\bm x_k,\\bm u_k).}",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming",
      "Dynamic programming and discrete-time optimal control"
    ]
  },
  {
    "objectID": "discr_indir_LQR_fin_horizon.html",
    "href": "discr_indir_LQR_fin_horizon.html",
    "title": "Discrete-time LQR on a finite horizon",
    "section": "",
    "text": "We consider a linear time-invariant (LTI) system described by the state equation \n\\bm x_{k+1} = \\mathbf A \\bm x_{k} + \\mathbf B \\bm u_k, \\qquad \\bm x_0 = \\mathbf x_0,\n and our goal is to find a (vector) control sequence \\bm u_0, \\bm u_{1},\\ldots, \\bm u_{N-1} that minimizes \nJ_0^N = \\frac{1}{2}\\bm x_N^\\top\\mathbf S_N\\bm x_N + \\frac{1}{2}\\sum_{k=0}^{N-1}\\left[\\bm x_k^\\top \\mathbf Q \\bm x_k+\\bm u_k^\\top \\mathbf R\\bm u_k\\right],\n where the quadratic cost function is parameterized the matrices that must be symmetric and at least positive semidefinite, otherwise the corresponding quadratic terms will not play a good role of penalizing the (weighted) distance from zero.\nWe will see in a moment that the matrix \\mathbf R must comply with an even stricter condition – it must be positive definite. To summarize the assumptions about the matrices, we require \n\\mathbf S_N\\succeq 0, \\mathbf Q\\succeq 0, \\mathbf R\\succ 0.\nThe Hamiltonian for our problem is \n\\boxed{\nH(\\bm x_k, \\bm u_k, \\bm \\lambda_{k+1}) = \\frac{1}{2}\\left(\\bm x_k^\\top \\mathbf Q\\bm x_k+\\bm u_k^\\top \\mathbf R\\bm u_k\\right) + \\boldsymbol \\lambda_{k+1}^\\top\\left(\\mathbf A\\bm x_k+\\mathbf B\\bm u_k\\right).\n}\nIn the following derivations we use the shorthand notation H_k for H(\\bm x_k, \\bm u_k, \\bm \\lambda_{k+1}).\nSubstituting into the general necessary conditions derived in the previous section we obtain \n\\begin{aligned}\n\\mathbf x_{k+1} &= \\nabla_{\\boldsymbol \\lambda_{k+1}}H_k=\\mathbf A\\bm x_k+\\mathbf B\\bm u_k,\\\\\n\\boldsymbol\\lambda_k &= \\nabla_{\\mathbf x_{k}}H_k=\\mathbf Q\\bm x_k+\\mathbf A^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\mathbf 0 &= \\nabla_{\\mathbf u_{k}}H_k = \\mathbf R\\bm u_k + \\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\\\\\n0 &= (\\mathbf S_N \\bm x_N - \\boldsymbol \\lambda_N)^\\top\\; \\text{d} \\bm x_N,\\\\\n\\bm x_0 &= \\mathbf x_0.\n\\end{aligned}\nThe last two equations represent boundary conditions. Note that here we have already fixed the initial state. If this is not appropriate in a particular scenario, go back and adjust the boundary equation accordingly.\nThe third equation above – the stationarity equation – can be used to extract the optimal control \n\\bm u_k = -\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1}.\nThe need for nonsingularity of \\mathbf R is now obvious. Upon substituting the recipe for the optimal \\bm u_k into the state and the co-state equations, two recursive (or recurrent or just discrete-time) equations result \n\\begin{bmatrix}\n\\mathbf x_{k+1}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf A & -\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q & \\mathbf A^\\top\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bm x_k \\\\ \\boldsymbol\\lambda_{k+1}\n\\end{bmatrix}.\nThis is a two-point boundary value problem (TP-BVP). The problem is of order 2n, where n is the dimension of the state space. In order to solve it we need 2n boundary values: n boundary values are provided by \\bm x_i = \\mathbf x_0, and n boundary values are given by the other boundary condition, from which \\boldsymbol\\lambda_N must be extracted. Most of our subsequent discussion will revolve around this task.\nAn idea might come into our mind: provided \\mathbf A is nonsingular, we can left-multiply the above equation by the inverse of \\mathbf A to obtain \n\\begin{bmatrix}\n\\mathbf x_{k}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf A^{-1} & \\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q\\mathbf A^{-1} & \\mathbf A^\\top+\\mathbf Q\\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf x_{k+1} \\\\ \\boldsymbol\\lambda_{k+1}\n\\end{bmatrix}\n\\tag{1}\nThis helped at least to have both variable evolving in the same direction in time (both backward) but we do not know \\boldsymbol\\lambda_N anyway. Nonetheless, do not forget this result. We are going to invoke it later.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on a finite horizon"
    ]
  },
  {
    "objectID": "discr_indir_LQR_fin_horizon.html#fixed-final-state-and-finite-time-horizon",
    "href": "discr_indir_LQR_fin_horizon.html#fixed-final-state-and-finite-time-horizon",
    "title": "Discrete-time LQR on a finite horizon",
    "section": "Fixed final state and finite time horizon",
    "text": "Fixed final state and finite time horizon\nBack to the nonzero control case. First we are going to investigate the scenario when the final requested state is given by \\mathbf x^\\text{ref}. The optimal control problem turns into \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x_0, \\bm{x}_{1},\\ldots,\\bm{x}_{N},\\bm{u}_{0},\\ldots,\\bm{u}_{N-1}} &\\; \\frac{1}{2}\\sum_{k=0}^{N-1}\\left[\\bm x_k^T \\mathbf Q \\bm x_k+\\bm u_k^T \\mathbf R\\bm u_k\\right]\\\\\n\\text{s.t. } & \\; \\mathbf x_{k+1} = \\mathbf A \\mathbf x_{k} + \\mathbf B \\bm u_k,\\\\\n&\\; \\bm x_0 = \\mathbf x_0,\\\\\n&\\; \\bm x_N = \\mathbf x^\\text{ref},\\\\\n&\\; \\mathbf Q\\geq 0, \\mathbf R&gt;0.\n\\end{aligned}\n\n\nNote also that the term penalizing the final state is removed from the cost because it is always fixed. After eliminating the controls using the stationarity equation \n\\bm u_k = -\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\n and replacing the general boundary condition at the final time by \\bm x_N = \\mathbf x^\\text{ref}, the two-point boundary value problem specializes to \n\\begin{aligned}\n\\mathbf x_{k+1} &=\\mathbf A\\bm x_k-\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\boldsymbol\\lambda_k &= \\mathbf Q\\bm x_k+\\mathbf A^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\bm x_0 &= \\mathbf x_0,\\\\\n\\bm x_N &= \\mathbf x^\\text{ref}.\n\\end{aligned}\n\nThis problem is clearly an instance of a two-point boundary value problem (TP-BVP) as the state vector is specified at both ends of the time interval. The costate is left unspecified, but it is fine because only 2n boundary conditions are needed. While BVP are generally difficult to solve, our problem at hand adds one more layer of complexity. For the state variable its evolution forward in time is specified by the state equation, while for the co-state variable the evolution backward in time is prescribed by the co-state equation.\n\n\\begin{bmatrix}\n\\mathbf x_{k+1}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf A & -\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q & \\mathbf A^\\top\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bm x_k \\\\ \\boldsymbol\\lambda_{k+1}.\n\\end{bmatrix}\n\nThere is not much we can do with these equations in this form. However, in case of a nonsingular matrix \\mathbf A, we can invoke the discrete-time Hamiltonian system (Equation 1), in which we reorganized the equations so that both state and co-state variables evolve backwards. For convenience we give it here again \n\\begin{bmatrix}\n\\mathbf x_{k}\\\\\\boldsymbol\\lambda_k\n\\end{bmatrix}\n=\\underbrace{\n\\begin{bmatrix}\n\\mathbf A^{-1} & \\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q\\mathbf A^{-1} & \\mathbf A^\\top+\\mathbf Q\\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\n\\end{bmatrix}}_{\\mathbf H}\n\\begin{bmatrix}\n\\mathbf x_{k+1} \\\\ \\boldsymbol\\lambda_{k+1}.\n\\end{bmatrix}\n\nThis can be used to relate the state and costate at the initial and final times of the interval \n\\begin{bmatrix}\n\\mathbf x_{0}\\\\\\boldsymbol\\lambda_0\n\\end{bmatrix}\n=\\underbrace{\n\\begin{bmatrix}\n\\mathbf A^{-1} & \\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\\\\\mathbf Q\\mathbf A^{-1} & \\mathbf A^\\top+\\mathbf Q\\mathbf A^{-1}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\n\\end{bmatrix}^N}_{\\mathbf M\\coloneqq \\mathbf H^N}\n\\begin{bmatrix}\n\\mathbf x_{N} \\\\ \\boldsymbol\\lambda_{N}\n\\end{bmatrix}.\n\nFrom the first equation we can get \\boldsymbol \\lambda_N. First, let’s rewrite it here \n\\mathbf M_{12}\\boldsymbol \\lambda_N = \\bm x_0-\\mathbf M_{11}\\bm x_N,\n from which (after substituting for the known initial and final states) \n\\boldsymbol \\lambda_N = \\mathbf M_{12}^{-1}(\\mathbf r_0-\\mathbf M_{11}\\mathbf r_N).\n\nHaving the final state and the final co-state, \\bm x_N and \\boldsymbol \\lambda_N, respectively, we can solve the Hamiltonian system backward to get the states and co-states on the whole time interval [0,N-1].\n\nSpecial case: minimum-energy control (\\mathbf Q = \\mathbf 0)\nWe can get some more insight into the problem if we further restrict the class of problems we can treat. Namely, we will assume \n\\mathbf Q = \\mathbf 0.\n\nThis is a significant restriction, nonetheless the resulting problem is still practically reasonable. And we do not need to assume that \\mathbf A is nonsingular. The cost function is then \nJ = \\sum_{k=0}^N \\mathbf u^\\top_k\\;\\bm u_k = \\sum_{k=0}^N \\|\\mathbf u\\|_2^2,   \n which is why the problem is called the minimum-energy control problem. Rewriting the state and co-state equations with the new restriction \\mathbf Q=\\mathbf 0 we get \n\\begin{aligned}\n\\bm x_{k+1} &= \\mathbf A\\bm x_k - \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1}\\\\\n\\boldsymbol \\lambda_k &= \\mathbf A^\\top\\boldsymbol\\lambda_{k+1}.\n\\end{aligned}\n\nIt is obvious why we wanted to enforce the \\mathbf Q=\\mathbf 0 restriction — the co-state equation is now completely decoupled from the state equation and can be solved independently \n\\boldsymbol \\lambda_k = (\\mathbf A^\\top)^{N-k}\\boldsymbol \\lambda_N.\n\nNow substitute this solution of the co-state equation into the state equation \n\\bm x_{k+1} = \\mathbf A\\bm x_k - \\mathbf B\\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-k-1}\\boldsymbol \\lambda_N.\n\nFinding a solution to the state equation is now straightforward — the second summand on the right is considered as a an “input”. The solution is then \n\\bm x_{k} = \\mathbf A^k\\bm x_0 - \\sum_{i=0}^{k-1}\\mathbf A^{k-1-i}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-i-1}\\boldsymbol \\lambda_N.\n\nThe last step reveals the motivation for all the previous steps — we can now express the state at the final time, and by doing that we introduce some known quantity into the problem \n\\bm x_{N} = \\mathbf x^\\text{ref}= \\mathbf A^N\\bm x_0 - \\underbrace{\\sum_{i=0}^{N-1}\\mathbf A^{N-1-i}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-i-1}}_{G_{0,N,R}}\\boldsymbol \\lambda_N.\n\nThis enables us to calculate \\boldsymbol \\lambda_N directly as a solution to a linear equation. To make the notation simpler, denote the sum in the expression above by \\mathbf G_{0,N,R} (we will discuss this particular object in a while) \n\\boldsymbol \\lambda_N = -\\mathbf G^{-1}_{0,N,R}\\; (\\mathbf x^\\text{ref}-\\mathbf A^N\\bm x_0).\n\nThe rest is quite straightforward as the optimal control depends (through the stationarity equation) on the co-state \n\\boxed{\n\\bm u_k = \\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-k-1}\\mathbf G^{-1}_{0,N,R}\\; (\\mathbf x^\\text{ref}-\\mathbf A^N\\bm x_0).\n}\n\nThis is the desired formula for computation of the optimal control.\nA few observations can be made\n\nThe control is proportional to the difference (\\mathbf x^\\text{ref}-\\mathbf A^N\\bm x_0). The intuitive interpretation is that the further the requested final state is from the state into which the system would finally evolve without any control, the higher the control is needed.\n\nThe control is proportional to the inverse of a matrix \\mathbf G_{0,N,R} which is called weighted reachability Gramian. The standard result from the theory of linear dynamic systems is that nonsingularity of a reachability Gramian is equivalent to reachability of the system. More on this below.\n\n\nWeighted reachability Gramian\nRecall (perhaps from your linear systems course) that there is a matrix called discrete-time reachability Gramian defined as \n\\mathbf G = \\sum_{k=0}^{\\infty} \\mathbf A^{k}\\mathbf B\\mathbf B^\\top(\\mathbf A^\\top)^k\n and the nonsingularity of this matrix serves as a test of reachability for stable discrete-time linear systems.\nHow does this classical object relate to the object \\mathbf G_{0,N,R} introduced in the previous paragraph? First consider the restriction of the summation from the infinite interval [0,\\infty] to [0,N-1]. In other words, we analyze the matrix \n\\mathbf G_{0,N} = \\sum_{k=0}^{N-1} \\mathbf A^{N-1-k}\\mathbf B\\mathbf B^\\top(\\mathbf A^\\top)^{N-1-k}.\n\nRecall that Caley-Hamilton theorem tells us that every higher power of an N\\times N matrix can be expressed as a linear combination of powers of 0 through N-1. In other words, using higher order powers of A than N-1 cannot increase the rank of the matrix.\nFinally, provided \\mathbf R is nonsingular (hence \\mathbf R^{-1} is nonsingular as well), the rank of the Gramian is not changed after introducing the weight\n\n\\mathbf G_{0,N,R} = \\sum_{k=0}^{N-1} \\mathbf A^{N-1-k}\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top(\\mathbf A^\\top)^{N-1-k}.\n\nThe weighted Gramian defined on a finite discrete-time horizon is invertible if and only if the (stable) system is reachable. This conclusion is quite natural: if an optimal control is to be found, first it must be guaranteed that any control can be found which brings the system from an arbitrary initial state into an arbitrary final state on a finite time interval — the very definition of reachability.\nTo summarize the whole fixed-final state case, the optimal control can be computed numerically by solving a TP-BVP. For the minimum-problem even a formula exists and there is no need for a numerical optimization solver. But the outcome is always just a sequence of controls. In this regard, the new (indirect) approach did not offer much more that what the direct approach did. Although the new insight is rewarding, it is paid for by the inability to handle constraints on the control or state variables.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on a finite horizon"
    ]
  },
  {
    "objectID": "discr_indir_LQR_fin_horizon.html#free-final-state-and-finite-time-horizon",
    "href": "discr_indir_LQR_fin_horizon.html#free-final-state-and-finite-time-horizon",
    "title": "Discrete-time LQR on a finite horizon",
    "section": "Free final state and finite time horizon",
    "text": "Free final state and finite time horizon\nThe previous discussion revolved around the task of bringing the system to a given final state exactly. What if we relax this strict requirement and instead just request that the system be eventually brought to the close vicinity of the requested state? How close — this could be affected by the terminal state penalty in the cost function.\n\nThe only change with respect to the previous development is just in the boundary condition — the one at the final time. Now the final state \\bm x_N can also be used as a parameter for our optimization. Hence \\text{d}\\bm x_N\\neq 0 and the other term in the product must vanish. We write down again the full necessary conditions including the new boundary conditions \n\\begin{aligned}\n\\bm x_{k+1} &=\\mathbf A\\bm x_k-\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\boldsymbol\\lambda_k &= \\mathbf Q\\bm x_k+\\mathbf A^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\bm u_k &= -\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1},\\\\\n\\mathbf S_N \\bm x_N &= \\boldsymbol \\lambda_N,\\\\\n\\bm x_0 &= \\mathbf x_0.\n\\end{aligned}\n\nWe find ourselves in a pretty much similar trouble as before. The final-time boundary condition refers to the variables whose values we do not know. The solution is provided by the insightful guess, namely, why not trying to extend the linear relationship between the state and the co-state at the final time to all preceding discrete times? That is, we assume \n\\mathbf S_k \\bm x_k = \\boldsymbol \\lambda_k.\n\\tag{2}\nAt first, we can have no idea if it works. But let’s try it and see what happens. Substitute (Equation 2) into the state and co-state equations. We start with the state equation \n\\bm x_{k+1} =\\mathbf A\\bm x_k-\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\bm x_{k+1}.\n\nSolving for \\bm x_{k+1} yields \n\\bm x_{k+1} =(\\mathbf I+\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1})^{-1}\\mathbf A\\bm x_k.\n\nNow perform the same substitution into the co-state equation \n\\mathbf S_k \\bm x_k = \\mathbf Q\\bm x_k+\\mathbf A^\\top\\mathbf S_{k+1}\\bm x_{k+1},\n and substitute for \\bm x_{k+1} from the state equation into the previous equation to get \n\\mathbf S_k \\bm x_k = \\mathbf Q\\bm x_k+\\mathbf A^\\top\\mathbf S_{k+1}(\\mathbf I+\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1})^{-1}\\mathbf A\\bm x_k.\n\nSince this equation must hold for an arbitrary \\bm x_k, we get an equation in the matrices \\mathbf S_k \n\\boxed{\n\\mathbf S_k = \\mathbf Q+\\mathbf A^\\top\\mathbf S_{k+1}(\\mathbf I+\\mathbf B\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1})^{-1}\\mathbf A.\n}\n\nThis is a superfamous equation and is called difference (or discrete-time) Riccati equation. When initialized with \\mathbf S_N, it generates the sequence of matrices \\mathbf S_{N-1}, \\mathbf S_{N-2}, \\mathbf S_{N-3},\\ldots Indeed, a noteworthy feature of this sequence is that it is initialized at the final time and the equation prescribes how the sequence evolves backwards.\nOnce we have generated a sufficiently long sequence (down to \\mathbf S_{1}), the optimal control sequence \\bm u_0, \\bm u_1, \\ldots, \\bm u_{N-1} is then computed using the stationary equation \n\\bm u_k = -\\mathbf R^{-1}\\mathbf B^\\top\\boldsymbol\\lambda_{k+1}=-\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\bm x_{k+1}.\n\nThis suggests that the optimal control is generated using the state but the current scheme is noncausal because the control at a given time depends on the state at the next time. But turning this into a causal one is easy — just substitute the state equation for \\bm x_{k+1} and get \n\\bm u_k =-\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}(\\mathbf A\\bm x_{k}+\\mathbf B\\bm u_{k}).\n\nSolving this equation for \\bm u_k gives \n\\bm u_k = -\\underbrace{(\\mathbf I + \\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\mathbf B)^{-1}\\mathbf R^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\mathbf A}_{\\mathbf K_k}\\mathbf x_{k}.\n\nMission accomplished. This is our desired control. A striking observation is that although we made no specifications as for the controller structure, the optimal control strategy turned out a feedback one! Let’s write it down explicitly \n\\boxed{\n\\bm u_k = -\\mathbf K_k \\bm x_{k}.\n}\n\n\n\n\n\n\n\nLQ-optimal control on a finite time horizon with a free final state is a feedback control\n\n\n\nThe importance of this result can hardly be overstated – the optimal control comes in the form of a proportional state-feedback control law.\n\n\nThe feedback gain is time-varying and deserves a name after its inventor — Kalman gain. Incorporating the knowledge that \\mathbf R is nonsingular, a minor simplification of the lengthy expression can be made \n\\mathbf K_k = (\\mathbf R + \\mathbf B^\\top\\mathbf S_{k+1}\\mathbf B)^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\mathbf A.\n\\tag{3}\nBefore we move on, let us elaborate a bit more on the difference Riccati equation. Invoking a popular (but hard to reliably memorize) rule for inversion of a sum of two matrices called matrix inversion lemma, which reads \n(\\mathbf A_{11}^{-1}+\\mathbf A_{12}\\mathbf A_{22}\\mathbf A_{21})^{-1} =\\mathbf A_{11}-\\mathbf A_{11}\\mathbf A_{12}(\\mathbf A_{21}\\mathbf A_{11}\\mathbf A_{12}+\\mathbf A_{22}^{-1})^{-1}\\mathbf A_{21}\\mathbf A_{11},\n the Riccati equation can be rewritten (after multiplying the brackets out) as \n\\boxed{\n\\mathbf S_k = \\mathbf Q + \\mathbf A^\\top\\mathbf S_{k+1}\\mathbf A - \\mathbf A^\\top\\mathbf S_{k+1}\\mathbf B( \\mathbf B^\\top\\mathbf S_{k+1}\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\top\\mathbf S_{k+1}\\mathbf A,\n}\n which we will regard as an alternative form of difference Riccati equation.\nObserving that the steps of the computation of the Kalman gain \\mathbf K_k reappear in the computation of the solution of the Riccati equation, a more efficient arrangement of the computation in every iteration step is \n\\boxed{\n\\begin{aligned}\n\\mathbf K_k &= \\left(\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B+\\mathbf R\\right)^{-1}\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf A\\\\\n\\mathbf S_k &= \\mathbf A^\\top \\mathbf S_{k+1}(\\mathbf A-\\mathbf B\\mathbf K_k) + \\mathbf Q.\n\\end{aligned}\n}\n\nFinally, yet another equivalent version of Riccati equation is known as Joseph stabilized form of Riccati equation \n\\boxed{\n\\mathbf S_k = (\\mathbf A-\\mathbf B\\mathbf K_k)^\\top \\mathbf S_{k+1}(\\mathbf A-\\mathbf B\\mathbf K_k) + \\mathbf K_k^\\top \\mathbf R\\mathbf K_k + \\mathbf Q.\n}\n\\tag{4}\nShowing the equivalence can be an exercise.\n\nSecond order sufficient conditions\nSo far we only found a solution that satisfies the first-order necessary equation but we have been warned at the introductory lessons to optimization that such solution need not necessarily constitute an optimum (minimum in our case). In order to check this, the second derivative (Hessian, curvature matrix) must be found and checked for positive definiteness. Our strategy will be to find the value of the optimal cost first and then we will identify its second derivative with respect to \\bm u_k.\nThe trick to find the value of the optimal cost is from (Lewis, Vrabie, and Syrmo 2012) and it is rather technical and it may be hard to learn a general lesson from it. Nonetheless we will need the result. Therefore we swiftly go through the procedure without pretending that we are building a general competence. The trick is based on the observation that \n\\frac{1}{2}\\sum_{k=0}^{N-1}(\\mathbf x^\\top _{k+1}\\mathbf S_{k+1} \\mathbf x_{k+1} - \\mathbf x^\\top _{k}\\mathbf S_{k} \\mathbf x_{k}) = \\frac{1}{2}\\mathbf x^\\top _{N}\\mathbf S_{N} \\mathbf x_{N} - \\frac{1}{2}\\mathbf x^\\top _{0}\\mathbf S_{0} \\mathbf x_{0}.\n\nNow consider our optimization criterion and add zero to it. The value of the cost function does not change. Weird procedure, right? Observing that zero can also be expressed as the right hand side minus the left hand side in the above equation, we get \nJ_0 = \\frac{1}{2}\\bm x_0^\\top\\mathbf S_0\\bm x_0 + \\frac{1}{2}\\sum_{k=0}^{N-1}\\left[\\mathbf x^\\top _{k+1}\\mathbf S_{k+1} \\mathbf x_{k+1}+\\bm x_k^\\top (\\mathbf Q - \\mathbf S_k) \\bm x_k+\\bm u_k^\\top \\mathbf R\\bm u_k\\right].\n\nSubstituting the state equation, the cost function transforms to \n\\begin{aligned}\nJ_0 &= \\frac{1}{2}\\bm x_0^\\top\\mathbf S_0\\bm x_0 + \\frac{1}{2}\\sum_{k=0}^{N-1}[\\mathbf x^\\top _{k}(\\mathbf A^\\top \\mathbf S_{k+1}\\mathbf A + \\mathbf Q - \\mathbf S_k) \\mathbf x_{k}+\\bm x_k^\\top \\mathbf A^\\top \\mathbf S_{k+1}\\mathbf B \\bm u_k\\\\\n&\\qquad\\qquad\\qquad\\qquad+\\bm u_k^\\top \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf A \\bm x_k+\\bm u_k^\\top (\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B + \\mathbf R)\\bm u_k].\n\\end{aligned}\n\nSubstituting for \\mathbf S_k from the Riccati equation gives \n\\begin{aligned}\nJ_0 &= \\frac{1}{2}\\bm x_0^\\top\\mathbf S_0\\bm x_0 + \\frac{1}{2}\\sum_{k=0}^{N-1}[\\mathbf x^\\top _{k}(\\mathbf A^\\top \\mathbf S_{k+1}\\mathbf B( \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B+\\mathbf R)^{-1}\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf A) \\mathbf x_{k}+\\bm x_k^\\top \\mathbf A^\\top \\mathbf S_{k+1}\\mathbf B \\bm u_k\\\\\n&\\qquad\\qquad\\qquad\\qquad+\\bm u_k^\\top \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf A \\bm x_k+\\bm u_k^\\top (\\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B + \\mathbf R)\\bm u_k].\n\\end{aligned}\n\nThe time-varying Hessian with respect to the control \\bm u_k is \n\\nabla_{\\bm u_k}^2 J_0 = \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B + \\mathbf R.\n\nProvided that \\mathbf R\\succ 0, it can be seen that it is always guaranteed that \\nabla_{\\bm u_k}^2 J_0\\succ 0. To prove this it must be shown that \\mathbf B^\\top \\mathbf S_{k+1}\\mathbf B\\succeq 0. As usual, let us make things more intuitive by switching to the scalar case. The previous expression simplifies to b^2s_{k+1}. No matter what the value of b is, the square is always nonnegative. It remains to show that s_{k+1}\\geq0 (and in the matrix case \\mathbf S_{k+1}\\succeq 0). This can be seen from the prescription for \\mathbf S_{k} given by the Riccati equation using similar arguments for proving positive semidefiniteness of compound expressions.\nTo conclude, the solution to the first-order necessary conditions represented by the Riccati equation is always a minimizing solution.\nWe can work a bit more with the value of the optimal cost. Substituting the optimal control we can see (after some careful two-line work) that \nJ_0 = \\frac{1}{2}\\bm x_0^\\top  \\mathbf S_0 \\bm x_0.\n\nThe same conclusion can be obtained for any time instant k inside the interval [0,N] \n\\boxed{\nJ_k = \\frac{1}{2}\\bm x_k^\\top  \\mathbf S_k \\bm x_k.\n}\n\nThis is a result that we have already seen in the no-control case: the optimal cost can be obtained as a quadratic function of the initial state using a matrix obtained as a solution to some iteration. We will use this result in the future derivations.\n\n\nNumerical example with a scalar and first-order system\nAs usual, some practical insight can be developed by analyzing the things when restricted to the scalar case. For this, consider a first order system described by the first-order state equation \nx_{k+1} = ax_k + bu_k\n and the optimization criterion in the form \nJ_0 = \\frac{1}{2}s_N x_N^2 + \\frac{1}{2}\\sum_{k=0}^{N-1}\\left[ q x_k^2+r u_k^2\\right ].\n\nThe scalar Riccati equation simplifies to \ns_k = a^2s_{k+1} - \\frac{a^2b^2s_{k+1}^2}{b^2s_{k+1}+r} + q\n or \ns_k = \\frac{a^2rs_{k+1}}{b^2s_{k+1}+r} + q.\n\nJulia code and its outputs follow.\n\n\nCode\nfunction dre(a,b,q,r,sN,N)\n    s = Vector{Float64}(undef,N+1)          # the S[1] will then not be needed (even defined) but the indices will fit\n    k = Vector{Float64}(undef,N)\n    s[end] = sN\n    for i=N:-1:1\n        k[i]=(a*b*s[i+1])/(r + s[i+1]*b^2);\n        s[i]= a*s[i+1]*(a-b*k[i]) + q;\n    end\n    return s,k\nend\n\na = 1.05;\nb = 0.01;\nq = 100;\nr = 1;\nx0 = 10;\nsN = 100;\nN = 20;\n\ns,k = dre(a,b,q,r,sN,N);\n\nusing Plots\n\np1 = plot(0:1:N,s,xlabel=\"i\",ylabel=\"RE solution\",label=\"s\",markershape=:circ,markersize=1,linetype=:steppost)\np2 = plot(0:1:N-1,k,xlabel=\"i\",ylabel=\"State-feedback gain\",label=\"k\",markershape=:circ,markersize=1,linetype=:steppost,xlims=xlims(p1))\n\nx = Vector{Float64}(undef,N+1)\nu = Vector{Float64}(undef,N)\n\nx[1]=x0;\n\nfor i=1:N\n    u[i] = -k[i]*x[i];\n    x[i+1] = a*x[i] + b*u[i];\nend\n\np3 = plot(0:1:N,x,xlabel=\"i\",ylabel=\"State\",label=\"x\",markershape=:circ,markersize=1,linetype=:steppost)\nplot(p1,p2,p3,layout=(3,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObviously the final state is not particularly close to zero, which is the desired final value. However, increasing the s_N term we can bring the system arbitrarily close, as the next simulation confirms.\n\n\nCode\nsN = 10000;\nN = 20;\n\ns,k = dre(a,b,q,r,sN,N);\n\np1 = plot(0:1:N,s,xlabel=\"i\",ylabel=\"RE solution\",label=\"s\",markershape=:circ,markersize=1,linetype=:steppost)\np2 = plot(0:1:N-1,k,xlabel=\"i\",ylabel=\"State-feedback gain\",label=\"k\",markershape=:circ,markersize=1,linetype=:steppost,xlims=xlims(p1))\n\nx = Vector{Float64}(undef,N+1)\nu = Vector{Float64}(undef,N)\n\nx[1]=x0;\n\nfor i=1:N\n    u[i] = -k[i]*x[i];\n    x[i+1] = a*x[i] + b*u[i];\nend\n\np3 = plot(0:1:N,x,xlabel=\"i\",ylabel=\"State\",label=\"x\",markershape=:circ,markersize=1,linetype=:steppost)\nplot(p1,p2,p3,layout=(3,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we explore what changes if we make the time horizon longer.\n\n\nCode\nN = 100;\n\ns,k = dre(a,b,q,r,sN,N);\n\np1 = plot(0:1:N,s,xlabel=\"i\",ylabel=\"RE solution\",label=\"s\",markershape=:circ,markersize=1,linetype=:steppost)\np2 = plot(0:1:N-1,k,xlabel=\"i\",ylabel=\"State-feedback gain\",label=\"k\",markershape=:circ,markersize=1,linetype=:steppost,xlims=xlims(p1))\n\nx = Vector{Float64}(undef,N+1)\nu = Vector{Float64}(undef,N)\n\nx[1]=x0;\n\nfor i=1:N\n    u[i] = -k[i]*x[i];\n    x[i+1] = a*x[i] + b*u[i];\nend\n\np3 = plot(0:1:N,x,xlabel=\"i\",ylabel=\"State\",label=\"x\",markershape=:circ,markersize=1,linetype=:steppost)\nplot(p1,p2,p3,layout=(3,1))\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe last outputs suggests that both s_N and K_k stay constant for most of the time interval and they only change dramatically towards the end of the control interval.\nThe observation in the example poses a question of how much is lost after replacing the optimal control represented by the sequence \\mathbf K_k by some constant value \\mathbf K. A natural candidate is the steady-state value that \\mathbf K_k has as the beginning of the control interval, that is at k=0 in our case.\nObviously, on a finite-horizon there is not much to be investigated, the constant feedback gain is just suboptimal, but things are somewhat more involved as the control horizon stretches to infinity, that is, N\\rightarrow \\infty.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR on a finite horizon"
    ]
  },
  {
    "objectID": "dynamic_programming_tabular.html",
    "href": "dynamic_programming_tabular.html",
    "title": "Tables as outcomes of dynamic programming",
    "section": "",
    "text": "Based on what we have seen so far, it turns out that the key to solving the discrete-time optimal control problem is to find some… functions. Either the optimal cost function J_k^\\star(\\bm x_k) or the optimal Q-factor Q_k^\\star(\\bm x_k,\\bm u_k). Once we have them, we can easily find the optimal control \\bm u_k^\\star(\\bm x_k). The question however is how to find these functions. We have seen some recursions for both of them, but it is not clear how to turn these into practical algorithms. We do it here.\nWe are going to solve ?@eq-bellman_for_discrete_time_optimal_control backwards in (discrete) time at a grid of states. Indeed, gridding the state space is the key technique in dynamic programming, because DP assumes a finite state space. If it is not finite, we must grid it.\nWe start with the final time N. We evaluate the terminal cost function \\phi(\\bm x_N) at a grid of states, which directly yields the optimal costs J_N^\\star(\\bm x_N).\nWe then proceed to the time N-1. Evaluating the optimal cost function J^\\star_{N-1} at each grid point in the state space calls for some optimization, namely \n\\min_{u_{N-1}} \\left(L_{N-1}(\\bm x_{N-1},\\bm u_{N-1}) + J_{N}^\\star(\\mathbf f_{N-1}(\\bm x_{N-1}, \\bm u_{N-1}))\\right).\n\nWe save the optimal costs and the corresponding controls at the given grid points (giving two arrays of values), decrement the time to N-2, and repeat. All the way down to the initial time i.\nLet’s summarize that as an outcome of this whole procedure we have two tables – one for the optimal cost, the other for the optimal control.\n\n\n\n Back to top",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming",
      "Tables as outcomes of dynamic programming"
    ]
  },
  {
    "objectID": "cont_indir_constrained.html",
    "href": "cont_indir_constrained.html",
    "title": "Constrained optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Constrained optimal control"
    ]
  },
  {
    "objectID": "discr_indir_general.html",
    "href": "discr_indir_general.html",
    "title": "General nonlinear discrete-time optimal control",
    "section": "",
    "text": "While in the previous chapter we formulated an optimal control problem (OCP) directly as a mathematical programming (general NLP or even QP) problem over the control (and possibly state) trajectories, in this chapter we introduce an alternative – indirect – approach. The essence of the approach is that we formulate first-order necessary conditions of optimality for the OCP in the form of equations, and then solve these. Although less straightforward to extend with additional constraints than the direct approach, the indirect approach also exhibits some advantages. In particular, in some cases (such as a quadratic cost and a linear system) it yields a feedback controller and not just a control trajetory.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General nonlinear discrete-time optimal control"
    ]
  },
  {
    "objectID": "discr_indir_general.html#optimization-constrains-given-only-by-the-state-equations",
    "href": "discr_indir_general.html#optimization-constrains-given-only-by-the-state-equations",
    "title": "General nonlinear discrete-time optimal control",
    "section": "Optimization constrains given only by the state equations",
    "text": "Optimization constrains given only by the state equations\nAs in the chapter on the direct approach, here we also start by considering a general nonlinear and possibly time-varying discrete-time dynamical system characterized by the state vector \\bm x_k\\in\\mathbb R^n whose evolution in discrete time k is uniquely determined by the state equation \n\\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\n accompanied by the initial state (vector) \\bm x_i\\in\\mathbb R^n and a sequence of control inputs \\bm u_i, \\bm u_{i+1}, \\ldots, \\bm u_{k-1}, where the control variable can also be a vector, that is, \\bm u_k \\in \\mathbb R^m.\nThese state equations will constitute the only constraints of the optimization problem. Unlike in the direct approach, here in our introductory treatment we do not impose any inequality constraints such as bounds on the control inputs, because the theory to be presented is not able to handle them.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General nonlinear discrete-time optimal control"
    ]
  },
  {
    "objectID": "discr_indir_general.html#general-additive-cost-function",
    "href": "discr_indir_general.html#general-additive-cost-function",
    "title": "General nonlinear discrete-time optimal control",
    "section": "General additive cost function",
    "text": "General additive cost function\nFor the above described dynamical system we want to find a control sequence \\bm u_k that minimizes a suitable optimization criterion over a finite horizon k\\in[i,N]. Namely, we will look for a control that minimizes a criterion of the following kind \nJ_i^N(\\underbrace{\\bm x_{i+1}, \\bm x_{i+2}, \\ldots, \\bm x_{N}}_{\\bar{\\bm x}}, \\underbrace{\\bm u_{i}, \\ldots, \\bm u_{N-1}}_{\\bar{\\bm u}};\\bm x_i) = \\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k).\n\\tag{1}\n\n\n\n\n\n\nNote\n\n\n\nRegarding the notation J_i^N(\\cdot) for the cost, if the initial and final times are understood from the context, they do not have to be displayed. But we will soon need to indicate the initial time explicitly in our derivations.\n\n\nThe property of the presented cost function that will turn out crucial in our subsequent work is that is additive over the time horizon. Although this restricts the class of cost functions a bit, it is still general enough to encompass a wide range of problems, such as minimizing the total (financial) cost to be paid, the total energy to be used, the total distance to be travelled, the cumulative error to be minimized, etc.\nHere is a list of a few popular cost functions.\n\nMinimum-time (or time-optimal) problem\n\nSetting \\phi=1 and L_k=1 gives J=N-i, that is, the length of the time horizon, the duration of control. Altough in this course we do not introduce concepts and tools for optimization over integer variables, in this simple case of just a single integer variable even a simple search over the length of control interval will be computationally tractable. Furthermore, as we will see in one of the next chapters once we switch from discrete-time to continuous-time systems, this time-optimal control design problem will turn out tractable using the tools presented in this course.\n\nMinimum-fuel problem\n\nSetting \\phi=0 and L_k=|u_k|, which gives J=\\sum_{k=i}^{N-1}|u_k|.\n\nMinimum-energy problem\n\nSetting \\phi=0 and L_k=\\frac{1}{2} u_k^2, which gives J=\\frac{1}{2} \\sum_{k=i}^{N-1} u_k^2. It is fair to admit that this sum of squared inputs cannot always be interpretted as the energy – for instance, what if the control input is a degree of openning of a valve? Sum of angles over time can hardly be interpreted as energy. Instead, it should be interpretted in the mathematical way as the (squared) norm, that is, a “size” of the input. Note that the same objection can be given to the previous case of a minimum-fuel problem.\n\nMixed quadratic problem (also LQ-optimal control problem)\n\nSetting \\phi=\\frac{1}{2}s_N x_N^2 and L_k=\\frac{1}{2} (qx_k^2+ru_k^2),\\, q,r\\geq 0, which gives J=\\frac{1}{2}s_Nx_N^2+\\frac{1}{2} \\sum_{k=i}^{N-1} (r x_k^2+q u_k^2). Or in the case of vector state and control variables J=\\frac{1}{2}\\bm x_N^\\top \\mathbf S_N \\bm x_N+\\frac{1}{2} \\sum_{k=i}^{N-1} (\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k), \\, \\mathbf Q, \\mathbf R \\succeq 0. This type of an optimization cost is particularly popular. Both for the mathematical reasons (we all now appreciate the nice properties of quadratic functions) and for practical engineering reasons as it allows us to capture a trade-off between the control performance (penalty on \\bm x_k) and control effort (penalty on \\bm u_k). Note also that the state at the terminal time N is penalized separately just in order to allow another trade-off between the transient and terminal behavior. The cost function can also be modified to penalize deviation of the state from some nonzero desired (aka reference) state trajectory, that is J=\\frac{1}{2}(\\bm x_N - \\bm x_N^\\text{ref})^\\top \\mathbf S_N (\\bm x_N - \\bm x_N^\\text{ref}) +\\frac{1}{2} \\sum_{k=i}^{N-1} \\left((\\bm x_k - \\bm x_k^\\text{ref})^\\top \\mathbf Q (\\bm x_k - \\bm x_k^\\text{ref}) + \\bm u_k^\\top \\mathbf R \\bm u_k\\right).\n\n\nNote that in none of these cost function did we include \\bm u_{N} as an optimization variables as it has no influence over the interval [i,N].\nIt is perhaps needless to emphasize that while in some other applications maximizing may seem more appropriate (such as maximizing the yield, bandwidth or robustness), we can always reformulate the maximization into minimization. Therefore in our course we alway formulate the optimal control problems as minimization problems.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General nonlinear discrete-time optimal control"
    ]
  },
  {
    "objectID": "discr_indir_general.html#derivation-of-the-first-order-necessary-conditions-of-optimality",
    "href": "discr_indir_general.html#derivation-of-the-first-order-necessary-conditions-of-optimality",
    "title": "General nonlinear discrete-time optimal control",
    "section": "Derivation of the first-order necessary conditions of optimality",
    "text": "Derivation of the first-order necessary conditions of optimality\nHaving formulated a finite-dimensional constrained nonlinear optimization problem, we avoid the temptation to call an NLP solver to solve it numerically and proceed instead with our own analysis of the problem. Let’s see how far we can get.\nBy introducing Lagrange multipliers {\\color{blue}\\bm\\lambda_k} we turn the constrained problem into an unconstrained one. The new cost function (we use the prime to distinguish it from the original cost) is \n\\begin{aligned}\n& {J'}_i^N(\\bm x_i, \\ldots, \\bm x_N, \\bm u_i, \\ldots, \\bm u_{N-1},{\\color{blue}\\bm \\lambda_i, \\ldots, \\bm \\lambda_{N-1}}) \\\\\n&\\qquad\\qquad\\qquad = \\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1}\\left[L_{k}(\\bm x_k,\\bm u_k)+\\bm {\\color{blue}\\lambda^\\top_{k}}\\;\\left[\\mathbf f_k(\\bm x_k,\\bm u_k)-\\bm x_{k+1}\\right]\\right].\n\\end{aligned}\n\nFrom now on, in principle, we do not need any guidance here, do we? We are given an unconstrained optimization problem and its solution is just a few steps away. In particular, stationary point(s) must be found (and then we are going to argue if these qualify as minimizers or not). This calls for differentiating the above expression with respect to all the variables and setting these derivatives equal to zeros.\nAlthough the principles are clear, some hindsight might be shared here if compact formulas are to be found. First such advice is to rename the variable(s) {\\color{blue}\\boldsymbol \\lambda_k} to {\\color{red}\\boldsymbol \\lambda_{k+1}} \n\\begin{aligned}\n& {J'}_i^N(\\bm x_i, \\ldots, \\bm x_N, \\bm u_i, \\ldots, \\bm u_{N-1},{\\color{red}\\bm \\lambda_{i+1}, \\ldots, \\bm \\lambda_{N}}) \\\\\n& \\qquad\\qquad\\qquad = \\phi(N,\\bm x_N) + \\sum_{k=i}^{N-1}\\left[L_{k}(\\bm x_k,\\bm u_k)+\\boldsymbol {\\color{red}\\boldsymbol \\lambda^\\top_{k+1}}\\; \\left[\\mathbf f_k(\\bm x_k,\\bm u_k)-\\mathbf x_{k+1}\\right]\\right].\n\\end{aligned}\n\nThis is really just a notational decision but thanks to it our resulting formulas will enjoy some symmetry.\n\n\n\n\n\n\nNote\n\n\n\nMaybe it would be more didactic to leave you to go on without this advice notation and only then to nudge you to figure out this remedy on your own. But admittedly this is not the kind of competence that we aim at in this course. Let’s spend time with more rewarding things.\n\n\nAnother notational advice – but this one is more systematic and fundamental — is to make the above expression a bit shorter by introducing a new variable defined as \\boxed{H_k(\\bm x_k,\\bm u_k,\\boldsymbol\\lambda_{k+1}) = L_{k}(\\bm x_k,\\bm u_k)+\\boldsymbol \\lambda_{k+1}^\\top \\; \\mathbf f_k(\\bm x_k,\\bm u_k).}\n\nWe will call this new function Hamiltonian. Indeed, the choice of this name is motivated by the analogy with the equally named concept used in physics and theoretical mechanics, but we will only make more references to this analogy later in the course once we transition to continuous-time systems modelled by differential equations.\nIntroducing the Hamiltonian reformulates the cost function (and we omit the explicit dependence on all its input arguments) as \n{J'}_i^N = \\phi(N,\\bm x_N) + \\sum_{k=i}^{N-1}\\left[H_{k}(\\bm x_k,\\bm u_k,\\boldsymbol\\lambda_{k+1})-\\boldsymbol\\lambda^\\top_{k+1}\\;\\mathbf x_{k+1}\\right].\n\nThe final polishing of the expression before starting to compute the derivatives consists in bringing together the terms that contain related variables: the state \\bm x_N at the final time, the state \\bm x_i at the initial time, and the states, controls and Lagrange multipliers in the transient period\n\n{J'}_i^N = \\underbrace{\\phi(N,\\bm x_N) -\\boldsymbol\\lambda^\\top_{N}\\;\\mathbf x_{N}}_\\text{at terminal time} + \\underbrace{H_i(\\bm x_i,\\mathbf u_i,\\boldsymbol\\lambda_{i+1})}_\\text{at initial time} + \\sum_{k=i+1}^{N-1}\\left[H_{k}(\\bm x_k,\\bm u_k,\\boldsymbol\\lambda_{k+1})-\\boldsymbol\\lambda^\\top_{k}\\;\\mathbf x_{k}\\right].\n\nAlthough this step was not necessary, it will make things a bit more convenient once we start looking for the derivatives. And the time for it has just come.\nRecall now the recommended procedure for finding derivatives of functions of vectors – find the differential instead and identify the derivative in the result. The gradient is then (by convention) obtained as the transpose of the derivative. Following this derivative-identification procedure, we anticipate the differential of the augmented cost function in the following form \n\\begin{split}\n\\text{d}{J'}_i^N &= (\\qquad)^\\top \\; \\text{d}\\bm x_N + (\\qquad)^\\top \\; \\text{d}\\bm x_i \\\\&+ \\sum_{k=i+1}^{N-1}(\\qquad)^\\top \\; \\text{d}\\bm x_k + \\sum_{k=i}^{N-1}(\\qquad)^\\top \\; \\text{d}\\bm u_k + \\sum_{k=i+1}^{N}(\\qquad)^\\top \\; \\text{d}\\boldsymbol \\lambda_k.\n\\end{split}\n\nIdentifying the gradients amounts to filling in the empty brackets. It straightforward if tedious (in particular the lower and upper bounds on the summation indices must be carefuly checked). The solution is \n\\begin{split}\n\\text{d}{J'}_i^N &= \\left(\\nabla_{\\bm x_N}\\phi-\\lambda_N\\right)^\\top \\; \\text{d}\\bm x_N + \\left(\\nabla_{\\bm x_i}H_i\\right)^\\top \\; \\text{d}\\bm x_i \\\\&+ \\sum_{k=i+1}^{N-1}\\left(\\nabla_{\\bm x_k}H_k-\\boldsymbol\\lambda_k\\right)^\\top \\; \\text{d}\\bm x_k + \\sum_{k=i}^{N-1}\\left(\\nabla_{\\bm u_k}H_k\\right)^\\top \\; \\text{d}\\bm u_k + \\sum_{k=i+1}^{N}\\left(\\nabla_{\\boldsymbol \\lambda_k}H_{k-1}-\\bm x_k\\right)^\\top \\; \\text{d}\\boldsymbol \\lambda_k.\n\\end{split}\n\nThe ultimate goal of this derivation was to find stationary points for the augmented cost function, that is, to find conditions under which \\text{d}{J'}_i^N=0. In typical optimization problems, the optimization is conducted with respect to all the participating variables, which means that the corresponding differentials may be arbitrary and the only way to guarantee that the total differential of J_i' is zeros is to make the associated gradients (the contents of the brackets) equal to zero. There are two exceptions to this rule in our case, though:\n\nThe state at the initial time is typically fixed and not available for optimization. Then \\text{d}\\bm x_i=0 and the corresponding necessary condition is replaced by the statement that \\bm x_i is equal to some particular value, say, \\bm x_i = \\mathbf x^\\text{init}. We have already discussed this before. In fact, in these situations we might even prefer to reflect it by the notation J_i^N(\\ldots;\\bm x_i), which emphasizes that \\bm x_i is a parameter and not a variable. But in the solution below we do allow for the possibility that \\bm x_i is a variable too (hence \\text{d}\\bm x_i\\neq 0) for completeness.\nThe state at the final time may also be given/fixed, in which case the corresponding condition is replaced by the statement that \\bm x_N is equal to some particular value, say, \\bm x_N = \\mathbf x^\\text{ref}. But if it is not the case, then the final state is also subject to optimization and the corresponding necessary condition of optimality is obtained by setting the content of the corresponding brackets to zero.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General nonlinear discrete-time optimal control"
    ]
  },
  {
    "objectID": "discr_indir_general.html#necessary-conditions-of-optimality-as-two-point-boundary-value-problem-tp-bvp",
    "href": "discr_indir_general.html#necessary-conditions-of-optimality-as-two-point-boundary-value-problem-tp-bvp",
    "title": "General nonlinear discrete-time optimal control",
    "section": "Necessary conditions of optimality as two-point boundary value problem (TP-BVP)",
    "text": "Necessary conditions of optimality as two-point boundary value problem (TP-BVP)\nThe ultimate form of the first-order necessary conditions of optimality, which incorporates the special cases discussed above, is given by these equations \n\\boxed{\n\\begin{aligned}\n\\mathbf x_{k+1} &= \\nabla_{\\boldsymbol\\lambda_{k+1}}H_k, \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1},\\\\\n\\boldsymbol\\lambda_k &= \\nabla_{\\bm x_k}H_k, \\;\\;\\; \\color{gray}{k=i+1,\\ldots, N-1}\\\\\n0 &=  \\nabla_{\\bm u_k}H_k, \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1}\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_N}\\phi-\\lambda_N\\right)^\\top \\mathrm{d}\\bm x_N},\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_i}H_i\\right)^\\top \\mathrm{d}\\bm x_i},\n\\end{aligned}\n}\n or more explicitly \n\\boxed{\n\\begin{aligned}\n\\mathbf x_{k+1} &= \\mathbf f_k(\\bm x_k,\\bm u_k), \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1},\\\\\n\\boldsymbol\\lambda_k &= \\nabla_{\\bm x_k}\\mathbf f_k\\;\\;   \\boldsymbol\\lambda_{\\mathbf k+1}+\\nabla_{\\bm x_k}L_k, \\;\\;\\; \\color{gray}{k=i+1,\\ldots, N-1}\\\\\n0 &=  \\nabla_{\\bm u_k}\\mathbf f_k\\;\\; \\boldsymbol\\lambda_{k+1}+\\nabla_{u_k}L_k, \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1}\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_N}\\phi-\\lambda_N\\right)^\\top \\mathrm{d}\\bm x_N},\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_i}H_i\\right)^\\top \\mathrm{d}\\bm x_i}.\n\\end{aligned}\n}\n\nRecall that since \\mathbf f is a vector function, \\nabla \\mathbf f is not just a gradient but rather a matrix whose columns are gradients of the individual components of the vector \\mathbf f — it is a transpose of Jacobian.\n\n\n\n\n\n\nNote\n\n\n\nThe first three necessary conditions above can be made completely “symmetric” by running the second one from k=i because the \\boldsymbol\\lambda_i introduced this way does not influence the rest of the problem and we could easily live with one useless variable.\n\n\nWe have just derived the (necessary) conditions of optimality in the form of five sets of (vector) equations:\n\nThe first two are recursive (or recurrent or also just discrete-time) equations, which means that they introduce coupling between the variables evaluated at consecutive times. In fact, the former is just the standard state equation that gives the state at one time as a function of the state (and the control) at the previous time. The latter gives a prescription for the variable \\bm \\lambda_k as a function of (among others) the same variable evaluated at the next (!) time, that is, \\bm \\lambda_{k+1}. Although from the optimization perspective these variables play the role of Lagrange multipliers, we call them co-state variables in optimal control theory because of the way they relate to the state equations. The corresponding vector equation is called a co-state equation.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is a crucial property of the co-state equation that it dictates the evolution of the co-state variable backward in time.\n\n\n\nThe third set of equations are just algebraic equations that relate the control inputs to the state and co-state variables. Sometimes it is called a stationarity equation.\nThe last two are just single (vector) equations related to the end and the beginning of the time horizon. They are both stated in the general enough form that allows the corresponding states to be treated as either fixed or subject to optimization. In particular, if the final state is to be treated as free (subject to optimization), that is, \\mathrm{d}\\bm x_N can be atritrary and the only way the corresponding equation can be satisfied is \\nabla_{\\bm x_N}\\phi=\\lambda_N. If, on the other hand, the final state is to be treated as fixed, the the corresponding equation is just replaced by \\bm x_N = \\mathbf x^\\text{ref}. Similarly for the initial state. But as we have hinted a few times, most often than not the initial state will be regarded as fixed and not subject to optimization, in which case the corresponding equation is replaced by \\bm x_i = \\mathbf x^\\text{init}.\n\nTo summarize, the equations that give the necessary conditions of optimality for a general nonlinear discrete-time optimal control problem form a two-point boundary value problem (TP-BVP). Values of some variables are specified at the initial time, values of some (maybe the same or some other) variables are defined at the final time. The equations prescribe the evolution of some variables forward in time while for some other variables the evolution backward in time is dictated.\n\n\n\n\n\n\nNote\n\n\n\nThis is in contrast with the initial value problem (IVP) for state equations, for which we only specify the state at one end of the time horizon — the initial state — and then the state equation disctates the evolution of the (state) variable forward in time.\n\n\nBoundary value problems are notoriously difficult to solve. Typically we can only solve them numerically, in which case it is appropriate to ask if anything has been gained by this indirect procedure compared with the direct one. After all, we did not even incorporate the inequality constraints in the problem, which was a piece of case in the direct approach. But we will see that in some special cases the TP-BVP they can be solved analytically and the outcome is particularly useful and would never have been discovered, if only the direct approach had been followed. We elaborate on this in the next section.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General nonlinear discrete-time optimal control"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html",
    "href": "discr_dir_mpc.html",
    "title": "Model predictive control (MPC)",
    "section": "",
    "text": "In the previous section we learnt how to compute an optimal control sequence on a finite time horizon using numerical methods for solving nonlinear programs (NLP), and quadratic programs (QP) in particular. There are two major deficiencies of such approach:\n\nThe control sequence was computed under the assumption that the mathematical model is perfectly accurate. As soon as the reality deviates from the model, either because of some unmodelled dynamics or because of the presence of (external) disturbances, the performance of the system will deteriorate. We need a way to turn the presented open-loop (also feedforward) control scheme into a feedback one.\nThe control sequence was computed for a finite time horizon. It is commonly required to consider an infinite time horizon, which is not possible with the presented approach based on solving finite-dimensional mathematical programs.\n\nThere are several ways to address these issues. Here we introduced one of them. It is knowns are Model Predictive Control (MPC), or also Receding Horizon Control (RHC). Some more are presented in the next two sections (one based on indirect approach, another one based on dynamic programming).",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#deficiencies-of-precomputed-open-loop-optimal-control",
    "href": "discr_dir_mpc.html#deficiencies-of-precomputed-open-loop-optimal-control",
    "title": "Model predictive control (MPC)",
    "section": "",
    "text": "In the previous section we learnt how to compute an optimal control sequence on a finite time horizon using numerical methods for solving nonlinear programs (NLP), and quadratic programs (QP) in particular. There are two major deficiencies of such approach:\n\nThe control sequence was computed under the assumption that the mathematical model is perfectly accurate. As soon as the reality deviates from the model, either because of some unmodelled dynamics or because of the presence of (external) disturbances, the performance of the system will deteriorate. We need a way to turn the presented open-loop (also feedforward) control scheme into a feedback one.\nThe control sequence was computed for a finite time horizon. It is commonly required to consider an infinite time horizon, which is not possible with the presented approach based on solving finite-dimensional mathematical programs.\n\nThere are several ways to address these issues. Here we introduced one of them. It is knowns are Model Predictive Control (MPC), or also Receding Horizon Control (RHC). Some more are presented in the next two sections (one based on indirect approach, another one based on dynamic programming).",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#model-predictive-control-mpc-as-a-way-to-turn-open-loop-control-into-feedback-control",
    "href": "discr_dir_mpc.html#model-predictive-control-mpc-as-a-way-to-turn-open-loop-control-into-feedback-control",
    "title": "Model predictive control (MPC)",
    "section": "Model predictive control (MPC) as a way to turn open-loop control into feedback control",
    "text": "Model predictive control (MPC) as a way to turn open-loop control into feedback control\nThe idea is to compute an optimal control sequence on a finite time horizon using the material presented in the previous section, apply only the first control action to the system, and then repeat the procedure upon shifting the time horizon by one time step.\nAlthough this name “model predictive control” is commonly used in the control community, the other – perhaps a bit less popular – name “receding horizon control” is equally descriptive, if not even a bit more.\n\n\n\n\n\n\nNote\n\n\n\nIt may take a few moments to digest the idea, but it is actually quite natural. As a matter of fact, this is the way most of us control our lifes every day. We plan our actions on a finite time horizon, and while building this plan we use our understanding (model) of the world. We then perform the first action from our plan, observe the impact of our action and possibly a change in the environment, and update our plan accordingly on a new (shifted) time horizon. We repeat this procedure over and over again. It is crucial that the prediction horizon must be long enough so that the full impact of our actions can be observed.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#linear-vs-nonlinear-mpc",
    "href": "discr_dir_mpc.html#linear-vs-nonlinear-mpc",
    "title": "Model predictive control (MPC)",
    "section": "Linear vs nonlinear MPC",
    "text": "Linear vs nonlinear MPC\nx",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#prediction-horizon-vs-control",
    "href": "discr_dir_mpc.html#prediction-horizon-vs-control",
    "title": "Model predictive control (MPC)",
    "section": "Prediction horizon vs control",
    "text": "Prediction horizon vs control\nx",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#hard-constraints-vs-soft-constraints",
    "href": "discr_dir_mpc.html#hard-constraints-vs-soft-constraints",
    "title": "Model predictive control (MPC)",
    "section": "Hard constraints vs soft constraints",
    "text": "Hard constraints vs soft constraints\nx",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#literature",
    "href": "discr_dir_mpc.html#literature",
    "title": "Model predictive control (MPC)",
    "section": "Literature",
    "text": "Literature\n(Rawlings, Mayne, and Diehl 2017), (Borrelli, Bemporad, and Morari 2017), (Gros and Diehl 2022) (Bemporad 2021)",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html",
    "href": "opt_theory_modellers.html",
    "title": "Optimization modelling languages",
    "section": "",
    "text": "Realistically complex optimization problems cannot be solved with just a pen and a paper – computer programs (often called optimization solvers) are needed to solve them. And now comes the challenge: as various solvers for even the same class of problems differ in the algorithms they implement, so do their interfaces – every solver expects the inputs (the data defining the optimization problem) in a specific format. This makes it difficult to switch between solvers, as the problem data has to be reformatted every time.\n\nExample 1 (Data formatting for different solvers) Consider the following optimization problem: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix} \\leq \\begin{bmatrix} 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix} \\bm x \\leq  \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7\\end{bmatrix}\n\\end{aligned}\n\nThere are dozens of solvers that can be used to solve this problem. Here we demonstrate a usage of these two: OSQP and COSMO.jl. And we are going to call the solvers in Julia (using the the wrappers OSQP.jl for the former). First, we start with OSQP (in fact, this is their example):\n\n\nShow the code\nusing OSQP\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nproblem_OSQP = OSQP.Model()\nOSQP.setup!(problem_OSQP; P=P, q=q, A=A, l=l, u=u, alpha=1, verbose=false)\n\n# Solve the optimization problem and show the results\nresults_OSQP = OSQP.solve!(problem_OSQP)\nresults_OSQP.x\n\n\nNow we do the same with COSMO. First, we must take into account that COSMO cannot accept two-sided inequalities, so we have to reformulate the problem so that the constraints are only in the form of \\mathbf A\\bm x + b \\geq \\bm 0: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix} -1 & -1\\\\ -1 & 0\\\\ 0 & -1\\\\ 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix}\\bm x + \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7 \\\\ -1 \\\\ 0 \\\\ 0\\end{bmatrix} \\geq  \\mathbf 0.\n\\end{aligned}\n\n\n\nShow the code\nusing COSMO\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nAa = [-A; A]\nba = [u; -l]\n\nproblem_COSMO = COSMO.Model()\nconstraint = COSMO.Constraint(Aa, ba, COSMO.Nonnegatives)\nsettings = COSMO.Settings(verbose=false)\nassemble!(problem_COSMO, P, q, constraint, settings = settings)\n\n# Solve the optimization problem and show the results\nresults_COSMO = COSMO.optimize!(problem_COSMO)\nresults_COSMO.x\n\n\nAlthough the two solvers are solving the same problem, the data has to be formatted differently for each of them (and the difference in syntax is not negligible either).\nWhat if we could formulate the same problem without considering the pecualiarities of each solver? It turns out that it is possible. In Julia we can use JuMP.jl:\n\n\nShow the code\nusing JuMP\nusing SparseArrays\nusing OSQP, COSMO\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nmodel_JuMP = Model()\n@variable(model_JuMP, x[1:2])\n@objective(model_JuMP, Min, 0.5*x'*P*x + q'*x)\n@constraint(model_JuMP, A*x .&lt;= u)\n@constraint(model_JuMP, A*x .&gt;= l)\n\n# Solve the optimization problem using OSQP and show the results\nset_silent(model_JuMP)\nset_optimizer(model_JuMP, OSQP.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_OSQP = value.(x)\n\n# Now solve the problem using COSMO and show the results\nset_optimizer(model_JuMP, COSMO.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_COSMO = value.(x)\n\n\n\nNotice how the optimization problem is defined just once in the last code and then different solvers can be chosen to solve it. The code represents an instance of a so-called optimization modelling language (OML), or actually its major class called algebraic modelling language (AML).\nThe key motivation for using an OML/AML is to separate the process of formulating the problem from the process of solving it (using a particular solver). Furthermore, such solver-independent problem description (called optimization model) better mimics the way we formulate these problems using a pen and a paper, making it (perhaps) a bit more convenient to write our own and read someone else’s models.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#why-optimization-modelling-languages",
    "href": "opt_theory_modellers.html#why-optimization-modelling-languages",
    "title": "Optimization modelling languages",
    "section": "",
    "text": "Realistically complex optimization problems cannot be solved with just a pen and a paper – computer programs (often called optimization solvers) are needed to solve them. And now comes the challenge: as various solvers for even the same class of problems differ in the algorithms they implement, so do their interfaces – every solver expects the inputs (the data defining the optimization problem) in a specific format. This makes it difficult to switch between solvers, as the problem data has to be reformatted every time.\n\nExample 1 (Data formatting for different solvers) Consider the following optimization problem: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix} \\leq \\begin{bmatrix} 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix} \\bm x \\leq  \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7\\end{bmatrix}\n\\end{aligned}\n\nThere are dozens of solvers that can be used to solve this problem. Here we demonstrate a usage of these two: OSQP and COSMO.jl. And we are going to call the solvers in Julia (using the the wrappers OSQP.jl for the former). First, we start with OSQP (in fact, this is their example):\n\n\nShow the code\nusing OSQP\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nproblem_OSQP = OSQP.Model()\nOSQP.setup!(problem_OSQP; P=P, q=q, A=A, l=l, u=u, alpha=1, verbose=false)\n\n# Solve the optimization problem and show the results\nresults_OSQP = OSQP.solve!(problem_OSQP)\nresults_OSQP.x\n\n\nNow we do the same with COSMO. First, we must take into account that COSMO cannot accept two-sided inequalities, so we have to reformulate the problem so that the constraints are only in the form of \\mathbf A\\bm x + b \\geq \\bm 0: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix} -1 & -1\\\\ -1 & 0\\\\ 0 & -1\\\\ 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix}\\bm x + \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7 \\\\ -1 \\\\ 0 \\\\ 0\\end{bmatrix} \\geq  \\mathbf 0.\n\\end{aligned}\n\n\n\nShow the code\nusing COSMO\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nAa = [-A; A]\nba = [u; -l]\n\nproblem_COSMO = COSMO.Model()\nconstraint = COSMO.Constraint(Aa, ba, COSMO.Nonnegatives)\nsettings = COSMO.Settings(verbose=false)\nassemble!(problem_COSMO, P, q, constraint, settings = settings)\n\n# Solve the optimization problem and show the results\nresults_COSMO = COSMO.optimize!(problem_COSMO)\nresults_COSMO.x\n\n\nAlthough the two solvers are solving the same problem, the data has to be formatted differently for each of them (and the difference in syntax is not negligible either).\nWhat if we could formulate the same problem without considering the pecualiarities of each solver? It turns out that it is possible. In Julia we can use JuMP.jl:\n\n\nShow the code\nusing JuMP\nusing SparseArrays\nusing OSQP, COSMO\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nmodel_JuMP = Model()\n@variable(model_JuMP, x[1:2])\n@objective(model_JuMP, Min, 0.5*x'*P*x + q'*x)\n@constraint(model_JuMP, A*x .&lt;= u)\n@constraint(model_JuMP, A*x .&gt;= l)\n\n# Solve the optimization problem using OSQP and show the results\nset_silent(model_JuMP)\nset_optimizer(model_JuMP, OSQP.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_OSQP = value.(x)\n\n# Now solve the problem using COSMO and show the results\nset_optimizer(model_JuMP, COSMO.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_COSMO = value.(x)\n\n\n\nNotice how the optimization problem is defined just once in the last code and then different solvers can be chosen to solve it. The code represents an instance of a so-called optimization modelling language (OML), or actually its major class called algebraic modelling language (AML).\nThe key motivation for using an OML/AML is to separate the process of formulating the problem from the process of solving it (using a particular solver). Furthermore, such solver-independent problem description (called optimization model) better mimics the way we formulate these problems using a pen and a paper, making it (perhaps) a bit more convenient to write our own and read someone else’s models.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#why-not-optimization-modelling-languages",
    "href": "opt_theory_modellers.html#why-not-optimization-modelling-languages",
    "title": "Optimization modelling languages",
    "section": "Why not optimization modelling languages?",
    "text": "Why not optimization modelling languages?\nAs a matter of fact, some optimization experts even keep avoiding OML/AML altogether. For example, if a company pays for a (not really cheap) license of Gurobi Optimizer – a powerful optimization library for (MI)LP/QP/QCQP –, it may be the case that for a particular very large-scale optimization problem their optimization specialist will have hard time to find a third-party solver of comparable performance. If then its Python API makes definition of optimization problems convenient too (see the code below), maybe there is little regret that such problem definitions cannot be reused with a third-party solver. The more so that since it is tailored to Gurobi solver, it will offer control over the finest details.\n\n\nShow the code\nimport gurobipy as gp\nimport numpy as np\n\n# Define the data for the model\nP = np.array([[4.0, 1.0], [1.0, 2.0]])\nq = np.array([1.0, 1.0])\nA = np.array([[1.0, 1.0], [1.0, 0.0], [0.0, 1.0]])\nl = np.array([1.0, 0.0, 0.0])\nu = np.array([1.0, 0.7, 0.7])\n\n# Create a new model\nm = gp.Model(\"qp\")\n\n# Create a vector variable\nx = m.addMVar((2,))\n\n# Set the objective\nobj = 1/2*(x@P@x + q@x)\nm.setObjective(obj)\n\n# Add the constraints\nm.addConstr(A@x &gt;= l, \"c1\")\nm.addConstr(A@x &lt;= u, \"c2\")\n\n# Run the solver\nm.optimize()\n\n# Print the results\nfor v in m.getVars():\n    print(f\"{v.VarName} {v.X:g}\")\n\nprint(f\"Obj: {m.ObjVal:g}\")\n\n\nSimilar and yet different is the story of the IBM ILOG CPLEX, another top-notch solvers addressing the same problems as Gurobi. They do have their own modeling language called Optimization Modelling Language (OPL), but it is also only interfacing with their solver(s). We can only guess that their motivation for developing their own optimization modelling language was that at the time of its developments (in 1990s) Python was still in pre-2.0 stage and formulating optimization problems in programming languages like C/C++ or Fortran was nowhere close to being convenient. Gurobi, in turn, started in 2008, when Python was already a popular language.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#language-independent-optimization-modelling-languages",
    "href": "opt_theory_modellers.html#language-independent-optimization-modelling-languages",
    "title": "Optimization modelling languages",
    "section": "Language-independent optimization modelling languages",
    "text": "Language-independent optimization modelling languages\nOptimization/algebraic modelling languages were originally developed outside programming languages, essentially as standalone tools. Examples are AMPL, GAMS, and, say, GLPK/GMPL (MathProg). We listed these main names here since they can be bumped across (they are still actively developed), but we are not going to discuss them in our course any further. The reason is that there are now alternatives that are implemented as packages/toolboxes in programming languages such as Julia, Matlab, and Python, which offer a more fluent workflow – a user can use the same programming language to acquire the data, preprocess them, formulate the optimization problem, configure and call a solver, and finally do some postprocessing including a visualization and whatever reporting, all without leaving the language of their choice.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#optimization-modelling-in-julia",
    "href": "opt_theory_modellers.html#optimization-modelling-in-julia",
    "title": "Optimization modelling languages",
    "section": "Optimization modelling in Julia",
    "text": "Optimization modelling in Julia\nMy obvious (personal) bias towards Julia programming language is partly due to the terrific support for optimization modelling in Julia:\n\nJuMP.jl not only constitutes one of the flagship packages of the Julia ecosystem but it is on par with the state of the art optimization modelling languages. Furthermore, being a free and open source software, it enjoys a vibrant community of developers and users. They even meet annually at JuMP-dev conference (in 2023 in Boston, MA).\nConvex.jl is an implementation of the concept of Disciplined Convex Programming (DCP) in Julia (below we also list its implementations in Matlab and Python). Even though it is now registered as a part of the JuMP.jl project, it is still a separate concept. Interesting, convenient, but it seems to be in a maintanence mode now.\n\n\n\nShow the code\nusing Convex, SCS\n\n# Define the problem data and build the problem description\nP = [4.0 1.0; 1.0 2.0]\nq = [1.0, 1.0]\nA = [1.0 1.0; 1.0 0.0; 0.0 1.0]\nl = [1.0, 0.0, 0.0]\nu = [1.0, 0.7, 0.7]\n\n# Create a vector variable of size n\nx = Variable(2)\n\n# Define the objective \nobjective = 1/2*quadform(x,P) + dot(q,x)\n\n# Define the constraints\nconstraints = [l &lt;= A*x, A*x &lt;= u]\n\n# Define the overal description of the optimization problem\nproblem = minimize(objective, constraints)\n\n# Solve the problem\nsolve!(problem, SCS.Optimizer; silent_solver = true)\n\n# Check the status of the problem\nproblem.status # :Optimal, :Infeasible, :Unbounded etc.\n\n# Get the optimum value\nproblem.optval\n\n# Get the optimal x\nx.value",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#optimization-modelling-in-matlab",
    "href": "opt_theory_modellers.html#optimization-modelling-in-matlab",
    "title": "Optimization modelling languages",
    "section": "Optimization modelling in Matlab",
    "text": "Optimization modelling in Matlab\nPopularity of Matlab as a language and an ecosystem for control-related computations is undeniable. Therefore, let’s have a look at what is available for modelling optimization problems in Matlab:\n\nOptimization Toolbox for Matlab is one of the commercial toolboxes produced by Matlab and Simulink creators. Since the R2017b release the toolbox supports Problem-based optimization workflow (besides the more traditional Solver-based optimization workflow supported since the beginning), which can be regarded as a kind of an optimization/algebraic modelling language, albeit restricted to their own solvers.\nYalmip started as Yet Another LMI Parser quite some time ago (which reveals its control theoretic roots), but these days it serves as fairly complete algebraic modelling language (within Matlab), interfacing to perhaps any optimization solver, both commercial and free&open-source. It is free and open-source. Is is still actively developed and maintained and it abounds with tutorials and examples.\nCVX is a Matlab counterpart of Convex.jl (or the other way around, if you like, since it has been here longer). The name stipulates that it only allows convex optimization probles (unlike Yalmip) – it follows the Disciplined Convex Programming (DCP) paradigm. Unfortunately, the development seems to have stalled – the last update is from 2020.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#optimization-modelling-in-python",
    "href": "opt_theory_modellers.html#optimization-modelling-in-python",
    "title": "Optimization modelling languages",
    "section": "Optimization modelling in Python",
    "text": "Optimization modelling in Python\nPython is a very popular language for scientific computing. Although it is arguable if it is actually suitable for implementation of numerical algoritms, when it comes to building optimization models, it does its job fairly well (and the numerical solvers it calls can be developed in different language). Several packages implementing OML/AML are available:\n\ncvxpy is yet another instantiation of Disciplined Convex Programming that we alredy mention when introducing Convex.jl and CVX. And it turns out that this one exhibits the greatest momentum. The team of developers seems to be have exceeded a critical mass, hence the tools seems like a safe bet already.\nPyomo is a popular open-source optimization modelling language within Python.\nAPMonitor and GEKKO are relatively young projects, primarily motivated by applications of machine learning and optimization in chemical process engineering.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "rocond_mixed_sensitivity.html",
    "href": "rocond_mixed_sensitivity.html",
    "title": "Mixed sensitivity design",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "12. Robust control",
      "Mixed sensitivity design"
    ]
  },
  {
    "objectID": "discr_dir_mpc_economic.html",
    "href": "discr_dir_mpc_economic.html",
    "title": "Economic MPC",
    "section": "",
    "text": "(Ellis, Liu, and Christofides 2017), (Ellis, Durand, and Christofides 2014), (Faulwasser, Grüne, and Müller 2018), (Rawlings, Angeli, and Bates 2012)",
    "crumbs": [
      "6. More on MPC",
      "Economic MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_economic.html#literature",
    "href": "discr_dir_mpc_economic.html#literature",
    "title": "Economic MPC",
    "section": "",
    "text": "(Ellis, Liu, and Christofides 2017), (Ellis, Durand, and Christofides 2014), (Faulwasser, Grüne, and Müller 2018), (Rawlings, Angeli, and Bates 2012)",
    "crumbs": [
      "6. More on MPC",
      "Economic MPC"
    ]
  },
  {
    "objectID": "ext_LTR.html",
    "href": "ext_LTR.html",
    "title": "Loop transfer recovery (LTR)",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Loop transfer recovery (LTR)"
    ]
  },
  {
    "objectID": "discr_dir_mpc_stability.html",
    "href": "discr_dir_mpc_stability.html",
    "title": "Stability of MPC",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "6. More on MPC",
      "Stability of MPC"
    ]
  },
  {
    "objectID": "cont_numerical_references.html",
    "href": "cont_numerical_references.html",
    "title": "References",
    "section": "",
    "text": "The indirect approach to the continuous-time optimal control problem (OCP) formulates the necessary conditions of optimality as a two-point boundary value problem (TP-BVP), which generally requires numerical methods. The direct approach to the continuous-time OCP relies heavily on numerical methods too, namely the methods for solving nonlinear programs (NLP) and methods for solving ordinary differential equations (ODE). Numerical methods for both approaches share a lot of common principles and tools, and these are collectively presented in the literature as called numerical optimal control. A recommendable (and freely online available) introduction to these methods is (Gros and Diehl 2022). Shorter version of this is in chapter 8 of (Rawlings, Mayne, and Diehl 2017), which is also available online. A more comprehensive treatment is in (Betts 2020).\nSome survey papers such as (Rao 2009) and (von Stryk and Bulirsch 1992) can also be useful, although now primarily as historical accounts. Similarly with the classics (Kirk 2004) and (Bryson and Ho 1975), which cover the indirect approach only.\nAnother name under which the numerical methods for the direct approach are presented is trajectory optimization. There are quite a few tutorials and surveys such as (M. Kelly 2017) and (M. P. Kelly 2017).\n\n\n\n\n Back to topReferences\n\nBetts, John T. 2020. Practical Methods for Optimal Control Using Nonlinear Programming. 3rd ed. Advances in Design and Control. Society for Industrial and Applied Mathematics. https://doi.org/10.1137/1.9781611976199.\n\n\nBryson, Arthur E., Jr., and Yu-Chi Ho. 1975. Applied Optimal Control: Optimization, Estimation and Control. Revised edition. CRC Press.\n\n\nGros, Sebastien, and Moritz Diehl. 2022. “Numerical Optimal Control (Draft).” Systems Control; Optimization Laboratory IMTEK, Faculty of Engineering, University of Freiburg. https://www.syscop.de/files/2020ss/NOC/book-NOCSE.pdf.\n\n\nKelly, Matthew. 2017. “An Introduction to Trajectory Optimization: How to Do Your Own Direct Collocation.” SIAM Review 59 (4): 849–904. https://doi.org/10.1137/16M1062569.\n\n\nKelly, Matthew P. 2017. “Transcription Methods for Trajectory Optimization: A Beginners Tutorial.” arXiv:1707.00284 [Math], July. http://arxiv.org/abs/1707.00284.\n\n\nKirk, Donald E. 2004. Optimal Control Theory: An Introduction. Reprint of the 1970 edition. Dover Publications.\n\n\nRao, Anil V. 2009. “A Survey of Numerical Methods for Optimal Control.” Advances in the Astronautical Sciences 135 (1): 497–528. http://vdol.mae.ufl.edu/ConferencePublications/trajectorySurveyAAS.pdf.\n\n\nRawlings, James B., David Q. Mayne, and Moritz M. Diehl. 2017. Model Predictive Control: Theory, Computation, and Design. 2nd ed. Madison, Wisconsin: Nob Hill Publishing, LLC. http://www.nobhillpublishing.com/mpc-paperback/index-mpc.html.\n\n\nvon Stryk, O., and R. Bulirsch. 1992. “Direct and Indirect Methods for Trajectory Optimization.” Annals of Operations Research 37 (1): 357–73. https://doi.org/10.1007/BF02071065.",
    "crumbs": [
      "9. Numerical methods for continuous-time optimal control - both indirect and direct approaches",
      "References"
    ]
  },
  {
    "objectID": "roban_software.html",
    "href": "roban_software.html",
    "title": "Software",
    "section": "",
    "text": "The primary tool for us in this part of the course is the Robust Control Toolbox for Matlab. A nice benefit is that accompanying video tutorials by Brian Douglas are available.\nAlternatives in other languages exist, but very often are less well developed and/or documented. A notable exception is RobustAndOptimalControl.jl for Julia.\n\n\n\n Back to top",
    "crumbs": [
      "11. Uncertainty modelling and robustness analysis",
      "Software"
    ]
  },
  {
    "objectID": "discr_dir_LQR.html",
    "href": "discr_dir_LQR.html",
    "title": "Finite-horizon LQR-optimal control as QP",
    "section": "",
    "text": "Here we specialize the general procedure from the previous section to the case of a linear system and a quadratic cost. we start by considering a simple problem of regulation, wherein the goal is to bring the system either exactly or approximately to zero final state, that is, \\mathbf x^\\text{ref}=\\mathbf 0 and we want \\bm x_N=\\mathbf x^\\text{ref} or \\bm x_N\\approx\\mathbf x^\\text{ref}, respectively. \n\\begin{aligned}\n\\operatorname*{minimize}_{\\mathbf u_0,\\ldots, \\mathbf u_{N-1}, \\mathbf x_{0},\\ldots, \\mathbf x_N} &\\quad  \\frac{1}{2} \\bm x_N^\\top \\mathbf S \\bm x_N + \\frac{1}{2} \\sum_{k=0}^{N-1} \\left(\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k \\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{k+1} = \\mathbf A\\bm x_k + \\mathbf B\\bm u_k,\\quad k = 0, \\ldots, N-1, \\\\\n                    &\\quad \\bm x_0 = \\mathbf x_0,\\\\\n                    &\\quad \\bm x_N = \\mathbf 0\\;  (\\text{or}\\, \\bm x_N \\approx \\mathbf 0).\n\\end{aligned}\n\nReferring to the two options for the last constraint,\n\nif the condition \\bm x_N=\\mathbf 0 on the final state is strictly enforced, the terminal state cost (the term \\frac{1}{2} \\bm x_N^\\top \\mathbf S \\bm x_N in the cost function) is redundant and can be removed;\nif the final state condition can be relaxed to \\bm x_N\\approx\\mathbf 0, it is by increasing the weight \\mathbf S in the terminal cost function \\frac{1}{2} \\bm x_N^\\top \\mathbf S \\bm x_N that \\bm x_N can be made arbitrarily close to \\mathbf 0.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a standard dilemma in optimization, not only in optimal control, that if we want to satisfy some requirement, we can either strictly enforce it through constraints or we can seemingly relax it and set a cost to be paid for not satysfying it.\n\n\n\nSimultaneous (sparse) formulation\nBelow we rewrite the latter problem, that is, \\bm x_N\\approx\\mathbf 0, in the “unrolled” form, where we stack the state and control variables into “long” vectors \\bar{\\bm x} and \\bar{\\bm u}. Doing the same for the former is straightforward. \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm u},\\bar{\\bm x}} & \\frac{1}{2}\\left(\\begin{bmatrix} \\bm x_1^\\top & \\bm x_2^\\top & \\ldots & \\bm x_N^\\top \\end{bmatrix}\n\\underbrace{\\begin{bmatrix}\\mathbf Q & & & \\\\ & \\mathbf Q & &\\\\ & &\\ddots & \\\\ & & & \\mathbf S \\end{bmatrix}}_{\\overline{\\mathbf Q}}\n\\underbrace{\\begin{bmatrix} \\bm x_1 \\\\ \\bm x_2 \\\\ \\vdots \\\\ \\bm x_N \\end{bmatrix}}_{\\bar{\\bm x}}\\right.\\\\\n&\\qquad +\\left.\n\\begin{bmatrix} \\bm u_0^\\top & \\bm u_1^\\top & \\ldots & \\bm u_{N-1}^\\top \\end{bmatrix}\n\\underbrace{\\begin{bmatrix}\\mathbf R & & & \\\\ & \\mathbf R & &\\\\ & &\\ddots & \\\\ & & & \\mathbf R \\end{bmatrix}}_{\\overline{\\mathbf R}}\n\\underbrace{\\begin{bmatrix} \\bm u_0 \\\\ \\bm u_1 \\\\ \\vdots \\\\ \\bm u_{N-1} \\end{bmatrix}}_{\\bar{\\bm u}}\\right)\n+ \\underbrace{\\frac{1}{2}\\mathbf x_0^\\top \\mathbf Q \\mathbf x_0}_{\\mathrm{constant}}\n\\end{aligned}\n subject to \n\\begin{bmatrix} \\bm x_1 \\\\ \\bm x_2 \\\\ \\bm x_3\\\\ \\vdots \\\\ \\bm x_N \\end{bmatrix} = \\underbrace{\\begin{bmatrix}\\mathbf 0 & & & &\\\\\\mathbf A & \\mathbf 0 & & &\\\\ &\\mathbf A &\\mathbf 0 & & \\\\ & & &\\ddots & \\\\& & &\\mathbf A & \\mathbf 0 \\end{bmatrix}}_{\\overline{\\mathbf A}}\n\\begin{bmatrix} \\bm x_1 \\\\ \\bm x_2 \\\\ \\bm x_3\\\\ \\vdots \\\\ \\bm x_N \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf B & & & & \\\\ & \\mathbf B & & & \\\\& &\\mathbf B & \\\\ & & &\\ddots \\\\ & & & & \\mathbf B \\end{bmatrix}}_{\\overline{\\mathbf B}}\\begin{bmatrix} \\bm u_0 \\\\ \\bm u_1 \\\\ \\bm u_2\\\\\\vdots \\\\ \\bm u_{N-1} \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf A\\\\\\mathbf 0\\\\\\mathbf 0\\\\\\vdots\\\\\\mathbf 0\\end{bmatrix}}_{\\overline{\\mathbf A}_0}\\mathbf x_0.   \n\nNote that the last term in the cost function can be discarded because it is constant.\nThe terms with the \\bar{\\bm x} vector can be combined and we get \n\\begin{bmatrix} \\mathbf 0 \\\\ \\mathbf 0 \\\\ \\mathbf 0\\\\ \\vdots \\\\ \\mathbf 0 \\end{bmatrix} = \\underbrace{\\begin{bmatrix}-\\mathbf I & & & &\\\\\\mathbf A & -\\mathbf I & & &\\\\ &\\mathbf A &-\\mathbf I & & \\\\ & & &\\ddots & \\\\& & &\\mathbf A & -\\mathbf I \\end{bmatrix}}_{\\overline{\\mathbf A} - \\mathbf I}\n\\begin{bmatrix} \\mathbf x_1 \\\\ \\mathbf x_2 \\\\ \\mathbf x_3\\\\ \\vdots \\\\ \\mathbf x_N \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf B & & & & \\\\ & \\mathbf B & & & \\\\& &\\mathbf B & \\\\ & & &\\ddots \\\\ & & & & \\mathbf B \\end{bmatrix}}_{\\overline{\\mathbf B}}\\begin{bmatrix} \\mathbf u_0 \\\\ \\mathbf u_1 \\\\ \\mathbf u_2\\\\\\vdots \\\\ \\mathbf u_{N-1} \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf A\\\\\\mathbf 0\\\\\\mathbf 0\\\\\\vdots\\\\\\mathbf 0\\end{bmatrix}}_{\\overline{\\mathbf A}_0}\\mathbf x_0.\n\\tag{1}\nUpon stacking the two “long” vectors into \\bar{\\bm z} we reformulate the optimization problem as \n\\operatorname*{minimize}_{\\widetilde{\\mathbf z}\\in\\mathbb{R}^{2N}}\\quad \\frac{1}{2}\\underbrace{\\begin{bmatrix}\\bar{\\bm x}^\\top &\\bar{\\bm u}^\\top\\end{bmatrix}}_{\\bar{\\bm z}^\\top} \\underbrace{\\begin{bmatrix}\\overline{\\mathbf Q} & \\\\ & \\overline{\\mathbf R} \\end{bmatrix}}_{\\widetilde{\\mathbf Q}}\\underbrace{\\begin{bmatrix}\\bar{\\bm x}\\\\\\bar{\\bm u}\\end{bmatrix}}_{\\bar{\\bm z}}\n subject to \n\\mathbf 0 = \\underbrace{\\begin{bmatrix}(\\overline{\\mathbf A}-\\mathbf I) & \\overline{\\mathbf B}\\end{bmatrix}}_{\\widetilde{\\mathbf A}}\\underbrace{\\begin{bmatrix}\\bar{\\bm x}\\\\\\bar{\\bm u}\\end{bmatrix}}_{\\bar{\\bm z}} + \\underbrace{\\overline{\\mathbf A}_0 \\mathbf x_0}_{\\tilde{\\mathbf b}}.\n\nTo summarize, we have reformulated the optimal control problem as a linearly constrained quadratic program \n\\boxed{\n\\begin{aligned}\n\\underset{\\bar{\\bm z}\\in\\mathbb{R}^{2N}}{\\text{minimize}} &\\quad \\frac{1}{2}\\bar{\\bm z}^\\top \\widetilde{\\mathbf Q} \\bar{\\bm z}\\\\\n\\text{subject to} &\\quad \\widetilde{\\mathbf A} \\bar{\\bm z} + \\tilde{\\bm b} = \\mathbf 0.\n\\end{aligned}}\n\n\n\nCode\nfunction direct_dlqr_simultaneous(A,B,x₀,Q,R,S,N)\n    Qbar = BlockArray(spzeros(N*n,N*n),repeat([n],N),repeat([n],N))\n    for i=1:(N-1)\n        Qbar[Block(i,i)] = Q\n    end\n    Qbar[Block(N,N)] = S\n    Rbar = BlockArray(spzeros(N*m,N*m),repeat([m],N),repeat([m],N))\n    for i=1:N\n        Rbar[Block(i,i)] = R\n    end\n    Qtilde = blockdiag(sparse(Qbar),sparse(Rbar))                               # The matrix defining the quadratic cost.\n    Bbar = BlockArray(spzeros(N*n,N*m),repeat([n],N),repeat([m],N))\n    for i=1:N\n        Bbar[Block(i,i)] = B\n    end\n    Abar = BlockArray(sparse(-1.0*I,n*N,n*N),repeat([n],N),repeat([n],N))\n    for i=2:N\n        Abar[Block(i,(i-1))] = A\n    end\n    Atilde = sparse([Abar Bbar])                                                # The matrix defining the linear (affine) equation.\n    A0bar = spzeros(n*N,n)\n    A0bar[1:n,1:n] = A\n    btilde = A0bar*sparse(x₀)                                                   # The constant offset for the linear (affine) equation.\n    K = [Qtilde Atilde'; Atilde spzeros(size(Atilde,1),size(Atilde,1))]         # Sparse KKT matrix.\n    F = qdldl(K)                                                                # KKT matrix LDL factorization.\n    k = [spzeros(size(Atilde,1)); -btilde]                                      # Right hand side of the KKT system\n    xtildeλ = solve(F,k)                                                        # Solving the KKT system using the factorization.\n    xopt = reshape(xtildeλ[1:(n*N)],(n,:))\n    uopt = reshape(xtildeλ[(n*N+1):(n+m)*N+1],(m,:))\n    return xopt,uopt\nend\n\nn = 2               # Number of state variables.\nm = 1               # Number of (control) input variables. \nA = rand(n,n)       # State matrix.\nB = rand(n,m)       # Input coupling matrix.\nx₀ = [1.0, 3.0]     # Initial state.\n\nN = 10              # Time horizon.\n\ns = [1.0, 2.0]      \nq = [1.0, 2.0]\nr = [1.0]\n\nS = diagm(0=&gt;s)     # Matrix defining the terminal state cost.\nQ = diagm(0=&gt;q)     # Matrix defining the running state dost.\nR = diagm(0=&gt;r)     # Matrix defining the cost of control.\n\nuopts,xopts = direct_dlqr_simultaneous(A,B,x₀,Q,R,S,N)\n\nusing Plots\np1 = plot(0:(N-1),uopts,marker=:diamond,label=\"u\",linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"u\")\n\np2 = plot(0:N,hcat(x0,xopts)',marker=:diamond,label=[\"x1\" \"x2\"],linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"x\")\n\nplot(p1,p2,layout=(2,1))\n\n\nThis constrained optimization problem can still be solved without invoking a numerical solver for solving quadratic programs (QP). We do it by introducing a vector \\boldsymbol\\lambda of Lagrange multipliers to form the Lagrangian function \n\\mathcal{L}(\\bar{\\bm z}, \\boldsymbol \\lambda) = \\frac{1}{2}\\bar{\\bm z}^\\top \\widetilde{\\mathbf Q} \\bar{\\bm z} + \\boldsymbol\\lambda^\\top(\\widetilde{\\mathbf A} \\bar{\\bm z} + \\tilde{\\mathbf b}),\n for which the gradients with respect to \\bar{\\bm z} and \\boldsymbol\\lambda are \n\\begin{aligned}\n\\nabla_{\\tilde{\\bm{z}}} \\mathcal{L}(\\bar{\\bm z}, \\boldsymbol\\lambda) &= \\widetilde{\\mathbf Q}\\bar{\\bm z} + \\tilde{\\mathbf A}^\\top\\boldsymbol\\lambda,\\\\\n\\nabla_{\\boldsymbol{\\lambda}} \\mathcal{L}(\\tilde{\\bm x}, \\boldsymbol\\lambda) &=\\widetilde{\\mathbf A} \\bar{\\bm z} + \\tilde{\\mathbf b}.\n\\end{aligned}\n\nRequiring that the overall gradient vanishes leads to the following KKT set of linear equations \n\\begin{bmatrix}\n  \\widetilde{\\mathbf Q} & \\widetilde{\\mathbf A}^\\top\\\\ \\widetilde{\\mathbf A} & \\mathbf 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bar{\\bm z}\\\\\\boldsymbol\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf 0\\\\ -\\tilde{\\mathbf b}\n\\end{bmatrix}.\n\nSolving this could be accomplished by using some general solver for linear systems or by using some more tailored solver for symmetric indefinite systems (based on LDL factorization, for example ldl in Matlab).\n\nAdding constraints on controls and states\nWhen solving a real optimal control problem, we may want to impose inequality constraints on \\bm u_k due to saturation of actuators. We may also want to add constraints on \\bm x_k as well, which may reflect some performance specifications. In both cases, the KKT system above would have to be augmented and we resort to some already finetuned numerical solver for quadratic programming (QP) instead.\n\n\n\nSequential (dense) formulation\nWe can express \\bar{\\bm x} as a function of \\bar{\\bm u} and \\mathbf x_0. This can be done in a straightforward way using (Equation 1), namely, \n\\bar{\\bm x} = (\\mathbf I-\\overline{\\mathbf A})^{-1}\\overline{\\mathbf B} \\bm u + (\\mathbf I-\\overline{\\mathbf A})^{-1} \\overline{\\mathbf A}_0 \\mathbf x_0.\n\nHowever, instead of solving the sets of equations, we can do this substitution in a more insightful way. Write down the state equation for several discrete times \n\\begin{aligned}\n\\bm x_1 &= \\mathbf A\\mathbf x_0 + \\mathbf B\\bm u_0\\\\\n\\bm x_2 &= \\mathbf A\\mathbf x_0 + \\mathbf B\\bm u_0\\\\\n     &= \\mathbf A(\\mathbf A\\mathbf x_0 + \\mathbf B\\bm u_0)+ \\mathbf B\\bm u_0\\\\\n     &= \\mathbf A^2\\mathbf x_0 + \\mathbf A\\mathbf B\\bm u_0 + \\mathbf B\\bm u_0\\\\\n     &\\vdots\\\\\n\\bm x_k &= \\mathbf A^k\\mathbf x_0 + \\mathbf A^{k-1}\\mathbf B\\bm u_0 +\\mathbf A^{k-2}\\mathbf B\\bm u_1 +\\ldots \\mathbf B\\bm u_{k-1}.\n\\end{aligned}\n\nRewriting into matrix-vector form (and extending the time k up to the final time N) \n\\begin{bmatrix}\n\\bm x_1\\\\\\bm x_2\\\\\\vdots\\\\\\bm x_N\n\\end{bmatrix}\n=\n\\underbrace{\n\\begin{bmatrix}\n  \\mathbf B & & & \\\\\n  \\mathbf A\\mathbf B & \\mathbf B & & \\\\\n  \\vdots & & \\ddots &\\\\\n  \\mathbf A^{N-1}\\mathbf B & \\mathbf A^{N-2}\\mathbf B & & \\mathbf B\n\\end{bmatrix}}_{\\widehat{\\mathbf C}}\n  \\begin{bmatrix}\n\\bm u_0\\\\\\bm u_1\\\\\\vdots\\\\\\bm u_{N-1}\n\\end{bmatrix}\n+\n\\underbrace{\n  \\begin{bmatrix}\n\\mathbf A\\\\\\mathbf A^2\\\\\\vdots\\\\\\mathbf A^N\n\\end{bmatrix}}_{\\widehat{\\mathbf A}}\\mathbf x_0\n\nFor convenience, let’s rewrite the compact relation between \\bar{\\bm x} and \\bar{\\bm u} and \\mathbf x_0 \n\\bar{\\bm x} = \\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\mathbf x_0.\n\\tag{2}\nWe can now substitute this into the original cost, which then becomes independent of \\bar{\\bm x}, which we reflect by using a new name \\tilde J \n\\begin{aligned}\n\\tilde J(\\bar{\\bm u};\\mathbf x_0) &= \\frac{1}{2}(\\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\mathbf x_0)^\\top\\overline{\\mathbf Q} (\\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\mathbf x_0) + \\frac{1}{2}\\bar{\\bm u}^\\top\\overline{\\mathbf R} \\bar{\\bm u} + \\frac{1}{2}\\mathbf x_0^\\top\\mathbf Q\\mathbf x_0\\\\\n&= \\frac{1}{2}\\bar{\\bm u}^\\top\\widehat{\\mathbf C}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} \\bar{\\bm u} + \\mathbf x_0^\\top\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} \\bar{\\bm u} + \\frac{1}{2} \\mathbf x_0^\\top\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf A} \\mathbf x_0 + \\frac{1}{2}\\bar{\\bm u}^\\top\\overline{\\mathbf R} \\bar{\\bm u} + \\frac{1}{2}\\mathbf x_0^\\top\\mathbf Q\\mathbf x_0\\\\\n&= \\frac{1}{2}\\bar{\\bm u}^\\top(\\widehat{\\mathbf C}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} + \\overline{\\mathbf R})\\bar{\\bm u} + \\mathbf x_0^\\top\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} \\bar{\\bm u} + \\frac{1}{2} \\mathbf x_0^\\top(\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf A} + \\mathbf Q)\\mathbf x_0.\n\\end{aligned}\n\nThe last term (the one independent of \\bar{\\bm u}) does not have an impact on the optimal \\bar{\\bm u} and therefore it can be discarded, but such minor modification perhaps does no justify a new name for the cost function and we write it as \n\\tilde J(\\bar{\\bm u};\\mathbf x_0) = \\frac{1}{2}\\bar{\\bm u}^\\top\\underbrace{(\\widehat{\\mathbf C}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} + \\overline{\\mathbf R})}_{\\mathbf H}\\bar{\\bm u} +  \\mathbf x_0^\\top\\underbrace{\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C}}_{\\mathbf F^\\top} \\bar{\\bm u}.\n\nThis cost is a function of \\bar{\\bm u}, the initial state \\mathbf x_0 is regarded as a fixed parameter. Its gradient is \n\\nabla \\tilde J = \\mathbf H\\bar{\\bm u}+\\mathbf F\\mathbf x_0.\n\nSetting it to zero leads to the following linear system of equations \n\\mathbf H\\bar{\\bm u}=-\\mathbf F\\mathbf x_0\n that needs to be solved for \\bar{\\bm u}. Formally, we write the solution as \n\\bar{\\bm u} = -\\mathbf H^{-1} \\mathbf F \\mathbf x_0.\n\n\n\n\n\n\n\nNote\n\n\n\nSolving linear equations by direct computation of the matrix inverse is not a recommended practice. Use dedicated solvers of linear equations instead. For example, in Matlab use the backslash operator, which invokes the most suitable solver.\n\n\n\nAdding the constraints on controls\nAdding constraints on \\bar{\\bm u} is straightforward. It is just that instead of a linear system we will have a linear system with additional inequality constraints. Let’s get one \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm u}} & \\quad \\frac{1}{2}\\bar{\\bm u}^T \\mathbf H \\bar{\\bm u} + \\mathbf x_0^T\\mathbf F^T \\bar{\\bm u}\\\\\n\\text{subject to} &\\quad \\bar{\\bm u} \\leq \\bar{\\mathbf u}_\\mathrm{max}\\\\\n               &\\quad \\bar{\\bm u} \\geq \\bar{\\mathbf u}_\\mathrm{min},\n\\end{aligned}\n which we can rewrite more explicitly (in the matrix-vector format) as \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm u}} & \\quad \\frac{1}{2}\\bar{\\bm u}^T \\mathbf H \\bar{\\bm u} + \\mathbf x_0^T\\mathbf F^T \\bar{\\bm u}\\\\\n\\text{subject to} & \\begin{bmatrix}\n                  \\mathbf{I}  &    &         &    \\\\\n                    & \\mathbf{I}  &         &    \\\\\n                    &    & \\ddots  &    \\\\\n                    &    &         &  \\mathbf{I} \\\\\n                    -\\mathbf{I}   &   &     &    \\\\\n                    & -\\mathbf{I} &         &    \\\\\n                    &    & \\ddots  &    \\\\\n                    &    &         &  -\\mathbf{I}\n                 \\end{bmatrix}\n                 \\begin{bmatrix}\n                  \\mathbf u_0 \\\\ \\mathbf u_1 \\\\ \\vdots \\\\ \\mathbf u_{N-1}\n                 \\end{bmatrix}\n                 \\leq\n                 \\begin{bmatrix}\n                  \\mathbf u_\\mathbf{max} \\\\ \\mathbf u_\\mathrm{max} \\\\ \\vdots \\\\ \\mathbf u_\\mathrm{max}\\\\ -\\mathbf u_\\mathrm{min} \\\\ -\\mathbf u_\\mathrm{min} \\\\ \\vdots \\\\ -\\mathbf u_\\mathrm{min}\n                 \\end{bmatrix}.\n\\end{aligned}\n\n\n\nAdding the constraints on states\nWe might feel a little bit uneasy about loosing an immediate access to \\bar{\\bm x}. But the game is not lost. We just need to express \\bar{\\bm x} as a function of \\bar{\\bm u} and \\mathbf x_0 and impose the constraint on the result. But such expression is already available, see (Equation 2). Therefore, we can formulate the constraint, say, an upper bound on the state vector \n\\bm x_k \\leq \\mathbf x_\\mathrm{max}\n as \n\\bar{\\mathbf x}_\\mathrm{min} \\leq \\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\mathbf x_0 \\leq \\bar{\\mathbf x}_\\mathrm{max},\n where the the bars in \\bar{\\mathbf x}_\\mathrm{min} and \\bar{\\mathbf x}_\\mathrm{max} obviously indicates that these vectors were obtained by stacking the corresponding vectors for all times k=1,\\ldots,N.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Finite-horizon LQR-optimal control as QP"
    ]
  }
]