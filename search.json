[
  {
    "objectID": "discr_dir_LQR.html",
    "href": "discr_dir_LQR.html",
    "title": "Finite-horizon LQR-optimal control as QP",
    "section": "",
    "text": "Here we specialize the general procedure from the previous section to the case of a linear system and a quadratic cost. we start by considering a simple problem of regulation, wherein the goal is to bring the system either exactly or approximately to zero final state, that is, \\bm r_N=\\mathbf 0 and we want \\bm x_N=\\mathbf r_N or \\bm x_N\\approx\\mathbf r_N, respectively. \n\\begin{aligned}\n\\operatorname*{minimize}_{\\mathbf u_0,\\ldots, \\mathbf u_{N-1}, \\mathbf x_{0},\\ldots, \\mathbf x_N} &\\quad  \\frac{1}{2} \\bm x_N^\\top \\mathbf S \\bm x_N + \\frac{1}{2} \\sum_{k=0}^{N-1} \\left(\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k \\right)\\\\\n\\text{subject to}   &\\quad \\bm x_{k+1} = \\mathbf A\\bm x_k + \\mathbf B\\bm u_k,\\quad k = 0, \\ldots, N-1, \\\\\n                    &\\quad \\bm x_0 = \\mathbf r_0,\\\\\n                    &\\quad \\bm x_N = \\mathbf 0\\;  (\\text{or}\\, \\bm x_N \\approx \\mathbf 0).\n\\end{aligned}\n\nReferring to the two options for the last constraint,\n\nif the condition \\bm x_N=\\mathbf 0 on the final state is strictly enforced, the terminal state cost (the term \\frac{1}{2} \\bm x_N^\\top \\mathbf S \\bm x_N in the cost function) is redundant and can be removed;\nif the final state condition can be relaxed to \\bm x_N\\approx\\mathbf 0, it is by increasing the weight \\mathbf S in the terminal cost function \\frac{1}{2} \\bm x_N^\\top \\mathbf S \\bm x_N that \\bm x_N can be made arbitrarily close to \\mathbf 0.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a standard dilemma in optimization, not only in optimal control, that if we want to satisfy some requirement, we can either strictly enforce it through constraints or we can seemingly relax it and set a cost to be paid for not satysfying it.\n\n\n\nSimultaneous (sparse) formulation\nBelow we rewrite the latter problem, that is, \\bm x_N\\approx\\mathbf 0, in the “unrolled” form, where we stack the state and control variables into “long” vectors \\bar{\\bm x} and \\bar{\\bm u}. Doing the same for the former is straightforward. \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm u},\\bar{\\bm x}} & \\frac{1}{2}\\left(\\begin{bmatrix} \\bm x_1^\\top & \\bm x_2^\\top & \\ldots & \\bm x_N^\\top \\end{bmatrix}\n\\underbrace{\\begin{bmatrix}\\mathbf Q & & & \\\\ & \\mathbf Q & &\\\\ & &\\ddots & \\\\ & & & \\mathbf S \\end{bmatrix}}_{\\overline{\\mathbf Q}}\n\\underbrace{\\begin{bmatrix} \\bm x_1 \\\\ \\bm x_2 \\\\ \\vdots \\\\ \\bm x_N \\end{bmatrix}}_{\\bar{\\bm x}}\\right.\\\\\n&\\qquad +\\left.\n\\begin{bmatrix} \\bm u_0^\\top & \\bm u_1^\\top & \\ldots & \\bm u_{N-1}^\\top \\end{bmatrix}\n\\underbrace{\\begin{bmatrix}\\mathbf R & & & \\\\ & \\mathbf R & &\\\\ & &\\ddots & \\\\ & & & \\mathbf R \\end{bmatrix}}_{\\overline{\\mathbf R}}\n\\underbrace{\\begin{bmatrix} \\bm u_0 \\\\ \\bm u_1 \\\\ \\vdots \\\\ \\bm u_{N-1} \\end{bmatrix}}_{\\bar{\\bm u}}\\right)\n+ \\underbrace{\\frac{1}{2}\\bm x_0^\\top \\mathbf Q \\bm x_0}_{\\mathrm{constant}}\n\\end{aligned}\n subject to \n\\begin{bmatrix} \\bm x_1 \\\\ \\bm x_2 \\\\ \\bm x_3\\\\ \\vdots \\\\ \\bm x_N \\end{bmatrix} = \\underbrace{\\begin{bmatrix}\\mathbf 0 & & & &\\\\\\mathbf A & \\mathbf 0 & & &\\\\ &\\mathbf A &\\mathbf 0 & & \\\\ & & &\\ddots & \\\\& & &\\mathbf A & \\mathbf 0 \\end{bmatrix}}_{\\overline{\\mathbf A}}\n\\begin{bmatrix} \\bm x_1 \\\\ \\bm x_2 \\\\ \\bm x_3\\\\ \\vdots \\\\ \\bm x_N \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf B & & & & \\\\ & \\mathbf B & & & \\\\& &\\mathbf B & \\\\ & & &\\ddots \\\\ & & & & \\mathbf B \\end{bmatrix}}_{\\overline{\\mathbf B}}\\begin{bmatrix} \\bm u_0 \\\\ \\bm u_1 \\\\ \\bm u_2\\\\\\vdots \\\\ \\bm u_{N-1} \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf A\\\\\\mathbf 0\\\\\\mathbf 0\\\\\\vdots\\\\\\mathbf 0\\end{bmatrix}}_{\\overline{\\mathbf A}_0}\\bm x_0.   \n\nNote that the last term in the cost function can be discarded because it is constant.\nThe terms with the \\bar{\\bm x} vector can be combined and we get \n\\begin{bmatrix} \\mathbf 0 \\\\ \\mathbf 0 \\\\ \\mathbf 0\\\\ \\vdots \\\\ \\mathbf 0 \\end{bmatrix} = \\underbrace{\\begin{bmatrix}-\\mathbf I & & & &\\\\\\mathbf A & -\\mathbf I & & &\\\\ &\\mathbf A &-\\mathbf I & & \\\\ & & &\\ddots & \\\\& & &\\mathbf A & -\\mathbf I \\end{bmatrix}}_{\\overline{\\mathbf A} - \\mathbf I}\n\\begin{bmatrix} \\mathbf x_1 \\\\ \\mathbf x_2 \\\\ \\mathbf x_3\\\\ \\vdots \\\\ \\mathbf x_N \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf B & & & & \\\\ & \\mathbf B & & & \\\\& &\\mathbf B & \\\\ & & &\\ddots \\\\ & & & & \\mathbf B \\end{bmatrix}}_{\\overline{\\mathbf B}}\\begin{bmatrix} \\mathbf u_0 \\\\ \\mathbf u_1 \\\\ \\mathbf u_2\\\\\\vdots \\\\ \\mathbf u_{N-1} \\end{bmatrix} + \\underbrace{\\begin{bmatrix}\\mathbf A\\\\\\mathbf 0\\\\\\mathbf 0\\\\\\vdots\\\\\\mathbf 0\\end{bmatrix}}_{\\overline{\\mathbf A}_0}\\bm x_0.\n\\tag{1}\nUpon stacking the two “long” vectors into \\bar{\\bm z} we reformulate the optimization problem as \n\\operatorname*{minimize}_{\\widetilde{\\mathbf z}\\in\\mathbb{R}^{2N}}\\quad \\frac{1}{2}\\underbrace{\\begin{bmatrix}\\bar{\\bm x}^\\top &\\bar{\\bm u}^\\top\\end{bmatrix}}_{\\bar{\\bm z}^\\top} \\underbrace{\\begin{bmatrix}\\overline{\\mathbf Q} & \\\\ & \\overline{\\mathbf R} \\end{bmatrix}}_{\\widetilde{\\mathbf Q}}\\underbrace{\\begin{bmatrix}\\bar{\\bm x}\\\\\\bar{\\bm u}\\end{bmatrix}}_{\\bar{\\bm z}}\n subject to \n\\mathbf 0 = \\underbrace{\\begin{bmatrix}(\\overline{\\mathbf A}-\\mathbf I) & \\overline{\\mathbf B}\\end{bmatrix}}_{\\widetilde{\\mathbf A}}\\underbrace{\\begin{bmatrix}\\bar{\\bm x}\\\\\\bar{\\bm u}\\end{bmatrix}}_{\\bar{\\bm z}} + \\underbrace{\\overline{\\mathbf A}_0 \\bm x_0}_{\\tilde{\\mathbf b}}.\n\nTo summarize, we have reformulated the optimal control problem as a linearly constrained quadratic program \n\\boxed{\n\\begin{aligned}\n\\underset{\\bar{\\bm z}\\in\\mathbb{R}^{2N}}{\\text{minimize}} &\\quad \\frac{1}{2}\\bar{\\bm z}^\\top \\widetilde{\\mathbf Q} \\bar{\\bm z}\\\\\n\\text{subject to} &\\quad \\widetilde{\\mathbf A} \\bar{\\bm z} + \\tilde{\\bm b} = \\mathbf 0.\n\\end{aligned}}\n\n\n\nCode\nfunction direct_dlqr_simultaneous(A,B,x₀,Q,R,S,N)\n    Qbar = BlockArray(spzeros(N*n,N*n),repeat([n],N),repeat([n],N))\n    for i=1:(N-1)\n        Qbar[Block(i,i)] = Q\n    end\n    Qbar[Block(N,N)] = S\n    Rbar = BlockArray(spzeros(N*m,N*m),repeat([m],N),repeat([m],N))\n    for i=1:N\n        Rbar[Block(i,i)] = R\n    end\n    Qtilde = blockdiag(sparse(Qbar),sparse(Rbar))                               # The matrix defining the quadratic cost.\n    Bbar = BlockArray(spzeros(N*n,N*m),repeat([n],N),repeat([m],N))\n    for i=1:N\n        Bbar[Block(i,i)] = B\n    end\n    Abar = BlockArray(sparse(-1.0*I,n*N,n*N),repeat([n],N),repeat([n],N))\n    for i=2:N\n        Abar[Block(i,(i-1))] = A\n    end\n    Atilde = sparse([Abar Bbar])                                                # The matrix defining the linear (affine) equation.\n    A0bar = spzeros(n*N,n)\n    A0bar[1:n,1:n] = A\n    btilde = A0bar*sparse(x₀)                                                   # The constant offset for the linear (affine) equation.\n    K = [Qtilde Atilde'; Atilde spzeros(size(Atilde,1),size(Atilde,1))]         # Sparse KKT matrix.\n    F = qdldl(K)                                                                # KKT matrix LDL factorization.\n    k = [spzeros(size(Atilde,1)); -btilde]                                      # Right hand side of the KKT system\n    xtildeλ = solve(F,k)                                                        # Solving the KKT system using the factorization.\n    xopt = reshape(xtildeλ[1:(n*N)],(n,:))\n    uopt = reshape(xtildeλ[(n*N+1):(n+m)*N+1],(m,:))\n    return xopt,uopt\nend\n\nn = 2               # Number of state variables.\nm = 1               # Number of (control) input variables. \nA = rand(n,n)       # State matrix.\nB = rand(n,m)       # Input coupling matrix.\nx₀ = [1.0, 3.0]     # Initial state.\n\nN = 10              # Time horizon.\n\ns = [1.0, 2.0]      \nq = [1.0, 2.0]\nr = [1.0]\n\nS = diagm(0=&gt;s)     # Matrix defining the terminal state cost.\nQ = diagm(0=&gt;q)     # Matrix defining the running state dost.\nR = diagm(0=&gt;r)     # Matrix defining the cost of control.\n\nuopts,xopts = direct_dlqr_simultaneous(A,B,x₀,Q,R,S,N)\n\nusing Plots\np1 = plot(0:(N-1),uopts,marker=:diamond,label=\"u\",linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"u\")\n\np2 = plot(0:N,hcat(x0,xopts)',marker=:diamond,label=[\"x1\" \"x2\"],linetype=:steppost)\nxlabel!(\"k\")\nylabel!(\"x\")\n\nplot(p1,p2,layout=(2,1))\n\n\nThis constrained optimization problem can still be solved without invoking a numerical solver for solving quadratic programs (QP). We do it by introducing a vector \\boldsymbol\\lambda of Lagrange multipliers to form the Lagrangian function \n\\mathcal{L}(\\bar{\\bm z}, \\boldsymbol \\lambda) = \\frac{1}{2}\\bar{\\bm z}^\\top \\widetilde{\\mathbf Q} \\bar{\\bm z} + \\boldsymbol\\lambda^\\top(\\widetilde{\\mathbf A} \\bar{\\bm z} + \\tilde{\\mathbf b}),\n for which the gradients with respect to \\bar{\\bm z} and \\boldsymbol\\lambda are \n\\begin{aligned}\n\\nabla_{\\tilde{\\bm{z}}} \\mathcal{L}(\\bar{\\bm z}, \\boldsymbol\\lambda) &= \\widetilde{\\mathbf Q}\\bar{\\bm z} + \\tilde{\\mathbf A}^\\top\\boldsymbol\\lambda,\\\\\n\\nabla_{\\boldsymbol{\\lambda}} \\mathcal{L}(\\tilde{\\bm x}, \\boldsymbol\\lambda) &=\\widetilde{\\mathbf A} \\bar{\\bm z} + \\tilde{\\mathbf b}.\n\\end{aligned}\n\nRequiring that the overall gradient vanishes leads to the following KKT set of linear equations \n\\begin{bmatrix}\n  \\widetilde{\\mathbf Q} & \\widetilde{\\mathbf A}^\\top\\\\ \\widetilde{\\mathbf A} & \\mathbf 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bar{\\bm z}\\\\\\boldsymbol\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf 0\\\\ -\\tilde{\\mathbf b}\n\\end{bmatrix}.\n\nSolving this could be accomplished by using some general solver for linear systems or by using some more tailored solver for symmetric indefinite systems (based on LDL factorization, for example ldl in Matlab).\n\nAdding constraints on controls and states\nWhen solving a real optimal control problem, we may want to impose inequality constraints on \\bm u_k due to saturation of actuators. We may also want to add constraints on \\bm x_k as well, which may reflect some performance specifications. In both cases, the KKT system above would have to be augmented and we resort to some already finetuned numerical solver for quadratic programming (QP) instead.\n\n\n\nSequential (dense) formulation\nWe can express \\bar{\\bm x} as a function of \\bar{\\bm u} and \\bm x_0. This can be done in a straightforward way using (Equation 1), namely, \n\\bar{\\bm x} = (\\mathbf I-\\overline{\\mathbf A})^{-1}\\overline{\\mathbf B} \\bm u + (\\mathbf I-\\overline{\\mathbf A})^{-1} \\overline{\\mathbf A}_0 \\bm x_0.\n\nHowever, instead of solving the sets of equations, we can do this substitution in a more insightful way. Write down the state equation for several discrete times \n\\begin{aligned}\n\\bm x_1 &= \\mathbf A\\bm x_0 + \\mathbf B\\bm u_0\\\\\n\\bm x_2 &= \\mathbf A\\bm x_0 + \\mathbf B\\bm u_0\\\\\n     &= \\mathbf A(\\mathbf A\\bm x_0 + \\mathbf B\\bm u_0)+ \\mathbf B\\bm u_0\\\\\n     &= \\mathbf A^2\\bm x_0 + \\mathbf A\\mathbf B\\bm u_0 + \\mathbf B\\bm u_0\\\\\n     &\\vdots\\\\\n\\bm x_k &= \\mathbf A^k\\bm x_0 + \\mathbf A^{k-1}\\mathbf B\\bm u_0 +\\mathbf A^{k-2}\\mathbf B\\bm u_1 +\\ldots \\mathbf B\\bm u_{k-1}.\n\\end{aligned}\n\nRewriting into matrix-vector form (and extending the time k up to the final time N) \n\\begin{bmatrix}\n\\bm x_1\\\\\\bm x_2\\\\\\vdots\\\\\\bm x_N\n\\end{bmatrix}\n=\n\\underbrace{\n\\begin{bmatrix}\n  \\mathbf B & & & \\\\\n  \\mathbf A\\mathbf B & \\mathbf B & & \\\\\n  \\vdots & & \\ddots &\\\\\n  \\mathbf A^{N-1}\\mathbf B & \\mathbf A^{N-2}\\mathbf B & & \\mathbf B\n\\end{bmatrix}}_{\\widehat{\\mathbf C}}\n  \\begin{bmatrix}\n\\bm u_0\\\\\\bm u_1\\\\\\vdots\\\\\\bm u_{N-1}\n\\end{bmatrix}\n+\n\\underbrace{\n  \\begin{bmatrix}\n\\mathbf A\\\\\\mathbf A^2\\\\\\vdots\\\\\\mathbf A^N\n\\end{bmatrix}}_{\\widehat{\\mathbf A}}\\bm x_0\n\nFor convenience, let’s rewrite the compact relation between \\bar{\\bm x} and \\bar{\\bm u} and \\bm x_0 \n\\bar{\\bm x} = \\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\bm x_0.\n\\tag{2}\nWe can now substitute this into the original cost, which then becomes independent of \\bar{\\bm x}, which we reflect by using a new name \\tilde J \n\\begin{aligned}\n\\tilde J(\\bar{\\bm u};\\bm x_0) &= \\frac{1}{2}(\\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\bm x_0)^\\top\\overline{\\mathbf Q} (\\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\bm x_0) + \\frac{1}{2}\\bar{\\bm u}^\\top\\overline{\\mathbf R} \\bar{\\bm u} + \\frac{1}{2}\\bm x_0^\\top\\mathbf Q\\bm x_0\\\\\n&= \\frac{1}{2}\\bar{\\bm u}^\\top\\widehat{\\mathbf C}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} \\bar{\\bm u} + \\bm x_0^\\top\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} \\bar{\\bm u} + \\frac{1}{2} \\bm x_0^\\top\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf A} \\bm x_0 + \\frac{1}{2}\\bar{\\bm u}^\\top\\overline{\\mathbf R} \\bar{\\bm u} + \\frac{1}{2}\\bm x_0^\\top\\mathbf Q\\bm x_0\\\\\n&= \\frac{1}{2}\\bar{\\bm u}^\\top(\\widehat{\\mathbf C}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} + \\overline{\\mathbf R})\\bar{\\bm u} + \\bm x_0^\\top\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} \\bar{\\bm u} + \\frac{1}{2} \\bm x_0^\\top(\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf A} + \\mathbf Q)\\bm x_0.\n\\end{aligned}\n\nThe last term (the one independent of \\bar{\\bm u}) does not have an impact on the optimal \\bar{\\bm u} and therefore it can be discarded, but such minor modification perhaps does no justify a new name for the cost function and we write it as \n\\tilde J(\\bar{\\bm u};\\bm x_0) = \\frac{1}{2}\\bar{\\bm u}^\\top\\underbrace{(\\widehat{\\mathbf C}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C} + \\overline{\\mathbf R})}_{\\mathbf H}\\bar{\\bm u} +  \\bm x_0^\\top\\underbrace{\\widehat{\\mathbf A}^\\top \\overline{\\mathbf Q} \\widehat{\\mathbf C}}_{\\mathbf F^\\top} \\bar{\\bm u}.\n\nThis cost is a function of \\bar{\\bm u}, the initial state \\bm x_0 is regarded as a fixed parameter. Its gradient is \n\\nabla \\tilde J = \\mathbf H\\bar{\\bm u}+\\mathbf F\\bm x_0.\n\nSetting it to zero leads to the following linear system of equations \n\\mathbf H\\bar{\\bm u}=-\\mathbf F\\bm x_0\n that needs to be solved for \\bar{\\bm u}. Formally, we write the solution as \n\\bar{\\bm u} = -\\mathbf H^{-1} \\mathbf F \\bm x_0.\n\n\n\n\n\n\n\nNote\n\n\n\nSolving linear equations by direct computation of the matrix inverse is not a recommended practice. Use dedicated solvers of linear equations instead. For example, in Matlab use the backslash operator, which invokes the most suitable solver.\n\n\n\nAdding the constraints on controls\nAdding constraints on \\bar{\\bm u} is straightforward. It is just that instead of a linear system we will have a linear system with additional inequality constraints. Let’s get one \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm u}} & \\quad \\frac{1}{2}\\bar{\\bm u}^T \\mathbf H \\bar{\\bm u} + \\bm x_0^T\\mathbf F^T \\bar{\\bm u}\\\\\n\\text{subject to} &\\quad \\bar{\\bm u} \\leq \\bar{\\mathbf u}_\\mathrm{max}\\\\\n               &\\quad \\bar{\\bm u} \\geq \\bar{\\mathbf u}_\\mathrm{min},\n\\end{aligned}\n which we can rewrite more explicitly (in the matrix-vector format) as \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm u}} & \\quad \\frac{1}{2}\\bar{\\bm u}^T \\mathbf H \\bar{\\bm u} + \\bm x_0^T\\mathbf F^T \\bar{\\bm u}\\\\\n\\text{subject to} & \\begin{bmatrix}\n                  \\mathbf{I}  &    &         &    \\\\\n                    & \\mathbf{I}  &         &    \\\\\n                    &    & \\ddots  &    \\\\\n                    &    &         &  \\mathbf{I} \\\\\n                    -\\mathbf{I}   &   &     &    \\\\\n                    & -\\mathbf{I} &         &    \\\\\n                    &    & \\ddots  &    \\\\\n                    &    &         &  -\\mathbf{I}\n                 \\end{bmatrix}\n                 \\begin{bmatrix}\n                  \\mathbf u_0 \\\\ \\mathbf u_1 \\\\ \\vdots \\\\ \\mathbf u_{N-1}\n                 \\end{bmatrix}\n                 \\leq\n                 \\begin{bmatrix}\n                  \\mathbf u_\\mathbf{max} \\\\ \\mathbf u_\\mathrm{max} \\\\ \\vdots \\\\ \\mathbf u_\\mathrm{max}\\\\ -\\mathbf u_\\mathrm{min} \\\\ -\\mathbf u_\\mathrm{min} \\\\ \\vdots \\\\ -\\mathbf u_\\mathrm{min}\n                 \\end{bmatrix}.\n\\end{aligned}\n\n\n\nAdding the constraints on states\nWe might feel a little bit uneasy about loosing an immediate access to \\bar{\\bm x}. But the game is not lost. We just need to express \\bar{\\bm x} as a function of \\bar{\\bm u} and \\bm x_0 and impose the constraint on the result. But such expression is already available, see (Equation 2). Therefore, we can formulate the constraint, say, an upper bound on the state vector \n\\bm x_k \\leq \\mathbf x_\\mathrm{max}\n as \n\\bar{\\mathbf x}_\\mathrm{min} \\leq \\widehat{\\mathbf C} \\bar{\\bm u} + \\widehat{\\mathbf A} \\bm x_0 \\leq \\bar{\\mathbf x}_\\mathrm{max},\n where the the bars in \\bar{\\mathbf x}_\\mathrm{min} and \\bar{\\mathbf x}_\\mathrm{max} obviously indicates that these vectors were obtained by stacking the corresponding vectors for all times k=1,\\ldots,N.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Finite-horizon LQR-optimal control as QP"
    ]
  },
  {
    "objectID": "cont_dir_shooting.html",
    "href": "cont_dir_shooting.html",
    "title": "Shooting method for optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "9. Continuous-time optimal control - direct approach via numerical optimization",
      "Shooting method for optimal control"
    ]
  },
  {
    "objectID": "LQG.html",
    "href": "LQG.html",
    "title": "LQG control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "LQG control"
    ]
  },
  {
    "objectID": "discr_dir_mpc_explicit.html",
    "href": "discr_dir_mpc_explicit.html",
    "title": "Explicit MPC",
    "section": "",
    "text": "(A. Bemporad, Borrelli, and Morari 2002), (Alberto Bemporad 2021), (Alessio and Bemporad 2009), (Borrelli, Bemporad, and Morari 2017)",
    "crumbs": [
      "6. More on MPC",
      "Explicit MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_explicit.html#literature",
    "href": "discr_dir_mpc_explicit.html#literature",
    "title": "Explicit MPC",
    "section": "",
    "text": "(A. Bemporad, Borrelli, and Morari 2002), (Alberto Bemporad 2021), (Alessio and Bemporad 2009), (Borrelli, Bemporad, and Morari 2017)",
    "crumbs": [
      "6. More on MPC",
      "Explicit MPC"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html",
    "href": "opt_theory_constrained.html",
    "title": "Theory for constrained optimization",
    "section": "",
    "text": "We consider the following optimization problem with equality constraints \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bm x) = \\mathbf 0,\n\\end{aligned}\n where \\mathbf h(\\bm x) \\in \\mathbb R^m defines a set of m equations \n\\begin{aligned}\nh_1(\\bm x) &= 0\\\\\nh_2(\\bm x) &= 0\\\\\n\\vdots\\\\\nh_m(\\bm x) &= 0.\n\\end{aligned}\n\nAugmenting the original cost function f with the constraint functions h_i scaled by Lagrange variables \\lambda_i gives the Lagrangian function \n\\mathcal{L}(\\bm x,\\boldsymbol\\lambda) \\coloneqq f(\\bm x) + \\sum_{i=1}^m \\lambda_i h_i(\\bm x) = f(\\bm x) + \\boldsymbol \\lambda^\\top \\mathbf h(\\bm x).\n\n\n\n\n\nThe first-order necessary condition of optimality is \n\\nabla \\mathcal{L}(\\bm x,\\boldsymbol\\lambda) = \\mathbf 0,\n which amounts to two (generally vector) equations \n\\boxed{\n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\bm x) &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nDefining a matrix \\nabla \\mathbf h(\\bm x) \\in \\mathbb R^{n\\times m} as horizontally stacked gradients of the constraint functions \n\\nabla \\mathbf h(\\bm x) \\coloneqq \\begin{bmatrix}\n                                 \\nabla h_1(\\bm x) && \\nabla h_2(\\bm x) && \\ldots && \\nabla h_m(\\bm x)\n                            \\end{bmatrix},\n in fact, a transpose of the Jacobian matrix, the necessary condition can be rewritten in a vector form as \\boxed\n{\\begin{aligned}\n\\nabla f(\\bm x) + \\nabla \\mathbf h(\\bm x)\\boldsymbol \\lambda &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nBeware of the nonregularity issue! The (\\nabla \\mathbf h(\\bm x))^\\mathrm T is regular at a given \\bm x (the \\bm x is a regular point) if it has a full column rank. Rank-deficiency reveals a defect in formulation.\n\nExample 1 (Equality-constrained quadratic program) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} &\\quad \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{r}^\\top\\bm{x}\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x + \\mathbf b = \\mathbf 0.\n\\end{aligned}\n\nThe first-order necessary condition of optimality is\n\n\\begin{bmatrix}\n  \\mathbf Q & \\mathbf A^\\top\\\\\\mathbf A & \\mathbf 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\bm x \\\\ \\boldsymbol \\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  -\\mathbf r\\\\\\mathbf b\n\\end{bmatrix}.\n\n\n\n\n\n\nUsing the unconstrained Hessian \\nabla^2_{\\mathbf{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda) is too conservative. Instead, use projected Hessian\n\n\\mathbf{Z}^\\mathrm{T}\\;\\nabla^2_{\\bm{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda)\\;\\mathbf Z &gt; 0,\n where \\mathbf Z is an (orthonormal) basis of the nullspace of the Jacobian (\\nabla \\mathbf h(\\bm x))^\\top.",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#equality-constraints",
    "href": "opt_theory_constrained.html#equality-constraints",
    "title": "Theory for constrained optimization",
    "section": "",
    "text": "We consider the following optimization problem with equality constraints \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bm x) = \\mathbf 0,\n\\end{aligned}\n where \\mathbf h(\\bm x) \\in \\mathbb R^m defines a set of m equations \n\\begin{aligned}\nh_1(\\bm x) &= 0\\\\\nh_2(\\bm x) &= 0\\\\\n\\vdots\\\\\nh_m(\\bm x) &= 0.\n\\end{aligned}\n\nAugmenting the original cost function f with the constraint functions h_i scaled by Lagrange variables \\lambda_i gives the Lagrangian function \n\\mathcal{L}(\\bm x,\\boldsymbol\\lambda) \\coloneqq f(\\bm x) + \\sum_{i=1}^m \\lambda_i h_i(\\bm x) = f(\\bm x) + \\boldsymbol \\lambda^\\top \\mathbf h(\\bm x).\n\n\n\n\n\nThe first-order necessary condition of optimality is \n\\nabla \\mathcal{L}(\\bm x,\\boldsymbol\\lambda) = \\mathbf 0,\n which amounts to two (generally vector) equations \n\\boxed{\n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\bm x) &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nDefining a matrix \\nabla \\mathbf h(\\bm x) \\in \\mathbb R^{n\\times m} as horizontally stacked gradients of the constraint functions \n\\nabla \\mathbf h(\\bm x) \\coloneqq \\begin{bmatrix}\n                                 \\nabla h_1(\\bm x) && \\nabla h_2(\\bm x) && \\ldots && \\nabla h_m(\\bm x)\n                            \\end{bmatrix},\n in fact, a transpose of the Jacobian matrix, the necessary condition can be rewritten in a vector form as \\boxed\n{\\begin{aligned}\n\\nabla f(\\bm x) + \\nabla \\mathbf h(\\bm x)\\boldsymbol \\lambda &= \\mathbf 0\\\\\n\\mathbf{h}(\\bm x) &= \\mathbf 0.\n\\end{aligned}}\n\nBeware of the nonregularity issue! The (\\nabla \\mathbf h(\\bm x))^\\mathrm T is regular at a given \\bm x (the \\bm x is a regular point) if it has a full column rank. Rank-deficiency reveals a defect in formulation.\n\nExample 1 (Equality-constrained quadratic program) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} &\\quad \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{r}^\\top\\bm{x}\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x + \\mathbf b = \\mathbf 0.\n\\end{aligned}\n\nThe first-order necessary condition of optimality is\n\n\\begin{bmatrix}\n  \\mathbf Q & \\mathbf A^\\top\\\\\\mathbf A & \\mathbf 0\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\bm x \\\\ \\boldsymbol \\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  -\\mathbf r\\\\\\mathbf b\n\\end{bmatrix}.\n\n\n\n\n\n\nUsing the unconstrained Hessian \\nabla^2_{\\mathbf{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda) is too conservative. Instead, use projected Hessian\n\n\\mathbf{Z}^\\mathrm{T}\\;\\nabla^2_{\\bm{x}\\bm{x}} \\mathcal{L}(\\bm x,\\boldsymbol \\lambda)\\;\\mathbf Z &gt; 0,\n where \\mathbf Z is an (orthonormal) basis of the nullspace of the Jacobian (\\nabla \\mathbf h(\\bm x))^\\top.",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#inequality-constraints",
    "href": "opt_theory_constrained.html#inequality-constraints",
    "title": "Theory for constrained optimization",
    "section": "Inequality constraints",
    "text": "Inequality constraints\n\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf g(\\bm x) \\leq \\mathbf 0,\n\\end{aligned}\n where \\mathbf g(\\bm x) \\in \\mathbb R^p defines a set of p inequalities.\n\nFirst-order necessary condition of optimality\nKarush-Kuhn-Tucker (KKT) conditions of optimality are then composed of these four (sets of) conditions \n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^p \\mu_i \\nabla g_i(\\bm x) &= \\mathbf 0,\\\\\n\\mathbf{g}(\\bm{x}) &\\leq \\mathbf 0,\\\\\n\\mu_i g_i(\\bm x) &= 0,\\quad i = 1,2,\\ldots, m\\\\\n\\mu_i &\\geq 0,\\quad   i = 1,2,\\ldots, m.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#equality-and-inequality-constraints",
    "href": "opt_theory_constrained.html#equality-and-inequality-constraints",
    "title": "Theory for constrained optimization",
    "section": "Equality and inequality constraints",
    "text": "Equality and inequality constraints\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x\\in\\mathbb{R}^n} &\\quad f(\\bm x)\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bm x) = \\mathbf 0,\\\\\n                    &\\quad \\mathbf g(\\bm x) \\leq \\mathbf 0.\n\\end{aligned}\n\n\nFirst-order necessary condition of optimality\nThe KKT conditions\n\n\\begin{aligned}\n\\nabla f(\\bm x) + \\sum_{i=1}^m \\lambda_i \\nabla h_i(\\bm x) + \\sum_{i=1}^p \\mu_i \\nabla g_i(\\bm x) &= \\mathbf 0\\\\\n\\mathbf{h}(\\mathbf{x}) &= \\mathbf 0\\\\\n\\mathbf{g}(\\mathbf{x}) &\\leq \\mathbf 0\\\\\n\\mu_i g_i(\\bm x) &= 0,\\quad i = 1,\\ldots, m\\\\\n\\mu_i &\\geq 0,\\quad   i = 1,\\ldots, m.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for constrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_constrained.html#duality",
    "href": "opt_theory_constrained.html#duality",
    "title": "Theory for constrained optimization",
    "section": "Duality",
    "text": "Duality\nDuality theory offers another view of the original optimization problem by bringing in another but related one.\nCorresponding to the general optimization problem \n  \\begin{aligned}\n  \\operatorname*{minimize}\\;&f(\\bm x)\\\\\n  \\text{subject to}\\; & \\mathbf g(\\bm x)\\leq \\mathbf 0\\\\\n  & \\mathbf h(\\bm x) = \\mathbf 0,\n  \\end{aligned}\n\nwe form the Lagrangian function \\mathcal L(\\bm x,\\bm \\lambda,\\bm \\mu) = f(\\bm x) + \\bm \\lambda^\\top \\mathbf h(\\bm x) + \\bm \\mu^\\top \\mathbf g(\\bm x)\nFor any (fixed) values of (\\bm \\lambda,\\bm \\mu) such that \\bm \\mu\\geq 0, we define the Lagrange dual function through the following unconstrained optimization problem \nq(\\bm\\lambda,\\bm\\mu) = \\inf_{\\bm x}\\mathcal L(\\bm x,\\bm \\lambda,\\bm \\mu).\n\nObviously, it is alway possible to pick a feasible solution \\bm x, in which case the value of the Lagrangian and the original function coincide, and so the result of this minimization is no worse (larger) than the minimum for the original optimization problem. It can thus serve as a lower bound q(\\bm \\lambda,\\bm \\mu) \\leq f(\\bm x^\\star).\nThis result is called weak duality. A natural idea is to find the values of \\bm \\lambda and \\bm \\mu such that this lower bound is tightest, that is, \n\\begin{aligned}\n  \\operatorname*{maximize}_{\\bm\\lambda, \\bm\\mu}\\; & q(\\bm\\lambda,\\bm\\mu)\\\\\n  \\text{subject to}\\;& \\bm\\mu \\geq \\mathbf 0.\n\\end{aligned}\n\nUnder some circumstances the result can be tight, which leads to strong duality, which means that the minimum of the original (primal) problem and the maximum of the dual problem coincide. \nq(\\bm \\lambda^\\star,\\bm \\mu^\\star) = f(\\bm x^\\star).\n\nThis related dual optimization problem can have some advantages for development of both theory and algorithms.",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for constrained optimization"
    ]
  },
  {
    "objectID": "cont_indir_via_calculus_of_variations.html",
    "href": "cont_indir_via_calculus_of_variations.html",
    "title": "Indirect approach to optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Indirect approach to optimal control"
    ]
  },
  {
    "objectID": "rocond_mixed_sensitivity.html",
    "href": "rocond_mixed_sensitivity.html",
    "title": "Mixed sensitivity design",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "12. Robust control",
      "Mixed sensitivity design"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html",
    "href": "opt_theory_modellers.html",
    "title": "Optimization modelling languages",
    "section": "",
    "text": "Realistically complex optimization problems cannot be solved with just a pen and a paper – computer programs (often called optimization solvers) are needed to solve them. And now comes the challenge: as various solvers for even the same class of problems differ in the algorithms they implement, so do their interfaces – every solver expects the inputs (the data defining the optimization problem) in a specific format. This makes it difficult to switch between solvers, as the problem data has to be reformatted every time.\n\nExample 1 (Data formatting for different solvers) Consider the following optimization problem: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix} \\leq \\begin{bmatrix} 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix} \\bm x \\leq  \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7\\end{bmatrix}\n\\end{aligned}\n\nThere are dozens of solvers that can be used to solve this problem. Here we demonstrate a usage of these two: OSQP and COSMO.jl. And we are going to call the solvers in Julia (using the the wrappers OSQP.jl for the former). First, we start with OSQP (in fact, this is their example):\n\n\nShow the code\nusing OSQP\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nproblem_OSQP = OSQP.Model()\nOSQP.setup!(problem_OSQP; P=P, q=q, A=A, l=l, u=u, alpha=1, verbose=false)\n\n# Solve the optimization problem and show the results\nresults_OSQP = OSQP.solve!(problem_OSQP)\nresults_OSQP.x\n\n\nNow we do the same with COSMO. First, we must take into account that COSMO cannot accept two-sided inequalities, so we have to reformulate the problem so that the constraints are only in the form of \\mathbf A\\bm x + b \\geq \\bm 0: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix} -1 & -1\\\\ -1 & 0\\\\ 0 & -1\\\\ 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix}\\bm x + \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7 \\\\ -1 \\\\ 0 \\\\ 0\\end{bmatrix} \\geq  \\mathbf 0.\n\\end{aligned}\n\n\n\nShow the code\nusing COSMO\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nAa = [-A; A]\nba = [u; -l]\n\nproblem_COSMO = COSMO.Model()\nconstraint = COSMO.Constraint(Aa, ba, COSMO.Nonnegatives)\nsettings = COSMO.Settings(verbose=false)\nassemble!(problem_COSMO, P, q, constraint, settings = settings)\n\n# Solve the optimization problem and show the results\nresults_COSMO = COSMO.optimize!(problem_COSMO)\nresults_COSMO.x\n\n\nAlthough the two solvers are solving the same problem, the data has to be formatted differently for each of them (and the difference in syntax is not negligible either).\nWhat if we could formulate the same problem without considering the pecualiarities of each solver? It turns out that it is possible. In Julia we can use JuMP.jl:\n\n\nShow the code\nusing JuMP\nusing SparseArrays\nusing OSQP, COSMO\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nmodel_JuMP = Model()\n@variable(model_JuMP, x[1:2])\n@objective(model_JuMP, Min, 0.5*x'*P*x + q'*x)\n@constraint(model_JuMP, A*x .&lt;= u)\n@constraint(model_JuMP, A*x .&gt;= l)\n\n# Solve the optimization problem using OSQP and show the results\nset_silent(model_JuMP)\nset_optimizer(model_JuMP, OSQP.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_OSQP = value.(x)\n\n# Now solve the problem using COSMO and show the results\nset_optimizer(model_JuMP, COSMO.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_COSMO = value.(x)\n\n\n\nNotice how the optimization problem is defined just once in the last code and then different solvers can be chosen to solve it. The code represents an instance of a so-called optimization modelling language (OML), or actually its major class called algebraic modelling language (AML).\nThe key motivation for using an OML/AML is to separate the process of formulating the problem from the process of solving it (using a particular solver). Furthermore, such solver-independent problem description (called optimization model) better mimics the way we formulate these problems using a pen and a paper, making it (perhaps) a bit more convenient to write our own and read someone else’s models.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#why-optimization-modelling-languages",
    "href": "opt_theory_modellers.html#why-optimization-modelling-languages",
    "title": "Optimization modelling languages",
    "section": "",
    "text": "Realistically complex optimization problems cannot be solved with just a pen and a paper – computer programs (often called optimization solvers) are needed to solve them. And now comes the challenge: as various solvers for even the same class of problems differ in the algorithms they implement, so do their interfaces – every solver expects the inputs (the data defining the optimization problem) in a specific format. This makes it difficult to switch between solvers, as the problem data has to be reformatted every time.\n\nExample 1 (Data formatting for different solvers) Consider the following optimization problem: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\end{bmatrix} \\leq \\begin{bmatrix} 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix} \\bm x \\leq  \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7\\end{bmatrix}\n\\end{aligned}\n\nThere are dozens of solvers that can be used to solve this problem. Here we demonstrate a usage of these two: OSQP and COSMO.jl. And we are going to call the solvers in Julia (using the the wrappers OSQP.jl for the former). First, we start with OSQP (in fact, this is their example):\n\n\nShow the code\nusing OSQP\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nproblem_OSQP = OSQP.Model()\nOSQP.setup!(problem_OSQP; P=P, q=q, A=A, l=l, u=u, alpha=1, verbose=false)\n\n# Solve the optimization problem and show the results\nresults_OSQP = OSQP.solve!(problem_OSQP)\nresults_OSQP.x\n\n\nNow we do the same with COSMO. First, we must take into account that COSMO cannot accept two-sided inequalities, so we have to reformulate the problem so that the constraints are only in the form of \\mathbf A\\bm x + b \\geq \\bm 0: \n\\begin{aligned}\n  \\operatorname*{minimize}_{\\bm x \\in \\mathbb R^2} & \\quad \\frac{1}{2} \\bm x^\\top \\begin{bmatrix}4 & 1\\\\ 1 & 2 \\end{bmatrix} \\bm x + \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}^\\top \\bm x \\\\\n  \\text{subject to} & \\quad \\begin{bmatrix} -1 & -1\\\\ -1 & 0\\\\ 0 & -1\\\\ 1 & 1\\\\ 1 & 0\\\\ 0 & 1\\end{bmatrix}\\bm x + \\begin{bmatrix}1 \\\\ 0.7 \\\\ 0.7 \\\\ -1 \\\\ 0 \\\\ 0\\end{bmatrix} \\geq  \\mathbf 0.\n\\end{aligned}\n\n\n\nShow the code\nusing COSMO\nusing SparseArrays\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nAa = [-A; A]\nba = [u; -l]\n\nproblem_COSMO = COSMO.Model()\nconstraint = COSMO.Constraint(Aa, ba, COSMO.Nonnegatives)\nsettings = COSMO.Settings(verbose=false)\nassemble!(problem_COSMO, P, q, constraint, settings = settings)\n\n# Solve the optimization problem and show the results\nresults_COSMO = COSMO.optimize!(problem_COSMO)\nresults_COSMO.x\n\n\nAlthough the two solvers are solving the same problem, the data has to be formatted differently for each of them (and the difference in syntax is not negligible either).\nWhat if we could formulate the same problem without considering the pecualiarities of each solver? It turns out that it is possible. In Julia we can use JuMP.jl:\n\n\nShow the code\nusing JuMP\nusing SparseArrays\nusing OSQP, COSMO\n\n# Define the problem data and build the problem description\nP = sparse([4.0 1.0; 1.0 2.0])\nq = [1.0; 1.0]\nA = sparse([1.0 1.0; 1.0 0.0; 0.0 1.0])\nl = [1.0; 0.0; 0.0]\nu = [1.0; 0.7; 0.7]\n\nmodel_JuMP = Model()\n@variable(model_JuMP, x[1:2])\n@objective(model_JuMP, Min, 0.5*x'*P*x + q'*x)\n@constraint(model_JuMP, A*x .&lt;= u)\n@constraint(model_JuMP, A*x .&gt;= l)\n\n# Solve the optimization problem using OSQP and show the results\nset_silent(model_JuMP)\nset_optimizer(model_JuMP, OSQP.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_OSQP = value.(x)\n\n# Now solve the problem using COSMO and show the results\nset_optimizer(model_JuMP, COSMO.Optimizer)\noptimize!(model_JuMP)\ntermination_status(model_JuMP)\nx_COSMO = value.(x)\n\n\n\nNotice how the optimization problem is defined just once in the last code and then different solvers can be chosen to solve it. The code represents an instance of a so-called optimization modelling language (OML), or actually its major class called algebraic modelling language (AML).\nThe key motivation for using an OML/AML is to separate the process of formulating the problem from the process of solving it (using a particular solver). Furthermore, such solver-independent problem description (called optimization model) better mimics the way we formulate these problems using a pen and a paper, making it (perhaps) a bit more convenient to write our own and read someone else’s models.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#why-not-optimization-modelling-languages",
    "href": "opt_theory_modellers.html#why-not-optimization-modelling-languages",
    "title": "Optimization modelling languages",
    "section": "Why not optimization modelling languages?",
    "text": "Why not optimization modelling languages?\nAs a matter of fact, some optimization experts even keep avoiding OML/AML altogether. For example, if a company pays for a (not really cheap) license of Gurobi Optimizer – a powerful optimization library for (MI)LP/QP/QCQP –, it may be the case that for a particular very large-scale optimization problem their optimization specialist will have hard time to find a third-party solver of comparable performance. If then its Python API makes definition of optimization problems convenient too (see the code below), maybe there is little regret that such problem definitions cannot be reused with a third-party solver. The more so that since it is tailored to Gurobi solver, it will offer control over the finest details.\n\n\nShow the code\nimport gurobipy as gp\nimport numpy as np\n\n# Define the data for the model\nP = np.array([[4.0, 1.0], [1.0, 2.0]])\nq = np.array([1.0, 1.0])\nA = np.array([[1.0, 1.0], [1.0, 0.0], [0.0, 1.0]])\nl = np.array([1.0, 0.0, 0.0])\nu = np.array([1.0, 0.7, 0.7])\n\n# Create a new model\nm = gp.Model(\"qp\")\n\n# Create a vector variable\nx = m.addMVar((2,))\n\n# Set the objective\nobj = 1/2*(x@P@x + q@x)\nm.setObjective(obj)\n\n# Add the constraints\nm.addConstr(A@x &gt;= l, \"c1\")\nm.addConstr(A@x &lt;= u, \"c2\")\n\n# Run the solver\nm.optimize()\n\n# Print the results\nfor v in m.getVars():\n    print(f\"{v.VarName} {v.X:g}\")\n\nprint(f\"Obj: {m.ObjVal:g}\")\n\n\nSimilar and yet different is the story of the IBM ILOG CPLEX, another top-notch solvers addressing the same problems as Gurobi. They do have their own modeling language called Optimization Modelling Language (OPL), but it is also only interfacing with their solver(s). We can only guess that their motivation for developing their own optimization modelling language was that at the time of its developments (in 1990s) Python was still in pre-2.0 stage and formulating optimization problems in programming languages like C/C++ or Fortran was nowhere close to being convenient. Gurobi, in turn, started in 2008, when Python was already a popular language.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#language-independent-optimization-modelling-languages",
    "href": "opt_theory_modellers.html#language-independent-optimization-modelling-languages",
    "title": "Optimization modelling languages",
    "section": "Language-independent optimization modelling languages",
    "text": "Language-independent optimization modelling languages\nOptimization/algebraic modelling languages were originally developed outside programming languages, essentially as standalone tools. Examples are AMPL, GAMS, and, say, GLPK/GMPL (MathProg). We listed these main names here since they can be bumped across (they are still actively developed), but we are not going to discuss them in our course any further. The reason is that there are now alternatives that are implemented as packages/toolboxes in programming languages such as Julia, Matlab, and Python, which offer a more fluent workflow – a user can use the same programming language to acquire the data, preprocess them, formulate the optimization problem, configure and call a solver, and finally do some postprocessing including a visualization and whatever reporting, all without leaving the language of their choice.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#optimization-modelling-in-julia",
    "href": "opt_theory_modellers.html#optimization-modelling-in-julia",
    "title": "Optimization modelling languages",
    "section": "Optimization modelling in Julia",
    "text": "Optimization modelling in Julia\nMy obvious (personal) bias towards Julia programming language is partly due to the terrific support for optimization modelling in Julia:\n\nJuMP.jl not only constitutes one of the flagship packages of the Julia ecosystem but it is on par with the state of the art optimization modelling languages. Furthermore, being a free and open source software, it enjoys a vibrant community of developers and users. They even meet annually at JuMP-dev conference (in 2023 in Boston, MA).\nConvex.jl is an implementation of the concept of Disciplined Convex Programming (DCP) in Julia (below we also list its implementations in Matlab and Python). Even though it is now registered as a part of the JuMP.jl project, it is still a separate concept. Interesting, convenient, but it seems to be in a maintanence mode now.\n\n\n\nShow the code\nusing Convex, SCS\n\n# Define the problem data and build the problem description\nP = [4.0 1.0; 1.0 2.0]\nq = [1.0, 1.0]\nA = [1.0 1.0; 1.0 0.0; 0.0 1.0]\nl = [1.0, 0.0, 0.0]\nu = [1.0, 0.7, 0.7]\n\n# Create a vector variable of size n\nx = Variable(2)\n\n# Define the objective \nobjective = 1/2*quadform(x,P) + dot(q,x)\n\n# Define the constraints\nconstraints = [l &lt;= A*x, A*x &lt;= u]\n\n# Define the overal description of the optimization problem\nproblem = minimize(objective, constraints)\n\n# Solve the problem\nsolve!(problem, SCS.Optimizer; silent_solver = true)\n\n# Check the status of the problem\nproblem.status # :Optimal, :Infeasible, :Unbounded etc.\n\n# Get the optimum value\nproblem.optval\n\n# Get the optimal x\nx.value",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#optimization-modelling-in-matlab",
    "href": "opt_theory_modellers.html#optimization-modelling-in-matlab",
    "title": "Optimization modelling languages",
    "section": "Optimization modelling in Matlab",
    "text": "Optimization modelling in Matlab\nPopularity of Matlab as a language and an ecosystem for control-related computations is undeniable. Therefore, let’s have a look at what is available for modelling optimization problems in Matlab:\n\nOptimization Toolbox for Matlab is one of the commercial toolboxes produced by Matlab and Simulink creators. Since the R2017b release the toolbox supports Problem-based optimization workflow (besides the more traditional Solver-based optimization workflow supported since the beginning), which can be regarded as a kind of an optimization/algebraic modelling language, albeit restricted to their own solvers.\nYalmip started as Yet Another LMI Parser quite some time ago (which reveals its control theoretic roots), but these days it serves as fairly complete algebraic modelling language (within Matlab), interfacing to perhaps any optimization solver, both commercial and free&open-source. It is free and open-source. Is is still actively developed and maintained and it abounds with tutorials and examples.\nCVX is a Matlab counterpart of Convex.jl (or the other way around, if you like, since it has been here longer). The name stipulates that it only allows convex optimization probles (unlike Yalmip) – it follows the Disciplined Convex Programming (DCP) paradigm. Unfortunately, the development seems to have stalled – the last update is from 2020.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "opt_theory_modellers.html#optimization-modelling-in-python",
    "href": "opt_theory_modellers.html#optimization-modelling-in-python",
    "title": "Optimization modelling languages",
    "section": "Optimization modelling in Python",
    "text": "Optimization modelling in Python\nPython is a very popular language for scientific computing. Although it is arguable if it is actually suitable for implementation of numerical algoritms, when it comes to building optimization models, it does its job fairly well (and the numerical solvers it calls can be developed in different language). Several packages implementing OML/AML are available:\n\ncvxpy is yet another instantiation of Disciplined Convex Programming that we alredy mention when introducing Convex.jl and CVX. And it turns out that this one exhibits the greatest momentum. The team of developers seems to be have exceeded a critical mass, hence the tools seems like a safe bet already.\nPyomo is a popular open-source optimization modelling language within Python.\nAPMonitor and GEKKO are relatively young projects, primarily motivated by applications of machine learning and optimization in chemical process engineering.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization modelling languages"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html",
    "href": "discr_dir_mpc.html",
    "title": "Model predictive control (MPC)",
    "section": "",
    "text": "In the previous section we learnt how to compute an optimal control sequence on a finite time horizon using numerical methods for solving nonlinear programs (NLP), and quadratic programs (QP) in particular. There are two major deficiencies of such approach:\n\nThe control sequence was computed under the assumption that the mathematical model is perfectly accurate. As soon as the reality deviates from the model, either because of some unmodelled dynamics or because of the presence of (external) disturbances, the performance of the system will deteriorate. We need a way to turn the presented open-loop (also feedforward) control scheme into a feedback one.\nThe control sequence was computed for a finite time horizon. It is commonly required to consider an infinite time horizon, which is not possible with the presented approach based on solving finite-dimensional mathematical programs.\n\nThere are several ways to address these issues. Here we introduced one of them. It is knowns are Model Predictive Control (MPC), or also Receding Horizon Control (RHC). Some more are presented in the next two sections (one based on indirect approach, another one based on dynamic programming).",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#deficiencies-of-precomputed-open-loop-optimal-control",
    "href": "discr_dir_mpc.html#deficiencies-of-precomputed-open-loop-optimal-control",
    "title": "Model predictive control (MPC)",
    "section": "",
    "text": "In the previous section we learnt how to compute an optimal control sequence on a finite time horizon using numerical methods for solving nonlinear programs (NLP), and quadratic programs (QP) in particular. There are two major deficiencies of such approach:\n\nThe control sequence was computed under the assumption that the mathematical model is perfectly accurate. As soon as the reality deviates from the model, either because of some unmodelled dynamics or because of the presence of (external) disturbances, the performance of the system will deteriorate. We need a way to turn the presented open-loop (also feedforward) control scheme into a feedback one.\nThe control sequence was computed for a finite time horizon. It is commonly required to consider an infinite time horizon, which is not possible with the presented approach based on solving finite-dimensional mathematical programs.\n\nThere are several ways to address these issues. Here we introduced one of them. It is knowns are Model Predictive Control (MPC), or also Receding Horizon Control (RHC). Some more are presented in the next two sections (one based on indirect approach, another one based on dynamic programming).",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#model-predictive-control-mpc-as-a-way-to-turn-open-loop-control-into-feedback-control",
    "href": "discr_dir_mpc.html#model-predictive-control-mpc-as-a-way-to-turn-open-loop-control-into-feedback-control",
    "title": "Model predictive control (MPC)",
    "section": "Model predictive control (MPC) as a way to turn open-loop control into feedback control",
    "text": "Model predictive control (MPC) as a way to turn open-loop control into feedback control\nThe idea is to compute an optimal control sequence on a finite time horizon using the material presented in the previous section, apply only the first control action to the system, and then repeat the procedure upon shifting the time horizon by one time step.\nAlthough this name “model predictive control” is commonly used in the control community, the other – perhaps a bit less popular – name “receding horizon control” is equally descriptive, if not even a bit more.\n\n\n\n\n\n\nNote\n\n\n\nIt may take a few moments to digest the idea, but it is actually quite natural. As a matter of fact, this is the way most of us control our lifes every day. We plan our actions on a finite time horizon, and while building this plan we use our understanding (model) of the world. We then perform the first action from our plan, observe the impact of our action and possibly a change in the environment, and update our plan accordingly on a new (shifted) time horizon. We repeat this procedure over and over again. It is crucial that the prediction horizon must be long enough so that the full impact of our actions can be observed.",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#linear-vs-nonlinear-mpc",
    "href": "discr_dir_mpc.html#linear-vs-nonlinear-mpc",
    "title": "Model predictive control (MPC)",
    "section": "Linear vs nonlinear MPC",
    "text": "Linear vs nonlinear MPC\nx",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#prediction-horizon-vs-control",
    "href": "discr_dir_mpc.html#prediction-horizon-vs-control",
    "title": "Model predictive control (MPC)",
    "section": "Prediction horizon vs control",
    "text": "Prediction horizon vs control\nx",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#hard-constraints-vs-soft-constraints",
    "href": "discr_dir_mpc.html#hard-constraints-vs-soft-constraints",
    "title": "Model predictive control (MPC)",
    "section": "Hard constraints vs soft constraints",
    "text": "Hard constraints vs soft constraints\nx",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_dir_mpc.html#literature",
    "href": "discr_dir_mpc.html#literature",
    "title": "Model predictive control (MPC)",
    "section": "Literature",
    "text": "Literature\n(Rawlings, Mayne, and Diehl 2017), (Borrelli, Bemporad, and Morari 2017), (Gros and Diehl 2022) (Bemporad 2021)",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Model predictive control (MPC)"
    ]
  },
  {
    "objectID": "discr_indir_LQR_inf_horizon.html",
    "href": "discr_indir_LQR_inf_horizon.html",
    "title": "Discrete-time LQR-optimal control on an infinite horizon",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR-optimal control on an infinite horizon"
    ]
  },
  {
    "objectID": "limits_of_performance_MIMO.html",
    "href": "limits_of_performance_MIMO.html",
    "title": "Limits for MIMO systems",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "13. Limits of performance",
      "Limits for MIMO systems"
    ]
  },
  {
    "objectID": "discr_indir_LQR_fin_horizon.html",
    "href": "discr_indir_LQR_fin_horizon.html",
    "title": "Discrete-time LQR-optimal control on a finite horizon",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time LQR-optimal control on a finite horizon"
    ]
  },
  {
    "objectID": "rocond_mu_synthesis.html",
    "href": "rocond_mu_synthesis.html",
    "title": "Mu synthesis",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "12. Robust control",
      "Mu synthesis"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html",
    "href": "opt_algo_constrained.html",
    "title": "Algorithms for constrained optimization",
    "section": "",
    "text": "We keep adhering to our previous decision to focus on the algorithms that use derivatives. But even then the number of derivative-based algorithms for constrained optimization – and we consider both equality and inequality constraints – is large. They can be classified in many ways.\nOne way to classify the derivative-based algorithms for constrained optimization is based on is to based on the dimension of the space in which they work. For an optimization problem with n variables and m constraints, we have the following possibilities: n-m, n, m, and n+m.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for constrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html#primal-methods",
    "href": "opt_algo_constrained.html#primal-methods",
    "title": "Algorithms for constrained optimization",
    "section": "Primal methods",
    "text": "Primal methods\n\nWith m equality constraints, they work in the space of dimension n-m.\nThree advantages\n\neach point generated by the iterative algoritm is feasible – if terminated early, such point is feaible.\nif they generate a converging sequence, it typically converges at least to a local constrained minimum.\nit does not rely on a special structure of the problem, it can be even nonconvex.\n\nbut it needs a feasible initial point.\nThey may fail for inequality constraints.\n\nThey are particularly useful for linear/affine constraints or simple nonlinear constraints (norm balls or ellipsoids).\n\nProjected gradient method\n\n\nActive set methods\n\n\nSequential quadratic programming (SQP)\nKKT conditions for a nonlinear program with equality constraints solved by Newton’s method.\nInterpretation: at each iteration, we solve a quadratic program (QP) with linear constraints.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for constrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html#penalty-and-barrier-methods",
    "href": "opt_algo_constrained.html#penalty-and-barrier-methods",
    "title": "Algorithms for constrained optimization",
    "section": "Penalty and barrier methods",
    "text": "Penalty and barrier methods",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for constrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html#dual-methods",
    "href": "opt_algo_constrained.html#dual-methods",
    "title": "Algorithms for constrained optimization",
    "section": "Dual methods",
    "text": "Dual methods",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for constrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_constrained.html#primal-dual-methods",
    "href": "opt_algo_constrained.html#primal-dual-methods",
    "title": "Algorithms for constrained optimization",
    "section": "Primal-dual methods",
    "text": "Primal-dual methods",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for constrained optimization"
    ]
  },
  {
    "objectID": "cont_indir_LQR_fin_horizon.html",
    "href": "cont_indir_LQR_fin_horizon.html",
    "title": "Indirect approach to LQR on a finite horizon",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Indirect approach to LQR on a finite horizon"
    ]
  },
  {
    "objectID": "stochastic_LQR.html",
    "href": "stochastic_LQR.html",
    "title": "Stochastic LQR control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Stochastic LQR control"
    ]
  },
  {
    "objectID": "reduction_order_model.html",
    "href": "reduction_order_model.html",
    "title": "Model order reduction",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "14. Model and controller order reduction",
      "Model order reduction"
    ]
  },
  {
    "objectID": "LTR.html",
    "href": "LTR.html",
    "title": "Loop transfer recovery (LTR)",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "Loop transfer recovery (LTR)"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html",
    "href": "opt_algo_derivatives.html",
    "title": "Computing derivatives",
    "section": "",
    "text": "We have already argued that using derivatives within gives us a huge advantage in solving optimization problems.\nThere are three ways to compute derivatives:",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing derivatives"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html#symbolic-methods",
    "href": "opt_algo_derivatives.html#symbolic-methods",
    "title": "Computing derivatives",
    "section": "Symbolic methods",
    "text": "Symbolic methods\nThese are essentially the methods that we have all learnt to apply using a pen and paper. A bunch of rules. The outcome is an expression.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing derivatives"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html#numerical-finite-difference-fd-methods",
    "href": "opt_algo_derivatives.html#numerical-finite-difference-fd-methods",
    "title": "Computing derivatives",
    "section": "Numerical finite-difference (FD) methods",
    "text": "Numerical finite-difference (FD) methods\nThese methods approximate the derivative by computing differences between the function values at different points, hence the name finite-difference (FD) methods. The simplest FD methods follow from the definition of the derivative after omiting the limit:\n\n\\frac{\\mathrm d f(x)}{\\mathrm d x} \\approx \\frac{f(x+\\alpha)-f(x)}{\\alpha}\\qquad\\qquad \\text{forward difference}\n or \n\\frac{\\mathrm d f(x)}{\\mathrm d x} \\approx \\frac{f(x)-f(x-\\alpha)}{\\alpha}\\qquad\\qquad \\text{backward difference}\n or \n\\frac{\\mathrm d f(x)}{\\mathrm d x} \\approx \\frac{f(x+\\frac{\\alpha}{2})-f(x-\\frac{\\alpha}{2})}{\\alpha}\\qquad\\qquad \\text{central difference}\n\nFor functions of vector variables, the same idea applies, but now we have to compute the difference for each component of the vector.",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing derivatives"
    ]
  },
  {
    "objectID": "opt_algo_derivatives.html#algorithmic-also-automatic-differentiation-methods",
    "href": "opt_algo_derivatives.html#algorithmic-also-automatic-differentiation-methods",
    "title": "Computing derivatives",
    "section": "Algorithmic (also Automatic) differentiation methods",
    "text": "Algorithmic (also Automatic) differentiation methods",
    "crumbs": [
      "2. Optimization – algorithms",
      "Computing derivatives"
    ]
  },
  {
    "objectID": "dynamic_programming_LQR.html",
    "href": "dynamic_programming_LQR.html",
    "title": "LQR via dynamic programming",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming",
      "LQR via dynamic programming"
    ]
  },
  {
    "objectID": "uncertainty.html",
    "href": "uncertainty.html",
    "title": "Uncertainty modelling",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "11. Uncertainty modelling and analysis",
      "Uncertainty modelling"
    ]
  },
  {
    "objectID": "cont_indir_CARE.html",
    "href": "cont_indir_CARE.html",
    "title": "Continuous-time Riccati equation",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Continuous-time Riccati equation"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html",
    "href": "opt_theory_reformulations.html",
    "title": "Problem reformulations",
    "section": "",
    "text": "Given a function f(\\bm x), we can maximize it by minimizing -f(\\bm x).",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#maximization-into-minimization",
    "href": "opt_theory_reformulations.html#maximization-into-minimization",
    "title": "Problem reformulations",
    "section": "",
    "text": "Given a function f(\\bm x), we can maximize it by minimizing -f(\\bm x).",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#equality-into-inequality-constraints",
    "href": "opt_theory_reformulations.html#equality-into-inequality-constraints",
    "title": "Problem reformulations",
    "section": "Equality into inequality constraints",
    "text": "Equality into inequality constraints\nAs a matter of fact, we could declare as the most general format of an NLP problem the one with only inequality constraints. This is because we can always transform an equality constraint into two inequality constraints. Given an equality constraint h(\\bm x) = 0, we can write it as h(\\bm x) \\leq 0 and -h(\\bm x) \\leq 0, that is,\n\n\\underbrace{\\begin{bmatrix}\nh(\\bm x) \\\\\n-h(\\bm x)\n\\end{bmatrix}}_{\\mathbf g(\\bm x)} \\leq \\mathbf 0.\n\nOn the other hand, it may be useful to keep the equality constraints explicit in the problem formulation for the benefit of theoretical analysis, numerical methods and convenience of the user/modeller.",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#inequality-into-sort-of-equality-constraints",
    "href": "opt_theory_reformulations.html#inequality-into-sort-of-equality-constraints",
    "title": "Problem reformulations",
    "section": "Inequality into “sort-of” equality constraints",
    "text": "Inequality into “sort-of” equality constraints\nConsider the inequality constraint g(\\bm x) \\leq 0. By introducing a slack variable s and imposing the nonnegativity condition, we can turn the inequality into the equality g(\\bm x) + s = 0. Well, we have not completely discarded an inequality because now we have s \\geq 0. But this new problem may be better suited for some theoretical analysis or numerical methods.\nIt is also possible to express the nonnegativity constraint implicitly by considering an unrestricted variable s and using it within the inequality through its square s^2:\n\ng(\\bm x) + s^2 = 0.",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#linear-cost-function-always-possible",
    "href": "opt_theory_reformulations.html#linear-cost-function-always-possible",
    "title": "Problem reformulations",
    "section": "Linear cost function always possible",
    "text": "Linear cost function always possible\nGiven a cost function f(\\bm x) to be minimized, we can always upper-bound it by a new variable \\gamma accompanied by a new constraint f(\\bm x) \\leq \\gamma and then minimize just \\gamma \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm{x}\\in\\mathbb R^n, \\gamma\\in\\mathbb R} & \\quad \\gamma \\\\\n\\text{subject to} & \\quad f(\\bm x) \\leq \\gamma.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#absolute-value",
    "href": "opt_theory_reformulations.html#absolute-value",
    "title": "Problem reformulations",
    "section": "Absolute value",
    "text": "Absolute value\nConsider an optimization problem in which the cost function contains the absolute value of a variable \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\sum_i c_i|x_i|\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x \\geq \\mathbf b.\n\\end{aligned}\n\nWe also impose the restriction that all the coefficients c_i are nonnegative. The cost function is then a sum of piecewise linear convex function, which can be shown to be convex.\nThe trouble with the absolute value function is that it is not linear, it is not even smooth. And yet, as we will see below, this optimization with the absolute value can be reformulated as a linear program.\nOne possible reformulation introduces two new nonnegative (vector) variables \\bm x^+\\geq 0 and \\bm x^-\\geq 0 and with which the original variables can be expressed as x_i = x_i^+ - x_i^-, \\; i=1, \\ldots, n. The cost function can then be written as \\sum c_i|x_i| = \\sum_i c_i (x_i^+ + x_i^-).\nThis may look surprising (and incorrect) at first, but we argue that at an optimum, x_i^+ or x_i^- must be zero. Otherwise we could subtract (in case c_i&gt;0) the same amount from/to both, which would not change the satisfaction of the constraints (this modification cancels in x_i = x_i^+ - x_i^-), and the cost would be further reduced.\nThe LP in the standard form then changes to\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x^+\\in \\mathbb R^n, \\bm x^-\\in \\mathbb R^n} &\\quad \\mathbf c^\\top (\\bm x^+ + \\bm x^-)\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x^+ - \\mathbf A \\bm x^- \\geq \\mathbf b,\\\\\n&\\quad \\bm x^+ \\geq \\mathbf 0,\\\\\n&\\quad \\bm x^- \\geq \\mathbf 0.\n\\end{aligned}\n\nAnother possibility is to exploit the reformulation of z_i = |x_i| as x_i\\leq z and -x_i\\leq z. The original problem then transforms into\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm z\\in \\mathbb R^n, \\bm x\\in \\mathbb R^n} &\\quad \\mathbf c^\\top \\bm z\\\\\n\\text{subject to} &\\quad \\mathbf A \\bm x \\geq \\mathbf b,\\\\\n&\\qquad \\bm x \\leq \\bm z,\\\\\n&\\quad -\\bm x \\leq \\bm z.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#piecewise-linear",
    "href": "opt_theory_reformulations.html#piecewise-linear",
    "title": "Problem reformulations",
    "section": "Piecewise linear",
    "text": "Piecewise linear",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_reformulations.html#quadratic",
    "href": "opt_theory_reformulations.html#quadratic",
    "title": "Problem reformulations",
    "section": "Quadratic",
    "text": "Quadratic",
    "crumbs": [
      "1. Optimization – theory",
      "Problem reformulations"
    ]
  },
  {
    "objectID": "opt_theory_problems.html",
    "href": "opt_theory_problems.html",
    "title": "Optimization problems",
    "section": "",
    "text": "We formulate a general optimization problem (also a mathematical program) as \n\\begin{aligned}\n\\operatorname*{minimize} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\bm x \\in \\mathcal{X},\n\\end{aligned}\n where f is a scalar function, \\bm x can be a scalar, a vector, a matrix or perhaps even a variable of yet another type, and \\mathcal{X} is a set of values that \\bm x can take, also called the feasible set.\n\n\n\n\n\n\nNote\n\n\n\nThe term “program” here has nothing to do with a computer program. Instead, it was used by the US military during the WWII to refer to plans or schedules in training and logistics.\n\n\nIf maximization of the objective function f() is desired, we can simply multiply the objective function by -1 and minimize the resulting function.\nTypically there are two types of constraints that can be imposed on the optimization variable \\bm x:\n\nexplicit characterization of the set such as \\bm x \\in \\mathbb{R}^n or \\bm x \\in \\mathbb{Z}^n, possibly even a direction enumeration such as \\bm x \\in \\{0,1\\}^n in the case of binary variables,\nimplicit characterization of the set using equations and inequalities such as g_i(\\bm x) \\leq 0 and h_i(\\bm x) = 0 for i = 1, \\ldots, m and j = 1, \\ldots, p.\n\nAn example of a more structured and yet sufficiently general optimization problems over several real and integer variables is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^{n_x}, \\, \\bm y \\in \\mathbb{Z}^{n_y}} \\quad & f(\\bm x, \\bm y) \\\\\n\\text{subject to} \\quad & g_i(\\bm x, \\bm y) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_j(\\bm x, \\bm y) = 0, \\quad j = 1, \\ldots, p.\n\\end{aligned}\n\nIndeed, for named sets such as \\mathbb R or \\mathbb Z, it is common to place the set constraints directly underneath the word “minimize”. But it is just one convention, and these constraints could be listed into the “subject to” section as well.\n\n\n\n\n\n\nInteger optimization not in this course\n\n\n\nIn this course we are only going to consider optimization problems with real-valued variables. This decision does not suggest that optimization with integer variables is less relevant for optimal control, quite the opposite! It is just that the theory and algorithms for integer or mixed integer optimization are based on different principles than those for real variables. And they can hardly fit into a single course. Good news for the interested students is that a graduate course named Combinatorial algorithms (B3B35KOA) covering integer optimization in detail is offered by the Cybernetics and Robotics study program at CTU FEE. Applications of integer optimization to optimal control are part of the course on Hybrid systems (B3B39HYS).\n\n\nThe form of an optimization problem that we are going to use in our course most often than not is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & g_i(\\bm x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_i(\\bm x) = 0, \\quad i = 1, \\ldots, p,\n\\end{aligned}\n which can also be written using vector-valued functions (reflected in the use of the bold face for their names) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\mathbf g(\\bm x) \\leq 0,\\\\\n& \\mathbf h(\\bm x) = 0.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_theory_problems.html#optimization-problem-formulation",
    "href": "opt_theory_problems.html#optimization-problem-formulation",
    "title": "Optimization problems",
    "section": "",
    "text": "We formulate a general optimization problem (also a mathematical program) as \n\\begin{aligned}\n\\operatorname*{minimize} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\bm x \\in \\mathcal{X},\n\\end{aligned}\n where f is a scalar function, \\bm x can be a scalar, a vector, a matrix or perhaps even a variable of yet another type, and \\mathcal{X} is a set of values that \\bm x can take, also called the feasible set.\n\n\n\n\n\n\nNote\n\n\n\nThe term “program” here has nothing to do with a computer program. Instead, it was used by the US military during the WWII to refer to plans or schedules in training and logistics.\n\n\nIf maximization of the objective function f() is desired, we can simply multiply the objective function by -1 and minimize the resulting function.\nTypically there are two types of constraints that can be imposed on the optimization variable \\bm x:\n\nexplicit characterization of the set such as \\bm x \\in \\mathbb{R}^n or \\bm x \\in \\mathbb{Z}^n, possibly even a direction enumeration such as \\bm x \\in \\{0,1\\}^n in the case of binary variables,\nimplicit characterization of the set using equations and inequalities such as g_i(\\bm x) \\leq 0 and h_i(\\bm x) = 0 for i = 1, \\ldots, m and j = 1, \\ldots, p.\n\nAn example of a more structured and yet sufficiently general optimization problems over several real and integer variables is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^{n_x}, \\, \\bm y \\in \\mathbb{Z}^{n_y}} \\quad & f(\\bm x, \\bm y) \\\\\n\\text{subject to} \\quad & g_i(\\bm x, \\bm y) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_j(\\bm x, \\bm y) = 0, \\quad j = 1, \\ldots, p.\n\\end{aligned}\n\nIndeed, for named sets such as \\mathbb R or \\mathbb Z, it is common to place the set constraints directly underneath the word “minimize”. But it is just one convention, and these constraints could be listed into the “subject to” section as well.\n\n\n\n\n\n\nInteger optimization not in this course\n\n\n\nIn this course we are only going to consider optimization problems with real-valued variables. This decision does not suggest that optimization with integer variables is less relevant for optimal control, quite the opposite! It is just that the theory and algorithms for integer or mixed integer optimization are based on different principles than those for real variables. And they can hardly fit into a single course. Good news for the interested students is that a graduate course named Combinatorial algorithms (B3B35KOA) covering integer optimization in detail is offered by the Cybernetics and Robotics study program at CTU FEE. Applications of integer optimization to optimal control are part of the course on Hybrid systems (B3B39HYS).\n\n\nThe form of an optimization problem that we are going to use in our course most often than not is \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & g_i(\\bm x) \\leq 0, \\quad i = 1, \\ldots, m, \\\\\n& h_i(\\bm x) = 0, \\quad i = 1, \\ldots, p,\n\\end{aligned}\n which can also be written using vector-valued functions (reflected in the use of the bold face for their names) \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\mathbf g(\\bm x) \\leq 0,\\\\\n& \\mathbf h(\\bm x) = 0.\n\\end{aligned}",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_theory_problems.html#properties-of-optimization-problems",
    "href": "opt_theory_problems.html#properties-of-optimization-problems",
    "title": "Optimization problems",
    "section": "Properties of optimization problems",
    "text": "Properties of optimization problems\nIt is now the presence/absence and the properties of individual components in the optimization problem defined above that characterize classes of optimization problems. In particular, we can identify the following properties:\n\nUnconstrained vs constrained\n\nPractically relevant problems are almost always constrained. But still there are good reasons to study unconstrained problems too, as many theoretical results and algorithms for constrained problems are based on transformations to unconstrained problems.\n\nLinear vs nonlinear\n\nBy linear problems we mean problems where the objective function and all the functions defining the constraints are linear (or actually affine) functions of the optimization variable \\bm x. Such problems constitute the simplest class of optimization problems, are very well understood, and there are efficient algorithms for solving them. In contrast, nonlinear problems are typically more difficult to solve (but see the discussion of the role of convexity below).\n\nSmooth vs nonsmooth\n\nEfficient algorithms for optimization over real variables benefit heavily from knowledge of the derivatives of the objective and constraint functions. If the functions are differentiable (aka smooth), we say that the whole optimization problem is smooth. Nonsmooth problems are typically a more difficult to analyze and solve (but again, see the discussion of the role of convexity below).\n\nConvex vs nonconvex\n\nIf the objective function and the feasible set are convex (the latter holds when the functions defining the inequality constraints are convex and the functions defining the equality constrains are affine), the whole optimization problem is convex. Convex optimization problems are very well understood and there are efficient algorithms for solving them. In contrast, nonconvex problems are typically more difficult to solve. It turns out that convexity is a lot more important property than linearity and smoothness when it comes to solving optimization problems efficiently.",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_theory_problems.html#classes-of-optimization-problems",
    "href": "opt_theory_problems.html#classes-of-optimization-problems",
    "title": "Optimization problems",
    "section": "Classes of optimization problems",
    "text": "Classes of optimization problems\nBased on the properties discussed above, we can identify the following distinct classes of optimization problems:\n\nLinear program (LP)\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\mathbf A_\\mathrm{ineq}\\bm x \\leq \\mathbf b_\\mathrm{ineq},\\\\\n& \\mathbf A_\\mathrm{eq}\\bm x = \\mathbf b_\\mathrm{eq}.\n\\end{aligned}\n\nAn LP is obviously linear, hence it is also smooth and convex.\nSome theoretical results and numerical algorithms require a linear program in a specific form, called the standard form: \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\mathbf A\\bm x = \\mathbf b,\\\\\n& \\bm x \\geq \\mathbf 0,\n\\end{aligned}\n where the inequality \\bm x \\geq \\mathbf 0 is understood componentwise, that is, x_i \\geq 0 for all i = 1, \\ldots, n.\n\n\nQuadratic program (QP)\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\bm x^\\top \\mathbf Q \\bm x +  \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\mathbf A_\\mathrm{ineq}\\bm x \\leq \\mathbf b_\\mathrm{ineq},\\\\\n& \\mathbf A_\\mathrm{eq}\\bm x = \\mathbf b_\\mathrm{eq}.\n\\end{aligned}\n\nEven though the QP is nonlinear, it is smooth and if the matrix \\mathbf Q is positive semidefinite, it is convex. Its analysis and numerical solution are not much more difficult than those of an LP problem.\n\nQuadratically constrained quadratic program (QCQP)\nIt is worth emphasizing that for the standard QP the constraints are still given by a system of linear equations and inequalities. Sometimes we can encounter problems in which not only the cost function but also the functions defining the constraints are quadratic as in \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & \\bm x^\\top \\mathbf Q \\bm x +  \\mathbf c^\\top \\bm x \\\\\n\\text{subject to} \\quad & \\bm x^\\top \\mathbf A_i\\bm x + \\mathbf b_i \\bm x + c_i \\leq \\mathbf 0, \\quad i=1, \\ldots, m.\n\\end{aligned}\n\nA QCQP problem is convex if and only if the the constraints define a convex feasible set, which is the case when all the matrices \\mathbf A_i are positive semidefinite.\n\n\n\nConic program (CP)\nFirst, what is a cone? It is a set such that if something is in the cone, then a multiple of it by a nonnegative number is still in the set. We are going to restrict ourselves to regular cones, which are are pointed, closed and convex. An example of such regular cone in a plane is in Figure 1 below.\n\n\n\n\n\n\nFigure 1: Regular (pointed, convex, closed) cone in a plane\n\n\n\nNow, what is the point in using cones in optimization? Reformulation of nonlinear optimization problems using cones constitutes a systematic way to identify what these (conic) optimization problems have in common with linear programs, for which powerful theory and efficient algorithms exist.\nNote that an LP in the standard form can be written as \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad \\bm x\\in \\mathbb{R}_+^n,\n\\end{aligned}\n where \\mathbb R_+^n is a positive orthant. Now, the positive orthant is a convex cone! We can then see the LP as an instance of a general conic optimization problem (conic program)\n\n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad \\bm x\\in \\mathcal{K},\n\\end{aligned}\n where \\mathcal{K} is a cone in \\mathbb R^n.\n\n\n\n\n\n\nInequality as belonging to a cone\n\n\n\nA fundamental idea unrolled here: the inequality \\bm x\\geq 0 can be interpreted as \\bm x belonging to a componentwise nonegative cone, that is \\bm x \\in \\mathbb R_+^n. What if some other cone \\mathcal K is considered? What would be the interpretation of the inequality then?\n\n\nSometimes in order to emphasize that the inequality is induced by the cone \\mathcal K, we write it as \\geq_\\mathcal{K}. Another convention – the one we actually adopt here – is to use another symbol for the inequality \\succeq to distinguish it from the componentwise meaning, assuming that the cone is understood from the context. We then interpret conic inequalities such as \n\\mathbf A_\\mathrm{ineq}\\bm x \\succeq \\mathbf b_\\mathrm{ineq}\n in the sense that \n\\mathbf A_\\mathrm{ineq}\\bm x - \\mathbf b_\\mathrm{ineq} \\in \\mathcal{K}.\n\nIt is high time to explore some concrete cones (other than the positive orthant). We consider just two, but there are a few more, see the literature.\n\nSecond-order cone program (SOCP)\nThe most immediate cone in \\mathbb R^n is the second-order cone, also called the Lorentz cone or even the ice cream cone. We explain it in \\mathbb R^3 for the ease of visualization, but generalization to \\mathbb R^n is straightforward. The second-order cone in \\mathbb R^3 is defined as \n\\mathcal{K}_\\mathrm{SOC}^3 = \\left\\{ \\bm x \\in \\mathbb R^3 \\mid \\sqrt{x_1^2 + x_2^2} \\leq x_3 \\right\\}.\n\nand is visualized in Figure 2 below.\n\n\n\n\n\n\n\n\nFigure 2: A second-order cone in 3D\n\n\n\n\n\nWhich of the three axes plays the role of the axis of symmetry for the cone must be agreed beforhand. Singling this direction out, the SOC in \\mathbb R^n can also be formulated as \n\\mathcal{K}_\\mathrm{SOC}^n = \\left\\{ (\\bm x, t) \\in \\mathbb R^{n-1} \\times \\mathbb R \\mid \\|\\bm x\\|_2 \\leq t \\right\\}.\n\nA second-order conic program in standard form is then \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad \\bm x\\in \\mathcal{K}_\\mathrm{SOC}^n,\n\\end{aligned}\n\nwhich can be written explicitly as \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A\\bm x = \\mathbf b,\\\\\n&\\quad x_1^2 + \\cdots + x_{n-1}^2 - x_n^2 \\leq 0.\n\\end{aligned}\n\nA second-order conic program can also come in non-standard form such as \n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A_\\mathrm{ineq}\\bm x \\succeq  \\mathbf b_\\mathrm{ineq}.\n\\end{aligned}\n\nAssuming the data is structured as \n\\begin{bmatrix}\n\\mathbf A\\\\\n\\mathbf c^\\top\n\\end{bmatrix}\n\\bm x \\succeq\n\\begin{bmatrix}\n\\mathbf b\\\\\nd\n\\end{bmatrix},\n the inequality can be rewritten as \n\\begin{bmatrix}\n\\mathbf A\\\\\n\\mathbf c^\\top\n\\end{bmatrix}\n\\bm x -\n\\begin{bmatrix}\n\\mathbf b\\\\\nd\n\\end{bmatrix} \\in \\mathcal{K}_\\mathrm{SOC}^n,\n which finally gives \n\\|\\mathbf A \\bm x - \\mathbf b\\|_2 \\leq \\mathbf c^\\top \\bm x + d.\n\nTo summarize, another form of a second-order cone program (SOCP) is\n\n\\begin{aligned}\n\\operatorname*{minimize} &\\quad \\mathbf c^\\top \\bm x\\\\\n\\text{subject to} &\\quad \\mathbf A_\\mathrm{eq}\\bm x = \\mathbf b_\\mathrm{eq},\\\\\n&\\quad \\|\\mathbf A \\bm x - \\mathbf b\\|_2 \\leq \\mathbf c^\\top \\bm x + d.\n\\end{aligned}\n\nWe can see that the SOCP contains both linear and quadratic constraints, hence it generalizes LP and QP, including convex QCQP. To see the latter, expand the square of \\|\\mathbf A \\bm x - \\mathbf b\\|_2 into (\\bm x^\\top \\mathbf A^\\top  - \\mathbf b^\\top)(\\mathbf A \\bm x - \\mathbf b) = \\bm x^\\top \\mathbf A^\\top \\mathbf A \\bm x + \\ldots\n\n\nSemidefinite program (SDP)\nAnother cone of great importance the control theory is the cone of positive semidefinite matrices. It is commonly denoted as \\mathcal S_+^n and is defined as \n\\mathcal S_+^n = \\left\\{ \\bm X \\in \\mathbb R^{n \\times n} \\mid \\bm X = \\bm X^\\top, \\, \\bm z^\\top \\bm X \\bm z \\geq 0\\; \\forall \\bm z\\in \\mathbb R^n \\right\\},\n and with this cone the inequality \\mathbf X \\succeq 0 is a common way to express that \\mathbf X is positive semidefinite.\nUnlike the previous classes of optimization problems, this one is formulated with matrix variables instead of vector ones. But nothing prevents us from collecting the components of a symmetric matrix into a vector and proceed with vectors as usual, if needed:\n\n\\bm X = \\begin{bmatrix} x_1 & x_2 & x_3 \\\\ x_2 & x_4 & x_5\\\\ x_3 & x_5 & x_6 \\end{bmatrix},\n\\quad\n\\bm x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_6 \\end{bmatrix}.\n\nAn optimization problem with matrix variables constrained to be in the cone of semidefinite matrices (or their vector representations) is called a semidefinite program (SDP). As usual, we start with the standard form, in which the cost function is linear and the optimization is subject to an affine constraint and a conic constraint. In the following, in place of the inner products of two vectors \\mathbf c^\\top x we are going to use inner products of matrices defined as \n\\langle \\mathbf C, \\bm X\\rangle = \\operatorname{Tr} \\mathbf C \\bm X,\n where \\operatorname{Tr} is a trace of a matrix defined as the sum of the diagonal elements.\nThe SDP program in the standard form is then \n\\begin{aligned}\n\\operatorname{minimize}_{\\bm X} &\\quad \\operatorname{Tr} \\mathbf C \\bm X\\\\\n\\text{subject to} &\\quad \\operatorname{Tr} \\mathbf A_i \\bm X = \\mathbf b_i, \\quad i=1, \\ldots, m,\\\\\n&\\quad \\bm X \\in \\mathcal S_+^n,\n\\end{aligned}\n where the latter constraint is more often than not written as \\bm X \\succeq 0, understanding from the context that the cone of positive definite matrices is assumed.\n\n\nOther conic programs\nWe are not going to cover them here, but we only enumerate a few other cones useful in optimization: rotated second-order cone, exponential cone, power cone, … A concise overview is in (“MOSEK Modeling Cookbook” 2024)\n\n\n\nGeometric program (GP)\n\n\nNonlinear program (NLP)\nFor completeness we include here once again the general nonlinear programming problem:\n\n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad & f(\\bm x) \\\\\n\\text{subject to} \\quad & \\mathbf g(\\bm x) \\leq 0,\\\\\n& \\mathbf h(\\bm x) = 0.\n\\end{aligned}\n\nSmoothness of the problem can easily be determined based on the differentiability of the functions. Convexity can also be determined by inspecting the functions, but this is not necessarily easy. One way to check convexity of a function is to view it as a composition of simple functions and exploit the knowledge about convexity of these simple functions. See (Boyd and Vandenberghe 2004, sec. 3.2)",
    "crumbs": [
      "1. Optimization – theory",
      "Optimization problems"
    ]
  },
  {
    "objectID": "opt_algo_solvers.html",
    "href": "opt_algo_solvers.html",
    "title": "Numerical solvers",
    "section": "",
    "text": "The number of numerical solvers is huge. First, we give a short biased list of solvers which we may use within this course.\n\nOptimization Toolbox for Matlab: fmincon, fminunc, linprog, quadpro, … Available within the all-university Matlab license for all students and employees at CTU.\nGurobi Optimizer: LP, QP, SOCP, MIP, commercial (but free academic license available).\nIBM ILOG CPLEX: LP, QP, SOCP, MIP, commercial (but free academic license available).\nMOSEK: LP, QP, MIP, SOCP, SDP, commercial (but free academic license available).\nHIGHS: LP, QP, MIP, open source.\nKnitro: NLP, commercial.\nIpopt: NLP, open source.\nSEDUMI: SOCP, SDP, open source.\n…\n\nSecond, for a reasonably comprehensive and well maintained list of solvers, consult the NEOS Guide to Optimization web page (in particular the link at the bottom of that page). Similar list is maintained within Hans Mittelman’s Decision Tree for Optimization Software web page.\nWorking in Matlab and using Yalmip for defining and solving optimization problems, the list of optimization solvers supported by Yalmip shows what is available.\nSimilarly, users of Julia and JuMP will find the list of solvers supported by JuMP useful. The list is worth consulting even if Julia is not the tool of choice, as many solvers are indepdenent of Julia.\n\n\n\n Back to top",
    "crumbs": [
      "2. Optimization – algorithms",
      "Numerical solvers"
    ]
  },
  {
    "objectID": "cont_dir_collocation.html",
    "href": "cont_dir_collocation.html",
    "title": "Collocation methods for optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "9. Continuous-time optimal control - direct approach via numerical optimization",
      "Collocation methods for optimal control"
    ]
  },
  {
    "objectID": "cont_indir_time_optimal.html",
    "href": "cont_indir_time_optimal.html",
    "title": "Time-optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Time-optimal control"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html",
    "href": "opt_theory_unconstrained.html",
    "title": "Theory for unconstrained optimization",
    "section": "",
    "text": "Here we are going to analyze the optimization problem with no constraints \n\\operatorname*{minimize}_{\\bm x \\in \\mathbb{R}^n} \\quad  f(\\bm x).\nWhy are we considering such an unrealistic problem? After all, every engineering problem is subject to some constraints.\nBesides the standard teacher’s answer that we should always start with easier problems, there is another answer: it is common for analysis and algorithms for constrained optimization problems to reformulate them as unconstrained ones and then apply tools for unconstrained problems.",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html#local-vs-global-optimality",
    "href": "opt_theory_unconstrained.html#local-vs-global-optimality",
    "title": "Theory for unconstrained optimization",
    "section": "Local vs global optimality",
    "text": "Local vs global optimality\nFirst, let’s define carefully what we mean by a minimum in the unconstrained problem.\n\n\n\n\n\n\nCaution\n\n\n\nFor those whose mother tongue does not use articles such as the and a/an in English, it is worth emphasizing that there is a difference between “the minimum” and “a minimum”. In the former we assume that there is just one minimum, in the latter we make no such assumption.\n\n\nConsider a (scalar) function of a scalar variable for simplicity\n\nWe say, that the function has a local minimum at x^\\star if f(x)\\geq f(x^\\star) in an \\varepsilon neighbourhood. All the red dots in the above figure are local minima. Similarly, of course, the function has a local maximum at x^\\star if f(x)\\leq f(x^\\star) in an \\varepsilon neighbourhood. Such local maxima are the green dots in the figure. The smallest and the largest of these are global minima and maxima, respectively.",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html#conditions-of-optimality",
    "href": "opt_theory_unconstrained.html#conditions-of-optimality",
    "title": "Theory for unconstrained optimization",
    "section": "Conditions of optimality",
    "text": "Conditions of optimality\nHere we consider two types of conditions of optimality for unconstrained minimization problems: necessary and sufficient conditions. Necessary conditions must be satisfied at the minimum, but even when they are, the optimality is not guaranteed. On the other hand, the sufficient conditions need not be satisfied at the minimum, but if they are, the optimality is guaranteed. We show the necessary conditions of the first and second order, while the sufficient condition only of the second order.\n\nScalar optimization variable\nYou may want to have a look at the video, but below we continue with the text that summarizes the video.\n\nWe recall here the fundamental assumption made at the beginning of our introduction to optimization – we only consider optimization variables that are real-valued (first, just scalar x \\in \\mathbb R, later vectors \\bm x \\in \\mathbb R^n), and objective functions f() that are sufficiently smooth – all the derivatives exist. Then the conditions of optimality can be derived upon inspecting the Taylor series approximation of the cost function around the minimum.\n\nTaylor series approximation around the optimum\nDenote x^\\star as the (local) minimum of the function f(x). The Taylor series expansion of f(x) around x^\\star is\n\nf(x^\\star+\\alpha) = f(x^\\star)+\\left.\\frac{\\mathrm{d}f(x)}{\\mathrm{d} x}\\right|_{x=x^\\star}\\alpha + \\frac{1}{2}\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star}\\alpha^2 + {\\color{blue}\\mathcal{O}(\\alpha^3)},\n where \\mathcal{O}() is called Big O and has the property that \n\\lim_{\\alpha\\rightarrow 0}\\frac{\\mathcal{O}(\\alpha^3)}{\\alpha^3} \\leq M&lt;\\infty.\n\nAlternatively, we can write the Taylor series expansion as \nf(x^\\star+\\alpha) = f(x^\\star)+\\left.\\frac{\\mathrm{d}f(x)}{\\mathrm{d} x}\\right|_{x=x^\\star}\\alpha + \\frac{1}{2}\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star}\\alpha^2 + {\\color{red}o(\\alpha^2)},\n using the little o with the property that \n\\lim_{\\alpha\\rightarrow 0}\\frac{o(\\alpha^2)}{\\alpha^2} = 0.\n\nWhether \\mathcal{O}() or \\mathcal{o}() concepts are used, it is just a matter of personal preference. They both express that the higher-order terms in the expansion tend to be negligible compare to the first- and second-order term as \\alpha is getting smaller. \\mathcal O(\\alpha^3) goes to zero at least as fast as a cubic function, while o(\\alpha^2) goes to zero faster than a quadratic function.\nIt is indeed important to understand that this negligibility of the higher-order terms is only valid asymptotically – for a particular \\alpha it may easily happend that, say, the third-order term is still dominating.\n\n\nFirst-order necessary conditions of optimality\nFor \\alpha sufficiently small, the first-order Taylor series expansion is a good approximation of the function f(x) around the minimum. Since \\alpha enters this expansion linearly, the cost function can increase or decrease with \\alpha, depending on the sign of the first derivative. The only way to ensure that the function as a (local) minimu at x^\\star is to have the first derivative equal to zero, that is \\boxed{\n\\left.\\frac{\\mathrm{d}f(x)}{\\mathrm{d} x}\\right|_{x=x^\\star} = 0.}\n\n\n\nSecond-order necessary conditions of optimality\nOnce the first-order necessary condition of optimality is satisfied, the dominating term (as \\alpha is getting smalle) is the second-order term \\frac{1}{2}\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star}\\alpha^2. Since \\alpha is squared, it is the sign of the second derivative that determines the contribution of the whole second-order term to the cost function value. For the minimum, the second derivative must be nonnegative, that is\n\n\\boxed{\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star} \\geq 0.}\n\nFor completeness we state that the sign must be nonpositive for the maximum.\n\n\nSecond-order sufficient condition of optimality\nFollowing the same line of reasoning as above, the if the second derivative is positive, the miniumum is guaranteed, that is, the sufficient condition of optimality is \n\\boxed{\\left.\\frac{\\mathrm{d^2}f(x)}{\\mathrm{d} x^2}\\right|_{x=x^\\star} &gt; 0.}\n\nIf the second derivative fails to be positive and is just zero (thus still satisfying the necessary condition), does it mean that the point is not a minimum? No. We must examine higher order terms.\n\n\n\nVector optimization variable\nOnce again, should you prefer watching a video, here it is, but below we continue with the text that covers the content of the video.\n\n\nFirst-order necessary conditions of optimality\nOne way to handle the vector variables is to convert the vector problem into a scalar one by fixing a direction to an arbitrary vector \\bm d and then considering the scalar function of the form f(\\bm x^\\star + \\alpha \\bm d). For convenience we define a new function \ng(\\alpha) \\coloneqq f(\\bm x^\\star + \\alpha \\bm d)\n and from now on we can invoke the results for scalar functions. Namely, we expand the g() function around zero as \ng(\\alpha) = g(0) + \\frac{\\mathrm{d}g(\\alpha)}{\\mathrm{d}\\alpha}\\bigg|_{\\alpha=0}\\alpha + \\frac{1}{2}\\frac{\\mathrm{d}^2 g(\\alpha)}{\\mathrm{d}\\alpha^2}\\bigg|_{\\alpha=0}\\alpha^2 + \\mathcal{O}(\\alpha^3),\n and argue that the first-order necessary condition of optimality is \n\\frac{\\mathrm{d}g(\\alpha)}{\\mathrm{d}\\alpha}\\bigg|_{\\alpha=0} = 0.\n\nNow, invoking the chain rule, we go back from g() to f() \n\\frac{\\mathrm{d}g(\\alpha)}{\\mathrm{d}\\alpha}\\bigg|_{\\alpha=0} = \\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star} \\frac{\\partial(\\bm x^\\star + \\alpha \\bm d)}{\\partial\\alpha}\\bigg|_{\\alpha=0} = \\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star}\\,\\bm d = 0,\n where \\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star} is a row vector of partial derivatives of f() evaluated at \\bm x^\\star. Since the vector \\bm d is arbitrary, the necessary condition is that\n\n\\frac{\\partial f(\\bm x)}{\\partial\\bm x}\\bigg|_{\\bm x=\\bm x^\\star} = \\mathbf 0,  \n\nMore often than not we use the column vector to store partial derivatives. We call it the gradient of the function f() and denoted it as \n\\nabla f(\\bm x) \\coloneqq \\begin{bmatrix}\\frac{\\partial f(\\bm x)}{\\partial x_1} \\\\ \\frac{\\partial f(\\bm x)}{\\partial x_n} \\\\ \\vdots \\\\ \\frac{\\partial f(\\bm x)}{\\partial x_n}\\end{bmatrix}.  \n\nThe first-order necessary condition of optimality using gradients is then \n\\boxed{\\left.\\nabla f(\\bm x)\\right|_{x=x^\\star} = \\mathbf 0.  }\n\n\n\n\n\n\n\nGradient is a column vector\n\n\n\nIn some literature the gradient \\nabla f(\\bm x) is defined as a row vector. For the condition of optimality it does not matteer since all we require is that all partial derivatives vanish. But for other purposes in our text we regard the gradient as a vector living in the same vector space \\mathbb R^n as the optimization variable. The row vector is sometimes denoted as \\mathrm Df(\\bm x).\n\n\n\nComputing the gradient of a scalar function of a vector variable\nA convenient way is to compute the differential fist and then to identify the derivative in it. Recall that the differential is the first-order approximation to the increment of the function due to a change in the variable\n\n\\Delta f \\approx \\mathrm{d}f = \\nabla f(x)^\\top \\mathrm d \\bm x.\n\nFinding the differential of a function is conceptually easier than finding the derivative since it is a scalar quantity. When searching for the differential of a composed function, we follow the same rules as for the derivative (such as that the one for finding the differential of a product). Let’s illustrate it using an example.\n\nExample 1 For the function \nf(\\mathbf x) = \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\mathbf{r}^\\top\\bm{x},\n where \\mathbf Q is symetric, the differential is \n\\mathrm{d}f = \\frac{1}{2}\\mathrm d\\bm{x}^\\top\\mathbf{Q}\\bm{x} + \\frac{1}{2}\\bm{x}^\\top\\mathbf{Q}\\mathrm d\\bm{x} + \\mathbf{r}^\\top\\mathrm{d}\\bm{x},\n in which the first two terms can be combined thanks to the fact that they are scalars \n\\mathrm{d}f = \\left(\\bm{x}^\\top\\frac{\\mathbf{Q} + \\mathbf{Q}^\\top}{2} + \\mathbf{r}^\\top\\right)\\mathrm{d}\\bm{x},\n and finally, since we assumed that \\mathbf Q is a symmetric matrix, we get \n\\mathrm{d}f = \\left(\\mathbf{Q}\\bm{x} + \\mathbf{r}\\right)^\\top\\mathrm{d}\\bm{x},\n from which we can identify the gradient as \n\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\mathbf{x} + \\mathbf{r}.\n\nThe first-order condition of optimality is then \n\\boxed{\\mathbf{Q}\\mathbf{x} = -\\mathbf{r}.}\n\nAlthough this was just an example, it is actually a very useful one. Keep this result in mind – necessary condition of optimality of a quadratic function comes in the form of a set of linear equations.\n\n\n\n\nSecond-order necessary conditions of optimality\nAs before, we fix the direction \\bm d and consider the function g(\\alpha) = f(\\bm x^\\star + \\alpha \\bm d). We expand the expression for the first derivative as \n\\frac{\\mathrm d g(\\alpha)}{\\mathrm d \\alpha} = \\sum_{i=1}^{n}\\frac{\\partial f(\\bm x)}{\\partial x_i}\\bigg|_{\\bm x = \\bm x^\\star} d_i,\n and differentiating this once again, we get the second derivative \n\\frac{\\mathrm d^2 g(\\alpha)}{\\mathrm d \\alpha^2} = \\sum_{i,j=1}^{n}\\frac{\\partial^2 f(\\bm x)}{\\partial x_ix_j}\\bigg|_{\\bm x = \\bm x^\\star}d_id_j\n\n\n\\begin{aligned}\n\\frac{\\mathrm d^2 g(\\alpha)}{\\mathrm d \\alpha^2}\\bigg|_{\\alpha=0} &= \\sum_{i,j=1}^{n}\\frac{\\text{d}^2}{\\text{d}x_ix_j}f(\\bm x)\\bigg|_{\\bm x = \\bm x^\\star}d_id_j\\\\\n&= \\mathbf d^\\text{T} \\underbrace{\\nabla^\\text{2}f(\\mathbf x)\\bigg|_{\\bm x = \\bm x^\\star}}_\\text{Hessian} \\mathbf d.\n\\end{aligned}\n where \\nabla^2 f(\\mathbf x) is the Hessian (the symmetrix matrix of the second-order mixed partial derivatives) \n\\nabla^2 f(x) = \\begin{bmatrix}\n                 \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_1^2} && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_1\\partial x_2} && \\ldots && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_1\\partial x_n}\\\\\n                 \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_2\\partial x_1} && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_2^2} && \\ldots && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_2\\partial x_n}\\\\\n                 \\vdots\\\\\n                 \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_n\\partial x_1} && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_n\\partial x_2} && \\ldots && \\frac{\\partial^2 f(\\mathbf x)}{\\partial x_n\\partial x_n}.\n                \\end{bmatrix}\n\nSince \\bm d is arbitrary, the second-order necessary condition of optimality is then \n\\boxed{\\nabla^\\text{2}f(\\mathbf x)\\bigg|_{\\bm x = \\bm x^\\star} \\succeq 0,}\n where, once again, the inequality \\succeq reads that the matrix is positive semidefinite.\n\n\nSecond-order sufficient condition of optimality\n\n\\boxed{\\nabla^2 f(\\mathbf x)\\bigg|_{\\bm x = \\bm x^\\star} \\succ 0,}\n where, once again, the inequality \\succ reads that the matrix is positive definite.\n\nExample 2 For the quadratic function f(\\mathbf x) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{Q}\\mathbf{x} + \\mathbf{r}^\\mathrm{T}\\mathbf{x}, the Hessian is \n\\nabla^2 f(\\mathbf{x}) = \\mathbf{Q}\n and the second-order necessary condition of optimality is \n\\boxed{\\mathbf{Q} \\succeq 0.}\n\nSecond-order sufficient condition of optimality is then \n\\boxed{\\mathbf{Q} \\succ 0.}\n\nOnce again, this was more than just an example – quadratic functions are so important for us that it is worth remembering this result.",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_theory_unconstrained.html#classification-of-stationary-points",
    "href": "opt_theory_unconstrained.html#classification-of-stationary-points",
    "title": "Theory for unconstrained optimization",
    "section": "Classification of stationary points",
    "text": "Classification of stationary points\nFor a stationary (also critical) \\bm x^\\star, that is, one that satisfies the first-order necessary condition \n\\nabla f(\\bm x^\\star) = 0,\n\nwe can classify it as\n\nMinimum: \\nabla^2 f(x^\\star)\\succ 0\nMaximum: \\nabla^2 f(x^\\star)\\prec 0\nSaddle point: \\nabla^2 f(x^\\star) indefinite\nSingular point (we cannot decide): \\nabla^2 f(x^\\star)=0\n\n\nExample 3 (Minimum of a quadratic function) We consider a quadratic function f(\\mathbf x) = \\frac{1}{2}\\mathbf{x}^\\mathrm{T}\\mathbf{Q}\\mathbf{x} + \\mathbf{r}^\\mathrm{T}\\mathbf{x} for a particular \\mathbf{Q} and \\mathbf{r}.\n\n\nCode\nQ = [1 1; 1 2]\nr = [0, 1]\n\nusing LinearAlgebra\nx_stationary = -Q\\r\neigvals(Q) \n\n\n2-element Vector{Float64}:\n 0.38196601125010515\n 2.618033988749895\n\n\nThe matrix is positive definite, so the stationary point is a minimum. In fact, the minimum. Surface and contour plots of the function are shown below.\n\nCode\nf(x) = 1/2*dot(x,Q*x)+dot(x,r)\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ncontour(x1_data,x2_data,f_data)\ndisplay(plot!([x_stationary[1]], [x_stationary[2]], marker=:circle, label=\"Stationary point\"))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 1: Minimum of a quadratic function\n\n\n\n\n\nExample 4 (Saddle point of a quadratic function)  \n\n\nCode\nQ = [-1 1; 1 2]\nr = [0, 1]\n\nusing LinearAlgebra\nx_stationary = -Q\\r\neigvals(Q)\n\n\n2-element Vector{Float64}:\n -1.3027756377319946\n  2.302775637731995\n\n\nThe matrix is indefinite, so the stationary point is a saddle point. Surface and contour plots of the function are shown below.\n\nCode\nf(x) = 1/2*dot(x,Q*x)+dot(x,r)\n\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ncontour(x1_data,x2_data,f_data)\ndisplay(plot!([x_stationary[1]], [x_stationary[2]], marker=:circle, label=\"Stationary point\"))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 2: Saddle point of a quadratic function\n\n\n\n\n\nExample 5 (Singular point of a quadratic function)  \n\n\nCode\nQ = [2 0; 0 0]\nr = [3, 0]\n\nusing LinearAlgebra\neigvals(Q)\n\n\n2-element Vector{Float64}:\n 0.0\n 2.0\n\n\nThe matrix Q is singular, which has two consequences:\n\nWe cannot compute the stationary point since Q is not invertible. In fact, there is a whole line (a subspace) of stationary points.\nThe matrix Q is positive semidefinite, which generally means that optimality cannot be concluded. But in this particular case of a quadratic function, there are no higher-order terms in the Taylor series expansion, so the stationary point is a minimum.\n\nSurface and contour plots of the function are shown below.\n\nCode\nf(x) = 1/2*dot(x,Q*x)+dot(x,r)\n\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ndisplay(contour(x1_data,x2_data,f_data))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 3: Singular point of a quadratic function\n\n\n\n\n\nExample 6 (Singular point of a non-quadratic function) Consider the function f(\\bm x) = x_1^2 + x_2^4. Its gradient is \\nabla f(\\bm x) = \\begin{bmatrix}2x_1\\\\ 4x_2^3\\end{bmatrix} and it vanishes at \\bm x^\\star = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}. The Hessian is \\nabla^2 f(\\bm x) = \\begin{bmatrix}2 & 0\\\\ 0 & 12x_2^2\\end{bmatrix}, which when evaluated at the stationary point is \\nabla^2 f(\\bm x)\\bigg|_{\\bm x=\\mathbf 0} = \\begin{bmatrix}2 & 0\\\\ 0 & 0\\end{bmatrix}, which is positive semidefinite. We cannot conclude if the function attains a minimum at \\bm x^\\star.\nWe need to examine higher-order terms in the Taylor series expansion. The third derivatives are \n\\frac{\\partial^3 f}{\\partial x_1^3} = 0, \\quad \\frac{\\partial^3 f}{\\partial x_1^2\\partial x_2} = 0, \\quad \\frac{\\partial^3 f}{\\partial x_1\\partial x_2^2} = 0, \\quad \\frac{\\partial^3 f}{\\partial x_2^3} = 24x_2,\n and when evaluated at zero, they all vanish.\nAll but one fourth derivatives also vanish. The one that does not is \n\\frac{\\partial^4 f}{\\partial x_2^4} = 24,\n which is positive, and since the associated derivative is of the even order, the power si also even, hence the function attains a minimum at \\bm x^\\star = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}.\nThis can also be visually confirmed by the surface and contour plots of the function.\n\nCode\nf(x) = x[1]^2 + x[2]^4\n\nx1_data = x2_data = -4:0.1:4;  \nf_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ndisplay(surface(x1_data,x2_data,f_data))\ncontour(x1_data,x2_data,f_data)\ndisplay(plot!([x_stationary[1]], [x_stationary[2]], marker=:circle, label=\"Stationary point\"))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n(a) Surface plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Contour plot\n\n\n\n\n\n\n\nFigure 4: Singular point of a non-quadratic function",
    "crumbs": [
      "1. Optimization – theory",
      "Theory for unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_references.html",
    "href": "opt_algo_references.html",
    "title": "References for optimization algorithms",
    "section": "",
    "text": "Pretty much identical to the literature recommended in the previous section on optimization theory.\nSome practical aspects are discussed in Guidelines for Numerical Issues for Gurobi Optimizer.\n\n\n\n Back to top",
    "crumbs": [
      "2. Optimization – algorithms",
      "References for optimization algorithms"
    ]
  },
  {
    "objectID": "discr_dir_general.html",
    "href": "discr_dir_general.html",
    "title": "General finite-horizon nonlinear discrete-time optimal control as nonlinear program",
    "section": "",
    "text": "In this section we formulate a finite-horizon optimal control problem (OCP) for a discrete-time dynamical system as a mathematical optimization (also mathematical programming) problem, which can then be solved numerically by a suitable solvers for nonlinear programming (NLP), or possibly quadratic programming (QP). The outcome of such numerical optimization is an optimal control trajectory (a sequence of controls), which is why this approach is called direct – we optimize directly over the trajectories.\nIn the next chapter we then present an alternative – indirect – approach, wherein the conditions of optimality are formulated first. These come in the form of a set of equations, some of them recurrent/recursive, hence indirect approach amounts to solving such equations.\nBut now back to the direct approaches. We will start with a general nonlinear discrete-time optimal control problem in this section and then specialize to the linear quadratic regulation (LQR) problem in the next section. Finally, since the computed control trajectory constitutes an open-loop control scheme, something must be done about it if feedback scheme is preferred – we introduce the concept of a receding horizon control (RHC), perhaps bettern known as model predictive control (MPC), that turns the direct approach into a feedback control scheme.\nWe start by considering a nonlinear discrete-time system modelled by the state equation \n\\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\n where\n\n\\bm x_k\\in \\mathbb R^n is the state at the discrete time k\\in \\mathbb Z,\n\\bm u_k\\in \\mathbb R^m is the control at the discrete time k,\n\\mathbf f_k: \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb Z \\to \\mathbb{R}^n is a state transition function (in general not only nonlinear but also time-varying, with the convention that the dependence on k is expressed through the lower index).\n\nA general nonlinear discrete-time optimal control problem (OCP) is then formulated as \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_i,\\ldots, \\bm u_{N-1}, \\bm x_{i},\\ldots, \\bm x_N}&\\quad \\left(\\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k) \\right)\\\\\n\\text{subject to}  &\\quad \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm u_k \\in \\mathcal U_k,\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm x_k \\in \\mathcal X_k,\\quad k=i, \\ldots, N,\n\\end{aligned}\n where\n\ni is the initial discrete time,\nN is the final discrete time,\n\\phi() is a terminal cost function that penalizes the state at the final time,\nL_k() is a running (also stage) cost function,\nand \\mathcal U_k and \\mathcal X_k are sets of feasible controls and states – these sets are typically expressed using equations and inequalities. Should they be constant, the notation is just \\mathcal U and \\mathcal X.\n\nOftentimes it is convenient to handle the constraints of the initial and final states separately: \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_i,\\ldots, \\bm u_{N-1}, \\bm x_{i},\\ldots, \\bm x_N}&\\quad \\left(\\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k) \\right)\\\\\n\\text{subject to}  &\\quad \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm u_k \\in \\mathcal U_k,\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm x_k \\in \\mathcal X_k,\\quad k=i+1, \\ldots, N-1,\\\\\n                    &\\quad \\bm x_i \\in \\mathcal X_\\mathrm{init},\\\\\n                    &\\quad \\bm x_N \\in \\mathcal X_\\mathrm{final}.\n\\end{aligned}\n\nIn particular, at the initial time just one particular state is often considered. At the final time, the state might be required to be equal to some given value, it might be required to be in some set defined through equations or inequalities, or it might be left unconstrained. Finally, the constraints on the control and states typically (but not always) come in the form of lower and upper bounds. The optimal control problem then specializes to \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bm u_i,\\ldots, \\bm u_{N-1}, \\bm x_{i},\\ldots, \\bm x_N}&\\quad \\left(\\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k) \\right)\\\\\n\\text{subject to}  &\\quad \\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\\quad k=i, \\ldots, N-1,\\\\\n                    &\\quad \\bm u_{\\min} \\leq \\bm u_k \\leq \\bm u_{\\max},\\\\\n                    &\\quad \\bm x_{\\min} \\leq \\bm x_k \\leq \\bm x_{\\max},\\\\\n                    &\\quad\\bm x_i = \\mathbf r_\\text{init},\\\\\n                    &\\quad \\left(\\bm x_N = \\mathbf r_\\text{final}, \\; \\text{or} \\; \\mathbf h_\\text{final}(\\bm x_N) =  \\mathbf 0, \\text{or} \\; \\mathbf g_\\text{final}(\\bm x_N) \\leq  \\mathbf 0\\right),                    \n\\end{aligned}\n where\n\n\\bm u_{\\min} and \\bm u_{\\max} are lower and upper bounds on the control, respectively,\n\\bm x_{\\min} and \\bm x_{\\max} are lower and upper bounds on the state, respectively,\n\\mathbf r_\\text{init} is a constant (fixed) initial state,\n\\mathbf r_\\text{final} is a constant (fixed) final state,\nand the functions \\mathbf g_\\text{final}() and \\mathbf h_\\text{final}() can be used to define the constraint set for the final state.\n\nThis optimal control problem is an instance of a general nonlinear programming (NLP) problem \n\\begin{aligned}\n\\operatorname*{minimize}_{\\bar{\\bm x}\\in\\mathbb{R}^{n(N-i)},\\bar{\\bm u}\\in\\mathbb{R}^{m(N-i)}} &\\quad J(\\bar{\\bm x},\\bar{\\bm u})\\\\\n\\text{subject to} &\\quad \\mathbf h(\\bar{\\bm x},\\bar{\\bm u}) =0,\\\\\n&\\quad \\mathbf g(\\bar{\\bm x},\\bar{\\bm u}) \\leq \\mathbf 0,\n\\end{aligned}\n where \\bar{\\bm u} and \\bar{\\bm x} are vectors obtained by stacking control and state vectors for individual times\n\n\\begin{aligned}\n\\bar{\\bm u} &= \\operatorname*{vec}(\\bm u_i,\\ldots, \\bm u_{N-1}) = \\begin{bmatrix}\\bm u_i\\\\ \\bm u_{i+1}\\\\ \\vdots \\\\ \\bm u_{N-1} \\end{bmatrix},\\\\\n\\bar{\\bm x} &= \\operatorname*{vec}(\\bm x_{i+1},\\ldots, \\bm x_N) = \\begin{bmatrix}\\bm x_{i+1}\\\\ \\bm x_{i+2}\\\\ \\vdots \\\\ \\bm x_{N} \\end{bmatrix}.\n\\end{aligned}\n\n\n\n\n\n\n\nNote\n\n\n\n\nAlthought there may be applications where it is desirable to optimize over the initial state \\bm x_i as well, mostly the initial state \\bm x_i is fixed, and it does not have to be considered as an optimization variable. This can be even emphasized through the notation J(\\bar{\\bm x},\\bar{\\bm u}; \\bm x_i), where the semicolon separates the variables from (fixed) parameters.\nThe last control that affects the state trajectory on the interval [i,N] is \\bm u_{N-1}.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "General finite-horizon nonlinear discrete-time optimal control as nonlinear program"
    ]
  },
  {
    "objectID": "discr_dir_mpc_recursive_feasibility.html",
    "href": "discr_dir_mpc_recursive_feasibility.html",
    "title": "Recursive feasibility",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "6. More on MPC",
      "Recursive feasibility"
    ]
  },
  {
    "objectID": "cont_indir_LQR_inf_horizon.html",
    "href": "cont_indir_LQR_inf_horizon.html",
    "title": "Indirect approach to LQR on an infinite horizon",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Indirect approach to LQR on an infinite horizon"
    ]
  },
  {
    "objectID": "discr_indir_DARE.html",
    "href": "discr_indir_DARE.html",
    "title": "Discrete-time algebraic Riccati equation (DARE)",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "Discrete-time algebraic Riccati equation (DARE)"
    ]
  },
  {
    "objectID": "discr_dir_mpc_solvers.html",
    "href": "discr_dir_mpc_solvers.html",
    "title": "Numerical solvers for MPC",
    "section": "",
    "text": "Essentially QP solvers with some extra features for MPC:\n\nwarmstarting requires fesibility of the previous solution. If only a fixed number of iterations is allowed (in favor of predictable timing), for some methods the iterations may temporarily lose feasibility.\n…\nqpOASES\nOSQP\nDAQP\nqpSWIFT\nProxQP\nPiQP\nECOS\nHPIPM\n\n\n\n\n Back to top",
    "crumbs": [
      "3. Discrete-time optimal control – direct approach",
      "Numerical solvers for MPC"
    ]
  },
  {
    "objectID": "opt_algo_unconstrained.html",
    "href": "opt_algo_unconstrained.html",
    "title": "Algorithms for unconstrained optimization",
    "section": "",
    "text": "Our motivation for studying numerical algorithms for unconstrained optimization remains the same as when we studied the conditions of optimality for such unconstrained problems – such algorithms constitute building blocks for constrained optimization problems. Indeed, many algorithms for constrained problems are based on reformulating the constrained problem into an unconstrained one and then applying the algorithms studied in this section.\nIt may be useful to recapitulate our motivation for studying optimization algorithms in general – after all, there are dozens of commercial or free&open-source software tools for solving optimization problems. Why not just use them? There are two answers beyond the traditional “at a grad school we should understand what we are using”:\nThere is certainly no shortage of algorithms for unconstrained optimization. In this crash course we can cover only a few. But the few we cover here certainly form a solid theoretical basis and provide practically usable tools.\nOne possible way to classify the algorithms is based on whether they use derivatives of the objective functions or not. In this course, we only consider the former approaches as they leads to more efficient algorithms. For the latter methods, we can refer to the literature (the prominent example is Nelder-Mead method).\nAll the relevant methods are iterative. Based on what happens within each iteration, we can classify them into two categories:",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_unconstrained.html#descent-methods",
    "href": "opt_algo_unconstrained.html#descent-methods",
    "title": "Algorithms for unconstrained optimization",
    "section": "Descent methods",
    "text": "Descent methods\nThe obvious quality that the search direction needs to satisfy, is that the cost function decreses along it, at least locally (for a small step length).\n\nDefinition 1 (Descent direction) At the current iterate \\bm x_k, the direction \\bm d_k is called a descent direction if \n\\nabla f(\\bm x_k)^\\top \\bm d_k &lt; 0,\n that is, the directional derivative is negative along the direction \\bm d_k.\n\nThe product above is an inner product of the two vectors \\bm d_k and \\nabla f(\\mathbf x_k). Recall that it is defined as \n\\nabla f(\\bm x_k)^\\top \\bm d_k = \\|\\nabla f(\\bm x_k)\\| \\|\\bm d_k\\| \\cos \\theta,\n where \\theta is the angle between the gradient and the search direction. This condition has a nice geometric interpretation in a contour plot for an optimization in \\mathbb R^2. Consider the line tangent to the function countour at \\bm x_k. A descent direction must be in the other half-plane generated by the tangent line than into which the gradient \\nabla f(\\bm x_k) points.\nBeware that it is only guaranteed that the cost function is reduced if the length of the step is sufficently small. For longer steps the higher-order terms in the Taylor’s series approximation of the cost function can dominate.\nBefore we proceed to the question of which descent direction to choose and how to find it, we adress the question of how far to go along the chosen direction. This is the problem of line search.\n\nStep length determination (aka line search)\nNote that once the search direction has been fixed (whether we used the negative of the gradient or any other descent direction), the problem of finding the step length \\alpha_k is just a scalar optimization problem. It turns out, however, that besides finding the true minimum along the search directions, it is often sufficient to find the minimum only approximately, or not aiming at minimization at all and work with a fixed step length instead.\n\nFixed length of the step\nHere we give a guidance on the choice of the lenght of the step. But we need to introduce a useful concept first.\n\nDefinition 2 (L-smoothness) For a continuously differentiable function f, the gradient \\nabla f is said to be L-smooth if there exists a constant L&gt;0 such that \n\\|\\nabla f(x) - \\nabla f(y)\\| \\leq L \\|x-y\\|.\n\n\nNot that if the second derivatives exist, L is an upper bound on the norm of the Hessian \n\\|\\nabla^2 f\\|\\leq L.\n\nFor quadratic functions, L is the largest eigenvalue of the Hessian \nL = \\max_i \\lambda_i (\\mathbf Q).\n\nThe usefulness of the concept of L-smoothness is that it provides a quadratic function that serves as an upper bound for the original function. This is formulated as the following lemma.\n\nLemma 1 (Descent lemma) Consider an L-smooth function f. Then for any \\mathbf x_k and \\mathbf x_{k+1}, the following inequality holds \nf(\\mathbf x_{k+1}) \\leq  f(\\mathbf x_{k}) + \\nabla f(\\mathbf x_k)^\\top (\\mathbf x_{k-1}-\\mathbf x_{k}) + \\frac{L}{2}\\|\\mathbf x_{k-1}-\\mathbf x_{k}\\|^2\n\n\nWhat implication does the result have on the determination of the step length?\n\n\\alpha = \\frac{1}{L}\n\n\n\nExact line search\nA number of methos exist: bisection, golden section, Newton, As finding true minium in each iteration is often too computationally costly and hardly needed, we do not have them here. The only exception the Newton’s method, which for vector variables constitutes another descent method on its own and we cover it later.\n\nExample 1 Here we develop a solution for an exact minimization of a quadratic functions f(\\bm x) = \\frac{1}{2} \\bm x^\\top\\mathbf Q \\bm x + \\mathbf c^\\top \\bm x along a given direction. We show that it leads to a closed-form formula. Although not particularly useful in practice, it is a good exercise in understanding the problem of line search. Furthermore, we will use it later to demonstrate the behaviour of the steepest descent method. The problem is to \\operatorname*{minimize}_{\\alpha_k} f(\\bm x_k + \\alpha_k \\bm d_k). We express the cost as a function of the current iterate, the direction, and step length. \n\\begin{aligned}\nf(\\bm x_k + \\alpha_k \\bm d_k) &= \\frac{1}{2}(\\bm x_k + \\alpha_k\\bm d_k)^\\top\\mathbf Q (\\bm x_k + \\alpha_k\\bm d_k) +\\mathbf c^\\top(\\bm x_k + \\alpha_k\\bm d_k)\\\\\n&= \\frac{1}{2} \\bm x_k^\\top\\mathbf Q \\bm x_k + \\bm d_k^\\top\\mathbf Q\\bm x_k \\alpha_k + \\frac{1}{2} \\bm d_k^\\top\\mathbf Q\\bm d_k \\alpha_k^2+ \\mathbf c^\\top(\\bm x_k + \\alpha_k\\bm d_k).\n\\end{aligned}\n\nDifferentiating the function with respect to the length of the step, we get \n\\frac{\\mathrm{d}f(\\bm x_k + \\alpha_k\\bm d_k)}{\\mathrm{d}\\alpha_k} = \\bm d_k^\\top \\underbrace{(\\mathbf Q\\bm x_k + \\mathbf c)}_{\\nabla f(\\bm x_k)} + \\bm d_k^\\top\\mathbf Q\\bm d_k \\alpha_k.\n\nAnd now setting the derivative to zero, we find the optimal step length \n\\boxed{\n\\alpha_k = -\\frac{\\bm d_k^\\top \\nabla f(\\bm x_k)}{\\bm d_k^\\top\\mathbf Q\\bm d_k} = -\\frac{\\bm d_k^\\top (\\mathbf Q\\bm x_k + \\mathbf c)}{\\bm d_k^\\top\\mathbf Q\\bm d_k}.}\n\n\n\n\nApproximate line search – backtracking\nThere are several methods for approximate line search. Here we describe the backtracking algorithm, which is based on the sufficient decrease condition (also known as Armijo condition), which reads \nf(\\bm x_k+\\alpha_k\\bm d_k) - f(\\bm x_k) \\leq \\gamma \\alpha_k \\mathbf d^T \\nabla f(\\bm x_k),\n where \\gamma\\in(0,1), typically \\gamma is very small, say \\gamma = 10^{-4}.\nThe term on the right can be be viewed as a linear function of \\alpha_k. Its negative slope is a bit less steep than the directional derivative of the function f at \\bm x_k. The condition of sufficient decrease thus requires that the cost function (as a function of \\alpha_k) is below the graph of this linear function.\nNow, the backtracking algorithm is parameterized by three parameters: the initial step lenght \\alpha_0&gt;0, the typically very small \\gamma\\in(0,1) that parameterizes the Armijo condition, and yet another parameter \\beta\\in(0,1).\nThe k-th iteration of the algorithm goes like this: failure of the sufficient decrease condition for a given \\alpha_k, or, equivalently satisfaction of the condition \nf(\\bm x_k) - f(\\bm x_k+\\alpha_k\\bm d_k) &lt; -\\gamma \\alpha_k \\mathbf d^T \\nabla f(\\bm x_k)\n sends the algorithm into another reduction of \\alpha_k by \\alpha_k = \\beta\\alpha_k. A reasonable choice for \\beta is 0.5, which corresponds to halving the step length upon failure to decrease sufficiently.\nThe backtracking algorithm can be implemented as follows\n\n\nShow the code\nfunction backtracking_line_search(f, ∇fₖ, xₖ, dₖ; α₀=1.0, β=0.5, γ=0.1)\n    αₖ = α₀\n    while f(xₖ)-f(xₖ+αₖ*dₖ) &lt; -γ*αₖ*dot(dₖ,∇fₖ)\n        αₖ *= β\n    end\n    return αₖ\nend\n\n\nbacktracking_line_search (generic function with 1 method)\n\n\nNow we are ready to proceed to the question of choosing a descent direction.\n\n\n\nSteepest descent (aka gradient descent) method\nA natural candidate for a descent direction is the negative of the gradient \n\\bm d_k = -\\nabla f(\\bm x_k).\n\nIn fact, among all descent directions, this is the one for which the descent is steepest (the gradient determines the direction of steepest ascent), though we will see later that this does not mean that the convergence of the method is the fastest.\nIn each iteration of the gradient method, this is the how the solution is updated\n\n\\boxed{\n\\bm x_{k+1} = \\bm x_{k} - \\alpha_k \\nabla f(\\bm x_{k}),}\n where determinatio of the step length \\alpha_k has already been discussed in the prevous section.\nLet’s now examine the behaviour of the method by applying it to minimization of a quadratic function. Well, for a quadratic function it is obviously an overkill, but we use it in the example because we can compute the step lenght exactly, which then helps the methods show its best.\n\nExample 2  \n\n\nShow the code\nusing LinearAlgebra         # For dot() function.\nusing Printf                # For formatted output.\n\nx0 = [2, 3]                 # Initial vector.\nQ = [1 0; 0 3]              # Positive definite matrix defining the quadratic form.\nc = [1, 2]                   # Vector defining the linear part.\n\nxs = -Q\\c                   # Stationary point, automatically the minimizer for posdef Q. \n\nϵ  = 1e-5                   # Threshold on the norm of the gradient.\nN  = 100;                   # Maximum number of steps .\n\nfunction gradient_descent_quadratic_exact(Q,c,x0,ϵ,N)\n    x = x0\n    iter = 0\n    f = 1/2*dot(x,Q*x)+dot(x,c)\n    ∇f = Q*x+c\n    while (norm(∇f) &gt; ϵ)\n        α = dot(∇f,∇f)/dot(∇f,Q*∇f)\n        x = x - α*∇f\n        iter = iter+1\n        f = 1/2*dot(x,Q*x)+dot(x,c)\n        ∇f = Q*x+c\n        @printf(\"i = %3d   ||∇f(x)|| = %6.4e   f(x) = %6.4e\\n\", iter, norm(∇f), f)\n        if iter &gt;= N\n            return f,x\n        end\n    end\n    return f,x\nend\n\nfopt,xopt = gradient_descent_quadratic_exact(Q,c,x0,ϵ,N)\n\n\ni =   1   ||∇f(x)|| = 2.0229e+00   f(x) = 7.8495e-01\ni =   2   ||∇f(x)|| = 9.0210e-01   f(x) = -1.0123e+00\ni =   3   ||∇f(x)|| = 1.6005e-01   f(x) = -1.1544e+00\ni =   4   ||∇f(x)|| = 7.1374e-02   f(x) = -1.1657e+00\ni =   5   ||∇f(x)|| = 1.2663e-02   f(x) = -1.1666e+00\ni =   6   ||∇f(x)|| = 5.6470e-03   f(x) = -1.1667e+00\ni =   7   ||∇f(x)|| = 1.0019e-03   f(x) = -1.1667e+00\ni =   8   ||∇f(x)|| = 4.4679e-04   f(x) = -1.1667e+00\ni =   9   ||∇f(x)|| = 7.9269e-05   f(x) = -1.1667e+00\ni =  10   ||∇f(x)|| = 3.5350e-05   f(x) = -1.1667e+00\ni =  11   ||∇f(x)|| = 6.2718e-06   f(x) = -1.1667e+00\n\n\n(-1.1666666666479069, [-0.9999939492423319, -0.6666672167355456])\n\n\nWe can also decorate the code a bit to visualize how the iterations proceeded.\n\n\nShow the code\nfunction gradient_descent_quadratic_exact_decor(Q,c,x0,ϵ,N)\n    x = x0\n    X = x\n    f = 1/2*dot(x,Q*x)+dot(x,c)\n    F = [f,]\n    ∇f = Q*x+c\n    iter = 0\n    while (norm(∇f) &gt; ϵ)\n        α = dot(∇f,∇f)/dot(∇f,Q*∇f)\n        x = x - α*∇f\n        iter = iter+1\n        f = 1/2*dot(x,Q*x)+dot(x,c)\n        ∇f = Q*x+c\n        X = hcat(X,x)\n        push!(F,f)\n        if iter &gt;= N\n         return F,X\n        end\n    end\n    return F,X\nend\n\nF,X = gradient_descent_quadratic_exact_decor(Q,c,x0,ϵ,N)\n\nx1_data = x2_data = -4:0.01:4;\nf(x) = 1/2*dot(x,Q*x)+dot(x,c)\nz_data = [f([x1,x2]) for x2=x2_data, x1=x1_data];\n\nusing Plots\ncontour(x1_data,x2_data,z_data)\nplot!(X[1,:],X[2,:],label=\"xk\",marker=:diamond,aspect_ratio=1)\nscatter!([x0[1],],[x0[2],],label=\"x0\")\nscatter!([xs[1],],[xs[2],],label=\"xopt\")\nxlabel!(\"x1\");ylabel!(\"x2\");\nxlims!(-4,4); ylims!(-4,4)\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Zigzagging of the steepest descent method for a quadratic function\n\n\n\n\n\nAltough the number of iterations in the above example is acceptable, a major characteristic of the method is visible. Its convergence is slowing down as we are approaching a local minimum, which is visually recognizable oscillations or zig-zagging. But it can be much worse for some data.\n\nGradient method converges slowly for ill-conditioned problems\n\nExample 3 Consider minimization of the following cost function f(\\bm x) = 1000x_1^2 + 40x_1x_2 + x_2^2.\n\n\nShow the code\nQ = [1000 20; 20 1]\nc = [0, 0]\nx0 = [1,1000]\n\nF,X = gradient_descent_quadratic_exact(Q,c,x0,ϵ,N)\n\n\ni =   1   ||∇f(x)|| = 5.9951e+02   f(x) = 2.9939e+05\ni =   2   ||∇f(x)|| = 1.2093e+04   f(x) = 1.7221e+05\ni =   3   ||∇f(x)|| = 3.4484e+02   f(x) = 9.9052e+04\ni =   4   ||∇f(x)|| = 6.9560e+03   f(x) = 5.6974e+04\ni =   5   ||∇f(x)|| = 1.9835e+02   f(x) = 3.2771e+04\ni =   6   ||∇f(x)|| = 4.0011e+03   f(x) = 1.8850e+04\ni =   7   ||∇f(x)|| = 1.1409e+02   f(x) = 1.0842e+04\ni =   8   ||∇f(x)|| = 2.3014e+03   f(x) = 6.2364e+03\ni =   9   ||∇f(x)|| = 6.5623e+01   f(x) = 3.5872e+03\ni =  10   ||∇f(x)|| = 1.3237e+03   f(x) = 2.0633e+03\ni =  11   ||∇f(x)|| = 3.7746e+01   f(x) = 1.1868e+03\ni =  12   ||∇f(x)|| = 7.6141e+02   f(x) = 6.8264e+02\ni =  13   ||∇f(x)|| = 2.1711e+01   f(x) = 3.9265e+02\ni =  14   ||∇f(x)|| = 4.3796e+02   f(x) = 2.2585e+02\ni =  15   ||∇f(x)|| = 1.2488e+01   f(x) = 1.2991e+02\ni =  16   ||∇f(x)|| = 2.5191e+02   f(x) = 7.4722e+01\ni =  17   ||∇f(x)|| = 7.1831e+00   f(x) = 4.2980e+01\ni =  18   ||∇f(x)|| = 1.4490e+02   f(x) = 2.4722e+01\ni =  19   ||∇f(x)|| = 4.1317e+00   f(x) = 1.4220e+01\ni =  20   ||∇f(x)|| = 8.3344e+01   f(x) = 8.1791e+00\ni =  21   ||∇f(x)|| = 2.3765e+00   f(x) = 4.7046e+00\ni =  22   ||∇f(x)|| = 4.7939e+01   f(x) = 2.7061e+00\ni =  23   ||∇f(x)|| = 1.3670e+00   f(x) = 1.5565e+00\ni =  24   ||∇f(x)|| = 2.7574e+01   f(x) = 8.9529e-01\ni =  25   ||∇f(x)|| = 7.8627e-01   f(x) = 5.1497e-01\ni =  26   ||∇f(x)|| = 1.5861e+01   f(x) = 2.9621e-01\ni =  27   ||∇f(x)|| = 4.5226e-01   f(x) = 1.7038e-01\ni =  28   ||∇f(x)|| = 9.1229e+00   f(x) = 9.7999e-02\ni =  29   ||∇f(x)|| = 2.6014e-01   f(x) = 5.6369e-02\ni =  30   ||∇f(x)|| = 5.2474e+00   f(x) = 3.2423e-02\ni =  31   ||∇f(x)|| = 1.4963e-01   f(x) = 1.8649e-02\ni =  32   ||∇f(x)|| = 3.0183e+00   f(x) = 1.0727e-02\ni =  33   ||∇f(x)|| = 8.6065e-02   f(x) = 6.1701e-03\ni =  34   ||∇f(x)|| = 1.7361e+00   f(x) = 3.5490e-03\ni =  35   ||∇f(x)|| = 4.9504e-02   f(x) = 2.0414e-03\ni =  36   ||∇f(x)|| = 9.9859e-01   f(x) = 1.1742e-03\ni =  37   ||∇f(x)|| = 2.8475e-02   f(x) = 6.7539e-04\ni =  38   ||∇f(x)|| = 5.7439e-01   f(x) = 3.8848e-04\ni =  39   ||∇f(x)|| = 1.6378e-02   f(x) = 2.2345e-04\ni =  40   ||∇f(x)|| = 3.3038e-01   f(x) = 1.2853e-04\ni =  41   ||∇f(x)|| = 9.4207e-03   f(x) = 7.3928e-05\ni =  42   ||∇f(x)|| = 1.9003e-01   f(x) = 4.2523e-05\ni =  43   ||∇f(x)|| = 5.4188e-03   f(x) = 2.4459e-05\ni =  44   ||∇f(x)|| = 1.0931e-01   f(x) = 1.4069e-05\ni =  45   ||∇f(x)|| = 3.1168e-03   f(x) = 8.0922e-06\ni =  46   ||∇f(x)|| = 6.2873e-02   f(x) = 4.6546e-06\ni =  47   ||∇f(x)|| = 1.7928e-03   f(x) = 2.6773e-06\ni =  48   ||∇f(x)|| = 3.6164e-02   f(x) = 1.5400e-06\ni =  49   ||∇f(x)|| = 1.0312e-03   f(x) = 8.8578e-07\ni =  50   ||∇f(x)|| = 2.0801e-02   f(x) = 5.0949e-07\ni =  51   ||∇f(x)|| = 5.9314e-04   f(x) = 2.9306e-07\ni =  52   ||∇f(x)|| = 1.1965e-02   f(x) = 1.6856e-07\ni =  53   ||∇f(x)|| = 3.4117e-04   f(x) = 9.6958e-08\ni =  54   ||∇f(x)|| = 6.8821e-03   f(x) = 5.5769e-08\ni =  55   ||∇f(x)|| = 1.9624e-04   f(x) = 3.2078e-08\ni =  56   ||∇f(x)|| = 3.9585e-03   f(x) = 1.8451e-08\ni =  57   ||∇f(x)|| = 1.1288e-04   f(x) = 1.0613e-08\ni =  58   ||∇f(x)|| = 2.2769e-03   f(x) = 6.1045e-09\ni =  59   ||∇f(x)|| = 6.4925e-05   f(x) = 3.5113e-09\ni =  60   ||∇f(x)|| = 1.3097e-03   f(x) = 2.0197e-09\ni =  61   ||∇f(x)|| = 3.7345e-05   f(x) = 1.1617e-09\ni =  62   ||∇f(x)|| = 7.5331e-04   f(x) = 6.6821e-10\ni =  63   ||∇f(x)|| = 2.1480e-05   f(x) = 3.8435e-10\ni =  64   ||∇f(x)|| = 4.3330e-04   f(x) = 2.2107e-10\ni =  65   ||∇f(x)|| = 1.2355e-05   f(x) = 1.2716e-10\ni =  66   ||∇f(x)|| = 2.4923e-04   f(x) = 7.3142e-11\ni =  67   ||∇f(x)|| = 7.1068e-06   f(x) = 4.2071e-11\n\n\n(4.207097012941924e-11, [-2.371876542980443e-7, 1.1842143766107573e-5])\n\n\nWhile for the previous problem of the same kind and size the steepest descent method converged in just a few steps, for this particular data it takes many dozens of steps.\nThe culprit here are bad properties of the Hessian matrix Q. By ``bad properties’’ we mean the so-called , which is reflected in the very high . Recall that condition number \\kappa for a given matrix \\mathbf A is defined as \n\\kappa(\\mathbf A) = \\|\\mathbf A^{-1}\\|\\cdot \\|\\mathbf A\\|\n and it can be computed as ratio of the largest and smallest singular values, that is, \n\\kappa(\\mathbf A) = \\frac{\\sigma_{\\max}(\\mathbf A)}{\\sigma_{\\min}(\\mathbf A)}.\n\\end{equation}$$\nIdeally this number should be around 1. In the example above it is\n\n\nShow the code\ncond(Q)\n\n\n1668.0010671466664\n\n\nwhich is well above 1000. Is there anything that we can do about it? The answer is yes. We can scale the original date to improve the conditioning.\n\n\n\nScaled gradient method for ill-conditioned problems\nUpon introducing a matrix \\mathbf S that relates the original vector variable \\bm x with a new vector variable \\bm y according to \n\\bm x = \\mathbf S \\bm y,\n the optimization cost function changes from f(\\bm x) to f(\\mathbf S \\bm y). Let’s relabel the latter to g(\\bm y). And we will now examine how the steepest descent iteration changes. Straightforward application of a chain rule for finding derivatives of composite functions yields \ng'(\\bm y) = f'(\\mathbf S\\bm y) = f'(\\mathbf S\\bm y)\\mathbf S.\n\nKeeping in mind that gradients are transposes of derivatives, we can write \n\\nabla g(\\bm y) = \\mathbf S^\\top \\nabla f(\\mathbf S\\bm y).\n\nSteepest descent iterations then change accordingly\n\n\\begin{aligned}\n\\mathbf y_{k+1} &= \\mathbf y_k - \\alpha_k \\nabla g(\\mathbf y_k)\\\\\n\\mathbf y_{k+1} &= \\mathbf y_k - \\alpha_k \\mathbf S^T\\nabla f(\\mathbf S \\mathbf y_k)\\\\\n\\underbrace{\\mathbf S \\mathbf y_{k+1}}_{\\mathbf x_{k+1}} &= \\underbrace{\\mathbf S\\mathbf y_k}_{\\mathbf x_k} - \\alpha_k \\underbrace{\\mathbf S \\mathbf S^T}_{\\mathbf D}\\nabla f(\\underbrace{\\mathbf S \\mathbf y_k}_{\\mathbf x_k}).\n\\end{aligned}\n\nUpon defining the scaling matrix \\mathbf D as \\mathbf S \\mathbf S^T, a single iteration changes to \n\\boxed{\\bm x_{k+1} = \\bm x_{k} - \\alpha_k \\mathbf D_k\\nabla f(\\bm x_{k}).}\n\nThe question now is: how to choose the matrix \\mathbf D? We would like to make the Hessian matrix \\nabla^2 f(\\mathbf S \\bm y) (which in the case of a quadratic matrix form is the matrix \\mathbf Q as we used it above) better conditioned. Ideally, \\nabla^2 f(\\mathbf S \\bm y)\\approx \\mathbf I.\nA simple way for improving the conditioning is to define the scaling matrix \\mathbf D as a diagonal matrix whose diagonal entries are given by \n\\mathbf D_{ii} = [\\nabla^2 f(\\bm x_k)]^{-1}_{ii}.\n\nIn words, the diagonal entries of the Hessian matrix are inverted and they then form the diagonal of the scaling matrix.\nIt is worth emphasizing how the algorithm changed: the direction of steepest descent (the negative of the gradient) is premultiplied by some (scaling) matrix. We will see in a few moments that another method – Newton’s method – has a perfectly identical structure.\n\n\n\nNewton’s method\nNewton’s method is one of flagship algorithms in numerical computing. I am certainly not exaggerating if I include it in my personal Top 10 list of algorithms relevant for engineers. We may encounter the method in two settings: as a method for solving (sets of) nonlinear equations and as a method for optimization. The two are inherently related and it is useful to be able to see the connection.\n\nNewton’s method for rootfinding\nThe problem to be solved is that of finding x for which a given function g() vanishes. In other words, we solve the following equation \ng(x) = 0.\n\nThe above state scalar version has also its vector extension \n\\mathbf g(\\bm x) = \\mathbf 0,\n in which \\bm x stands for an n-tuple of variables and \\mathbf g() actually stands for an n-tuple of functions. Even more general version allows for different number of variables and equations.\nWe start with a scalar version. A single iteration of the method evaluates not only the value of the function g(x_k) at the given point but also its derivative g'(x_k). It then uses the two to approximate the function g() at x_k by a linear (actually affine) function and computes the intersection of this approximating function with the horizontal axis. This gives as x_{k+1}, that is, the (k+1)-th approximation to a solution (root). We can write this down as \n\\begin{aligned}\n\\underbrace{g(x_{k+1})}_{0} &= g(x_{k}) + g'(x_{k})(x_{k+1}-x_k)\\\\\n0 &= g(x_{k}) + g'(x_{k})x_{k+1}-g'(x_{k})x_k),\n\\end{aligned}\n from which the famous formula follows \n\\boxed{x_{k+1} = x_{k} - \\frac{g(x_k)}{g'(x_k)}.}\n\nIn the vector form, the formula is \n\\boxed{\\bm x_{k+1} = \\bm x_{k} - [\\nabla \\mathbf g(\\bm x_k)^\\top]^{-1}\\mathbf g(\\bm x_k),}\n where \\nabla \\mathbf g(\\bm x_k)^\\top is the (Jacobian) matrix of the first derivatives of \\mathbf g at \\bm x_k, that is, \\nabla \\mathbf g() is a matrix with the gradient of the g_i(\\bm x) function in its i-th column.\n\n\nNewton’s method for optimization\nOnce again, restrict ourselves to a scalar case first. The problem is \n\\operatorname*{minimize}_{x\\in\\mathbb{R}}\\quad f(x).\n\nAt the k-th iteration of the algorithm, the solution is x_k. The function to be minimized is approximated by a quadratic function m_k() in x. In order to find parameterization of this quadratic function, the function f() but also its first and second derivatives, f'() and f''(), respectively, need to be evaluated at x_k. Using these three, a function m_k(x) approximating f(x) at some x not too far from x_k can be defined \nm_k(x) = f(x_k) + f'(x_k)(x-x_k) + \\frac{1}{2}f''(x_k)(x-x_k)^2.\n\nThe problem of minimizing this new function in the k-th iteration is then formulated, namely,\n\n\\operatorname*{minimize}_{x_{k+1}\\in\\mathbb{R}}\\quad m_k(x_{k+1})\n and solved for some x_{k+1}. The way to find this solution is straightforward: find the derivative of m_k() and find the value of x_{k+1} for which this derivative vanishes. The result is \n\\boxed{x_{k+1} = x_{k} - \\frac{f'(x_k)}{f''(x_k)}.}\n\nThe vector version of the Newton’s step is \n\\boxed{\\bm x_{k+1} = \\bm x_{k} - [\\nabla^2 f(\\bm x_k)]^{-1} \\nabla f(\\bm x_k).}\n\nA few observations\n\nIf compared to the general prescription for descent direction methods (), the Newton’s method determines the direction and the step lenght at once (both \\alpha_k and \\mathbf d_k are hidden in - [\\nabla^2 f(\\mathbf x_k)]^{-1} \\nabla f(\\mathbf x_k)).\nIf compared with steepest descent (gradient) method, especially with its scaled version (), Newton’s method fits into the framework nicely because the inverse [\\nabla^2 f(\\mathbf x_k)]^{-1} of the Hessian can be regarded as a kind of a scaling matrix \\mathbf D. Indeed, you can find arguments in some textbooks that Newton’s method involves scaling that is optimal in some sense. We skip the details here because we only wanted to highlight the similarity in the structure of the two methods.\n\nThe great popularity of Newton’s method is mainly due to its nice convergence – quadratic. Although we skip any discussion of convergence rates here, note that for all other methods this is an ideal that aims to be approached.\nThe nice convergence rate of Newton’s method is compensated by a few disadvantages\n\nThe need to compute the Hessian. This is perhaps not quite clear with simple problems but can play some role with huge problems.\nOnce the Hessian is computed, it must be inverted (actually, a linear system must by solved). But this assumes that Hessian is nonsingular. How can we guarantee this for a given problem?\nIt is not only that Hessian must be nonsingular but it must also be positive (definite). Note that in the scalar case this corresponds to the situation when the second derivative is positive. Negativeness of the second derivative can send the algorithm in the opposite direction – away from the local minimum – , which which would ruin the convergence of the algorithm.\n\nThe last two issues are handled by some modification of the standard Newton’s method\n\nDamped Newton’s method\nA parameter \\alpha\\in(0,1) is introduced that shortens the step as in \n  \\bm x_{k+1} = \\bm x_{k} - \\alpha(\\nabla^2 f(\\bm x_k))^{-1} \\nabla f(\\bm x_k).\n\n\n\nFixed constant positive definite matrix instead of the inverse of the Hessian\nThe step is determined as \n  \\bm x_{k+1} = \\bm x_{k} - \\mathbf B \\nabla f(\\bm x_k).\n\nNote that the interpretation of the constant \\mathbf B in the position of the (inverse of the) Hessian in the rootfinding setting is that the slope of the approximating linear (affine) function is always constant.\nNow that we admitted to have something else then just the (inverse of the) Hessian in the formula for Newton’s method, we can explore further this new freedom. This will bring us into a family of methods called Quasi-Newton methods.\n\n\n\n\nQuasi-Newton’s methods\n…\n\n\nConjugate gradient method\n…",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for unconstrained optimization"
    ]
  },
  {
    "objectID": "opt_algo_unconstrained.html#trust-region-methods",
    "href": "opt_algo_unconstrained.html#trust-region-methods",
    "title": "Algorithms for unconstrained optimization",
    "section": "Trust region methods",
    "text": "Trust region methods\n…",
    "crumbs": [
      "2. Optimization – algorithms",
      "Algorithms for unconstrained optimization"
    ]
  },
  {
    "objectID": "rocond_H_infinity_control.html",
    "href": "rocond_H_infinity_control.html",
    "title": "Hinfinity-optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "12. Robust control",
      "Hinfinity-optimal control"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "B(E)3M35ORR – Optimal and Robust Control",
    "section": "",
    "text": "This website constitutes the (work-in-progress) lecture notes for the graduate course Optimal and Robust Control (B3M35ORR, BE3M35ORR) taught within Cybernetics and Robotics graduate program at Faculty of Electrical Engineering, Czech Technical University in Prague, Czechia.\nEach chapter corresponds to a single weekly block (covered by a lecture, a seminar/exercise, and some homework), hence 14 chapters in total.\nOrganizational instructions, description of grading policy, assignments of homework problems and other course related material relevant for officially enrolled students are published on the course page within the FEL Moodle system.\n\n\n\n Back to top"
  },
  {
    "objectID": "cont_dir_multiple_shooting.html",
    "href": "cont_dir_multiple_shooting.html",
    "title": "Multiple shooting method for optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "9. Continuous-time optimal control - direct approach via numerical optimization",
      "Multiple shooting method for optimal control"
    ]
  },
  {
    "objectID": "dynamic_programming.html",
    "href": "dynamic_programming.html",
    "title": "Optimal control via dynamic programming",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "5. Discrete-time optimal control – dynamic programming",
      "Optimal control via dynamic programming"
    ]
  },
  {
    "objectID": "cont_indir_Pontryagin.html",
    "href": "cont_indir_Pontryagin.html",
    "title": "Pontryagin’s maximum principle",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Pontryagin's maximum principle"
    ]
  },
  {
    "objectID": "cont_indir_constrained.html",
    "href": "cont_indir_constrained.html",
    "title": "Constrained optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "8. Continuous-time optimal control - indirect approach via Pontryagin's maximum principle",
      "Constrained optimal control"
    ]
  },
  {
    "objectID": "discr_indir_general.html",
    "href": "discr_indir_general.html",
    "title": "General nonlinear discrete-time optimal control",
    "section": "",
    "text": "While in the previous chapter we formulated an optimal control problem (OCP) directly as a mathematical programming (general NLP or even QP) problem over the control (and possibly state) trajectories, in this chapter we introduce an alternative – indirect – approach. The essence of the approach is that we formulate first-order necessary conditions of optimality for the OCP in the form of equations, and then solve these. Although less straightforward to extend with additional constraints than the direct approach, the indirect approach also exhibits some advantages. In particular, in some cases (such as a quadratic cost and a linear system) it yields a feedback controller and not just a control trajetory.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General nonlinear discrete-time optimal control"
    ]
  },
  {
    "objectID": "discr_indir_general.html#optimization-constrains-given-only-by-the-state-equations",
    "href": "discr_indir_general.html#optimization-constrains-given-only-by-the-state-equations",
    "title": "General nonlinear discrete-time optimal control",
    "section": "Optimization constrains given only by the state equations",
    "text": "Optimization constrains given only by the state equations\nAs in the chapter on the direct approach, here we also start by considering a general nonlinear and possibly time-varying discrete-time dynamical system characterized by the state vector \\bm x_k\\in\\mathbb R^n whose evolution in discrete time k is uniquely determined by the state equation \n\\bm x_{k+1} = \\mathbf f_k(\\bm x_k,\\bm u_k),\n accompanied by the initial state (vector) \\bm x_i\\in\\mathbb R^n and a sequence of control inputs \\bm u_i, \\bm u_{i+1}, \\ldots, \\bm u_{k-1}, where the control variable can also be a vector, that is, \\bm u_k \\in \\mathbb R^m.\nThese state equations will constitute the only constraints of the optimization problem. Unlike in the direct approach, here in our introductory treatment we do not impose any inequality constraints such as bounds on the control inputs, because the theory to be presented is not able to handle them.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General nonlinear discrete-time optimal control"
    ]
  },
  {
    "objectID": "discr_indir_general.html#general-additive-cost-function",
    "href": "discr_indir_general.html#general-additive-cost-function",
    "title": "General nonlinear discrete-time optimal control",
    "section": "General additive cost function",
    "text": "General additive cost function\nFor the above described dynamical system we want to find a control sequence \\bm u_k that minimizes a suitable optimization criterion over a finite horizon k\\in[i,N]. Namely, we will look for a control that minimizes a criterion of the following kind \nJ_i^N(\\underbrace{\\bm x_{i+1}, \\bm x_{i+2}, \\ldots, \\bm x_{N}}_{\\bar{\\bm x}}, \\underbrace{\\bm u_{i}, \\ldots, \\bm u_{N-1}}_{\\bar{\\bm u}};\\bm x_i) = \\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1} L_k(\\bm x_k,\\bm u_k).\n\\tag{1}\n\n\n\n\n\n\nNote\n\n\n\nRegarding the notation J_i^N(\\cdot) for the cost, if the initial and final times are understood from the context, they do not have to be displayed. But we will soon need to indicate the initial time explicitly in our derivations.\n\n\nThe property of the presented cost function that will turn out crucial in our subsequent work is that is additive over the time horizon. Although this restricts the class of cost functions a bit, it is still general enough to encompass a wide range of problems, such as minimizing the total (financial) cost to be paid, the total energy to be used, the total distance to be travelled, the cumulative error to be minimized, etc.\nHere is a list of a few popular cost functions.\n\nMinimum-time (or time-optimal) problem\n\nSetting \\phi=1 and L_k=1 gives J=N-i, that is, the length of the time horizon, the duration of control. Altough in this course we do not introduce concepts and tools for optimization over integer variables, in this simple case of just a single integer variable even a simple search over the length of control interval will be computationally tractable. Furthermore, as we will see in one of the next chapters once we switch from discrete-time to continuous-time systems, this time-optimal control design problem will turn out tractable using the tools presented in this course.\n\nMinimum-fuel problem\n\nSetting \\phi=0 and L_k=|u_k|, which gives J=\\sum_{k=i}^{N-1}|u_k|.\n\nMinimum-energy problem\n\nSetting \\phi=0 and L_k=\\frac{1}{2} u_k^2, which gives J=\\frac{1}{2} \\sum_{k=i}^{N-1} u_k^2. It is fair to admit that this sum of squared inputs cannot always be interpretted as the energy – for instance, what if the control input is a degree of openning of a valve? Sum of angles over time can hardly be interpreted as energy. Instead, it should be interpretted in the mathematical way as the (squared) norm, that is, a “size” of the input. Note that the same objection can be given to the previous case of a minimum-fuel problem.\n\nMixed quadratic problem (also LQ-optimal control problem)\n\nSetting \\phi=\\frac{1}{2}s_N x_N^2 and L_k=\\frac{1}{2} (qx_k^2+ru_k^2),\\, q,r\\geq 0, which gives J=\\frac{1}{2}s_Nx_N^2+\\frac{1}{2} \\sum_{k=i}^{N-1} (r x_k^2+q u_k^2). Or in the case of vector state and control variables J=\\frac{1}{2}\\bm x_N^\\top \\mathbf S_N \\bm x_N+\\frac{1}{2} \\sum_{k=i}^{N-1} (\\bm x_k^\\top \\mathbf Q \\bm x_k + \\bm u_k^\\top \\mathbf R \\bm u_k), \\, \\mathbf Q, \\mathbf R \\succeq 0. This type of an optimization cost is particularly popular. Both for the mathematical reasons (we all now appreciate the nice properties of quadratic functions) and for practical engineering reasons as it allows us to capture a trade-off between the control performance (penalty on \\bm x_k) and control effort (penalty on \\bm u_k). Note also that the state at the terminal time N is penalized separately just in order to allow another trade-off between the transient and terminal behavior. The cost function can also be modified to penalize deviation of the state from some nonzero desired (aka reference) state trajectory, that is J=\\frac{1}{2}(\\bm x_N - \\bm x_N^\\text{ref})^\\top \\mathbf S_N (\\bm x_N - \\bm x_N^\\text{ref}) +\\frac{1}{2} \\sum_{k=i}^{N-1} \\left((\\bm x_k - \\bm x_k^\\text{ref})^\\top \\mathbf Q (\\bm x_k - \\bm x_k^\\text{ref}) + \\bm u_k^\\top \\mathbf R \\bm u_k\\right).\n\n\nNote that in none of these cost function did we include \\bm u_{N} as an optimization variables as it has no influence over the interval [i,N].\nIt is perhaps needless to emphasize that while in some other applications maximizing may seem more appropriate (such as maximizing the yield, bandwidth or robustness), we can always reformulate the maximization into minimization. Therefore in our course we alway formulate the optimal control problems as minimization problems.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General nonlinear discrete-time optimal control"
    ]
  },
  {
    "objectID": "discr_indir_general.html#derivation-of-the-first-order-necessary-conditions-of-optimality",
    "href": "discr_indir_general.html#derivation-of-the-first-order-necessary-conditions-of-optimality",
    "title": "General nonlinear discrete-time optimal control",
    "section": "Derivation of the first-order necessary conditions of optimality",
    "text": "Derivation of the first-order necessary conditions of optimality\nHaving formulated a finite-dimensional constrained nonlinear optimization problem, we avoid the temptation to call an NLP solver to solve it numerically and proceed instead with our own analysis of the problem. Let’s see how far we can get.\nBy introducing Lagrange multipliers {\\color{blue}\\bm\\lambda_k} we turn the constrained problem into an unconstrained one. The new cost function (we use the prime to distinguish it from the original cost) is \n{J'}_i^N(\\bm x_i, \\ldots, \\bm x_N, \\bm u_i, \\ldots, \\bm u_{N-1},{\\color{blue}\\bm \\lambda_i, \\ldots, ..., \\bm \\lambda_{N-1}}) = \\phi(\\bm x_N,N) + \\sum_{k=i}^{N-1}\\left[L_{k}(\\bm x_k,\\bm u_k)+\\bm {\\color{blue}\\lambda^\\top_{k}}\\;\\left[\\mathbf f_k(\\bm x_k,\\bm u_k)-\\bm x_{k+1}\\right]\\right].\n\nFrom now on, in principle, we do not need any guidance here, do we? We are given an unconstrained optimization problem and its solution is just a few steps away. In particular, stationary point(s) must be found (and then we are going to argue if these qualify as minimizers or not). This calls for differentiating the above expression with respect to all the variables and setting these derivatives equal to zeros.\nAlthough the principles are clear, some hindsight might be shared here if compact formulas are to be found. First such advice is to rename the variable(s) {\\color{blue}\\boldsymbol \\lambda_k} to {\\color{red}\\boldsymbol \\lambda_{k+1}} \n{J'}_i^N(\\bm x_i, \\ldots, \\bm x_N, \\bm u_i, \\ldots, \\bm u_{N-1},{\\color{red}\\bm \\lambda_{i+1}, \\ldots, ..., \\bm \\lambda_{N}}) = \\phi(N,\\bm x_N) + \\sum_{k=i}^{N-1}\\left[L_{k}(\\bm x_k,\\bm u_k)+\\boldsymbol {\\color{red}\\boldsymbol \\lambda^\\top_{k+1}}\\;\\left[\\mathbf f_k(\\bm x_k,\\bm u_k)-\\mathbf x_{k+1}\\right]\\right].\n\nThis is really just a notational decision but thanks to it our resulting formulas will enjoy some symmetry.\n\n\n\n\n\n\nNote\n\n\n\nMaybe it would be more didactic to leave you to go on without this advice notation and only then to nudge you to figure out this remedy on your own. But admittedly this is not the kind of competence that we aim at in this course. Let’s spend time with more rewarding things.\n\n\nAnother notational advice – but this one is more systematic and fundamental – is to make the above expression a bit shorter by introducing a new variable defined as \\boxed{H_k(\\bm x_k,\\bm u_k,\\boldsymbol\\lambda_{k+1}) = L_{k}(\\bm x_k,\\bm u_k)+\\boldsymbol \\lambda_{k+1}^\\top \\; \\mathbf f_k(\\bm x_k,\\bm u_k).}\n\nWe will call this new function Hamiltonian. Indeed, the choice of this name is motivated by the analogy with the equally named concept used in physics and theoretical mechanics, but we will only make more references to this analogy later in the course once we transition to continuous-time systems modelled by differential equations.\nIntroducing the Hamiltonian reformulates the cost function (and we omit the explicit dependence on all its input arguments) as \n{J'}_i^N = \\phi(N,\\bm x_N) + \\sum_{k=i}^{N-1}\\left[H_{k}(\\bm x_k,\\bm u_k,\\boldsymbol\\lambda_{k+1})-\\boldsymbol\\lambda^\\top_{k+1}\\;\\mathbf x_{k+1}\\right].\n\nThe final polishing of the expression before starting to compute the derivatives consists in bringing together the terms that contain related variables: the state \\bm x_N at the final time, the state \\bm x_i at the initial time, and the states, controls and Lagrange multipliers in the transient period\n\n{J'}_i^N = \\underbrace{\\phi(N,\\bm x_N) -\\boldsymbol\\lambda^\\top_{N}\\;\\mathbf x_{N}}_\\text{at terminal time} + \\underbrace{H_i(\\bm x_i,\\mathbf u_i,\\boldsymbol\\lambda_{i+1})}_\\text{at initial time} + \\sum_{k=i+1}^{N-1}\\left[H_{k}(\\bm x_k,\\bm u_k,\\boldsymbol\\lambda_{k+1})-\\boldsymbol\\lambda^\\top_{k}\\;\\mathbf x_{k}\\right].\n\nAlthough this step was not necessary, it will make things a bit more convenient once we start looking for the derivatives. And the time for it has just come.\nRecall now the recommended procedure for finding derivatives of functions of vectors – find the differential instead and identify the derivative in the result. The gradient is then (by convention) obtained as the transpose of the derivative. Following this derivative-identification procedure, we anticipate the differential of the augmented cost function in the following form \n\\begin{split}\n\\text{d}{J'}_i^N &= (\\qquad)^\\text{T}\\; \\text{d}\\bm x_N + (\\qquad)^\\text{T}\\; \\text{d}\\bm x_i \\\\&+ \\sum_{k=i+1}^{N-1}(\\qquad)^\\text{T}\\; \\text{d}\\bm x_k + \\sum_{k=i}^{N-1}(\\qquad)^\\text{T}\\; \\text{d}\\bm u_k + \\sum_{k=i+1}^{N}(\\qquad)^\\text{T}\\; \\text{d}\\boldsymbol \\lambda_k.\n\\end{split}\n\nIdentifying the gradients amounts to filling in the empty brackets. It straightforward if tedious (in particular the lower and upper bounds on the summation indices must be carefuly checked). The solution is \n\\begin{split}\n\\text{d}{J'}_i^N &= \\left(\\nabla_{\\bm x_N}\\phi-\\lambda_N\\right)^\\text{T}\\; \\text{d}\\bm x_N + \\left(\\nabla_{\\bm x_i}H_i\\right)^\\text{T}\\; \\text{d}\\bm x_i \\\\&+ \\sum_{k=i+1}^{N-1}\\left(\\nabla_{\\bm x_k}H_k-\\boldsymbol\\lambda_k\\right)^\\text{T}\\; \\text{d}\\bm x_k + \\sum_{k=i}^{N-1}\\left(\\nabla_{\\bm u_k}H_k\\right)^\\text{T}\\; \\text{d}\\bm u_k + \\sum_{k=i+1}^{N}\\left(\\nabla_{\\boldsymbol \\lambda_k}H_{k-1}-\\bm x_k\\right)^\\text{T}\\; \\text{d}\\boldsymbol \\lambda_k.\n\\end{split}\n\nThe ultimate goal of this derivation was to find stationary points for the augmented cost function, that is, to find conditions under which \\text{d}{J'}_i^N=0. In typical optimization problems, the optimization is conducted with respect to all the participating variables, which means that the corresponding differentials may be arbitrary and the only way to guarantee that the total differential of J_i' is zeros is to make the associated gradients (the contents of the brackets) equal to zero. There are two exceptions to this rule in our case, though:\n\nThe state at the initial time is typically fixed and not available for optimization. Then \\text{d}\\bm x_i=0 and the corresponding necessary condition is replaced by the statement that \\bm x_i is equal to some particular value, say, \\bm x_i = \\mathbf x^\\text{init}. We have already discussed this before. In fact, in these situations we might even prefer to reflect it by the notation J_i^N(\\ldots;\\bm x_i), which emphasizes that \\bm x_i is a parameter and not a variable. But in the solution below we do allow for the possibility that \\bm x_i is a variable too (hence \\text{d}\\bm x_i\\neq 0) for completeness.\nThe state at the final time may also be given/fixed, in which case the corresponding condition is replaced by the statement that \\bm x_N is equal to some particular value, say, \\bm x_N = \\mathbf x^\\text{ref}. But if it is not the case, then the final state is also subject to optimization and the corresponding necessary condition of optimality is obtained by setting the content of the corresponding brackets to zero.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General nonlinear discrete-time optimal control"
    ]
  },
  {
    "objectID": "discr_indir_general.html#necessary-conditions-of-optimality-as-two-point-boundary-value-problem-tp-bvp",
    "href": "discr_indir_general.html#necessary-conditions-of-optimality-as-two-point-boundary-value-problem-tp-bvp",
    "title": "General nonlinear discrete-time optimal control",
    "section": "Necessary conditions of optimality as two-point boundary value problem (TP-BVP)",
    "text": "Necessary conditions of optimality as two-point boundary value problem (TP-BVP)\nThe ultimate form of the first-order necessary conditions of optimality, which incorporates the special cases discussed above, is given by these equations \n\\boxed{\n\\begin{aligned}\n\\mathbf x_{k+1} &= \\nabla_{\\boldsymbol\\lambda_{k+1}}H_k, \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1},\\\\\n\\boldsymbol\\lambda_k &= \\nabla_{\\bm x_k}H_k, \\;\\;\\; \\color{gray}{k=i+1,\\ldots, N-1}\\\\\n0 &=  \\nabla_{\\bm u_k}H_k, \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1}\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_N}\\phi-\\lambda_N\\right)^\\top \\mathrm{d}\\bm x_N},\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_i}H_i\\right)^\\top \\mathrm{d}\\bm x_i},\n\\end{aligned}\n}\n or more explicitly \n\\boxed{\n\\begin{aligned}\n\\mathbf x_{k+1} &= \\mathbf f_k(\\bm x_k,\\bm u_k), \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1},\\\\\n\\boldsymbol\\lambda_k &= \\nabla_{\\bm x_k}\\mathbf f_k\\;\\;   \\boldsymbol\\lambda_{\\mathbf k+1}+\\nabla_{\\bm x_k}L_k, \\;\\;\\; \\color{gray}{k=i+1,\\ldots, N-1}\\\\\n0 &=  \\nabla_{\\bm u_k}\\mathbf f_k\\;\\; \\boldsymbol\\lambda_{k+1}+\\nabla_{u_k}L_k, \\;\\;\\; \\color{gray}{k=i,\\ldots, N-1}\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_N}\\phi-\\lambda_N\\right)^\\top \\mathrm{d}\\bm x_N},\\\\\n\\color{blue}{0} &= \\color{blue}{\\left(\\nabla_{\\bm x_i}H_i\\right)^\\top \\mathrm{d}\\bm x_i}.\n\\end{aligned}\n}\n\nRecall that since \\mathbf f is a vector function, \\nabla \\mathbf f is not just a gradient but rather a matrix whose columns are gradients of the individual components of the vector \\mathbf f – it is a transpose of Jacobian.\n\n\n\n\n\n\nNote\n\n\n\nThe first three necessary conditions above can be made completely “symmetric” by running the second one from k=i because the \\boldsymbol\\lambda_i introduced this way does not influence the rest of the problem and we could easily live with one useless variable.\n\n\nWe have just derived the (necessary) conditions of optimality in the form of five sets of (vector) equations:\n\nThe first two are recursive (or recurrent or also just discrete-time) equations, which means that they introduce coupling between the variables evaluated at consecutive times. In fact, the former is just the standard state equation that gives the state at one time as a function of the state (and the control) at the previous time. The latter gives a prescription for the variable \\bm \\lambda_k as a function of (among others) the same variable evaluated at the next (!) time, that is, \\bm \\lambda_{k+1}. Although from the optimization perspective these variables play the role of Lagrange multipliers, we call them co-state variables in optimal control theory because of the way they relate to the state equations. The corresponding vector equation is called a co-state equation.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is a crucial property of the co-state equation that it dictates the evolution of the co-state variable backward in time.\n\n\n\nThe third set of equations are just algebraic equations that relate the control inputs to the state and co-state variables. Sometimes it is called a stationarity equation.\nThe last two are just single (vector) equations related to the end and the beginning of the time horizon. They are both stated in the general enough form that allows the corresponding states to be treated as either fixed or subject to optimization. In particular, if the final state is to be treated as free (subject to optimization), that is, \\mathrm{d}\\bm x_N can be atritrary and the only way the corresponding equation can be satisfied is \\nabla_{\\bm x_N}\\phi=\\lambda_N. If, on the other hand, the final state is to be treated as fixed, the the corresponding equation is just replaced by \\bm x_N = \\mathbf x^\\text{ref}. Similarly for the initial state. But as we have hinted a few times, most often than not the initial state will be regarded as fixed and not subject to optimization, in which case the corresponding equation is replaced by \\bm x_i = \\mathbf x^\\text{init}.\n\nTo summarize, the equations that give the necessary conditions of optimality for a general nonlinear discrete-time optimal control problem form a two-point boundary value problem (TP-BVP). Values of some variables are specified at the initial time, values of some (maybe the same or some other) variables are defined at the final time. The equations prescribe the evolution of some variables forward in time while for some other variables the evolution backward in time is dictated.\n\n\n\n\n\n\nNote\n\n\n\nThis is in contrast with the initial value problem (IVP) for state equations, for which we only specify the state at one end of the time horizon – the initial state – and then the state equation disctates the evolution of the (state) variable forward in time.\n\n\nBoundary value problems are notoriously difficult to solve. Typically we can only solve them numerically, in which case it is appropriate to ask if anything has been gained by this indirect procedure compared with the direct one. After all, we did not even incorporate the inequality constraints in the problem, which was a piece of case in the direct approach. But we will see that in some special cases the TP-BVP they can be solved analytically and the outcome is particularly useful and would never have been discovered, if only the direct approach had been followed. We elaborate on this in the next section.",
    "crumbs": [
      "4. Discrete-time optimal control – indirect approach",
      "General nonlinear discrete-time optimal control"
    ]
  },
  {
    "objectID": "cont_indir_calculus_of_variations.html",
    "href": "cont_indir_calculus_of_variations.html",
    "title": "Calculus of variations and optimal control",
    "section": "",
    "text": "(Kirk 2004), (Lewis, Vrabie, and Syrmo 2012), (Liberzon 2011), (Sussmann and Willems 1997)",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Calculus of variations and optimal control"
    ]
  },
  {
    "objectID": "cont_indir_calculus_of_variations.html#literature",
    "href": "cont_indir_calculus_of_variations.html#literature",
    "title": "Calculus of variations and optimal control",
    "section": "",
    "text": "(Kirk 2004), (Lewis, Vrabie, and Syrmo 2012), (Liberzon 2011), (Sussmann and Willems 1997)",
    "crumbs": [
      "7. Continuous-time optimal control - indirect approach via calculus of variations",
      "Calculus of variations and optimal control"
    ]
  },
  {
    "objectID": "limits_of_performance_SISO.html",
    "href": "limits_of_performance_SISO.html",
    "title": "Limits for SISO systems",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "13. Limits of performance",
      "Limits for SISO systems"
    ]
  },
  {
    "objectID": "robustness.html",
    "href": "robustness.html",
    "title": "Robustness analysis",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "11. Uncertainty modelling and analysis",
      "Robustness analysis"
    ]
  },
  {
    "objectID": "discr_dir_mpc_economic.html",
    "href": "discr_dir_mpc_economic.html",
    "title": "Economic MPC",
    "section": "",
    "text": "(Ellis, Liu, and Christofides 2017), (Ellis, Durand, and Christofides 2014), (Faulwasser, Grüne, and Müller 2018), (Rawlings, Angeli, and Bates 2012)",
    "crumbs": [
      "6. More on MPC",
      "Economic MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_economic.html#literature",
    "href": "discr_dir_mpc_economic.html#literature",
    "title": "Economic MPC",
    "section": "",
    "text": "(Ellis, Liu, and Christofides 2017), (Ellis, Durand, and Christofides 2014), (Faulwasser, Grüne, and Müller 2018), (Rawlings, Angeli, and Bates 2012)",
    "crumbs": [
      "6. More on MPC",
      "Economic MPC"
    ]
  },
  {
    "objectID": "discr_dir_mpc_stability.html",
    "href": "discr_dir_mpc_stability.html",
    "title": "Stability of MPC",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "6. More on MPC",
      "Stability of MPC"
    ]
  },
  {
    "objectID": "opt_theory_references.html",
    "href": "opt_theory_references.html",
    "title": "References for optimization theory",
    "section": "",
    "text": "If a single reference book on nonlinear optimization is to be recommended, be it (Nocedal and Wright 2006) that sits on your book shelf.\nIf one or two more can still fit, (Bertsekas 2016), (Luenberger and Ye 2021) are classical comprehensive references on nonlinear programming (the latter covers linear programming too).\nWhile all the three books are only available for purchase, there is a wealth of resources are freely available online such as the notes (Gros and Diehl 2022) accompanying a course on optimal control, which do a decent job of introduction to a nonlinear programming, and beautifully typeset modern textbooks (Kochenderfer and Wheeler 2019) and (Martins and Ning 2022), the former based on Julia language. Yet another high-quality textbook that is freely available online is (Bierlaire 2018).\nWhen restricting to convex optimization, the bible of this field (Boyd and Vandenberghe 2004) is also freely available online. It is a must-have for everyone interested in optimization. Another textbook biased towards convex optimization is (Calafiore 2014), which is freely accessible through its web version. Yet another advanced and treatment of convex optimization is (Ben-Tal and Nemirovski 2023), which is also freely available online.\nMaybe a bit unexpected resources on theory are materials accompanying some optimization software. Partilarly recommendable is (“MOSEK Modeling Cookbook” 2024), it is very useful even if you do not indend to use their software.\n\n\n\n\n Back to topReferences\n\nBen-Tal, Aharon, and Arkadi Nemirovski. 2023. “Lectures on Modern Convex Optimization - 2020/2021/2022/2023 Analysis, Algorithms, Engineering Applications.” Lecture Notes. Technion & Georgia Institute of Technology. https://www2.isye.gatech.edu/~nemirovs/LMCOLN2023Spring.pdf.\n\n\nBertsekas, Dimitri. 2016. Nonlinear Programming. 3rd ed. Belmont, Mass: Athena Scientific. http://www.athenasc.com/nonlinbook.html.\n\n\nBierlaire, Michel. 2018. Optimization: Principles and Algorithms. 2nd ed. Lausanne: EPFL Press. https://transp-or.epfl.ch/books/optimization/html/OptimizationPrinciplesAlgorithms2018.pdf.\n\n\nBoyd, Stephen, and Lieven Vandenberghe. 2004. Convex Optimization. Seventh printing with corrections 2009. Cambridge, UK: Cambridge University Press. https://web.stanford.edu/~boyd/cvxbook/.\n\n\nCalafiore, Giuseppe C. 2014. Optimization Models. Cambridge, UK: Cambridge University Press. https://people.eecs.berkeley.edu/~elghaoui/optmodbook.html.\n\n\nGros, Sebastien, and Moritz Diehl. 2022. “Numerical Optimal Control (Draft).” Systems Control and Optimization Laboratory IMTEK, Faculty of Engineering, University of Freiburg. https://www.syscop.de/files/2020ss/NOC/book-NOCSE.pdf.\n\n\nKochenderfer, Mykel J., and Tim A. Wheeler. 2019. Algorithms for Optimization. The MIT Press. https://algorithmsbook.com/optimization/.\n\n\nLuenberger, David G., and Yinyu Ye. 2021. Linear and Nonlinear Programming. 5th ed. Vol. 228. International Series in Operations Research & Management Science. Cham, Switzerland: Springer. https://doi.org/10.1007/978-3-030-85450-8.\n\n\nMartins, Joaquim R. R. A., and Andrew Ning. 2022. Engineering Design Optimization. Cambridge ; New York, NY: Cambridge University Press. https://mdobook.github.io/.\n\n\n“MOSEK Modeling Cookbook.” 2024. Mosek ApS. https://docs.mosek.com/MOSEKModelingCookbook-a4paper.pdf.\n\n\nNocedal, Jorge, and Stephen Wright. 2006. Numerical Optimization. 2nd ed. Springer Series in Operations Research and Financial Engineering. New York: Springer. https://doi.org/10.1007/978-0-387-40065-5.",
    "crumbs": [
      "1. Optimization – theory",
      "References for optimization theory"
    ]
  },
  {
    "objectID": "H2.html",
    "href": "H2.html",
    "title": "H2-optimal control",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "10. Some extensions: LQG, LTR, H2",
      "H2-optimal control"
    ]
  },
  {
    "objectID": "reduction_order_controller.html",
    "href": "reduction_order_controller.html",
    "title": "Controller order reduction",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "14. Model and controller order reduction",
      "Controller order reduction"
    ]
  }
]